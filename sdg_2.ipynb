{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniGPT For Generating Synthetic Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Toxic comments online come in many forms and in many arenas. There are currently several ways to mitigate these comments(for those organizations who wish to do so). Some of these ways include human moderators, and training machine learning models to detect toxicity in online comments.\n",
    "\n",
    "The issue with human moderators is that some of these platforms have grown so large so quickly that there are not nearly enough moderators to achieve any sense of control for most of these comments. The shear volume of toxicity and bots online makes it unrealistic to think we could do this job with humans at this point.\n",
    "\n",
    "Many companies are employing machine learning to assist with identifying toxic comments online automatically. The problem with this approach is the lack of labeled training data to train the models on.\n",
    "\n",
    "This is the problem I am going to solve using generative deep learning techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk) (4.64.1)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.8.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.3/774.3 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.8.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "## set seeds for repeatable conclusion\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data I will be using to train the generative model was released on Kaggle as part of an ongoing series of competitions sponsored by the [Google company Jigsaw](https://en.wikipedia.org/wiki/Jigsaw_(company)).\n",
    "\n",
    "The data consists of online comments with various severity levels of toxicity. There are versions of these comments labeled by human annotators wherein they label each comment as toxic or not, or other sets where they were labeled as different categories of toxic such as hatespeech, racist/sexist, obscene, etc. Although these are the labeled datasets we would be adding the synthetic data to in order to create more training data, for this task of simply generating similar text data we will only focus on the comments themselves.\n",
    "\n",
    "The data provided by this competition includes a total of `14,251` unique toxic comments. Theses are the comments I will use to train the generative model with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "The data came in two different files.\n",
    "\n",
    "1) Comments to score: This acts as a test dataset of comments for scoring after the model was trained.\n",
    "\n",
    "2) Validation data: This was the training data for the competition wherein there are two columns. One column labeled less toxic was a comment which human annotators labeled as less toxic than its more toxic counterpart in the other column. There was no actual training data where a comment was paired with its severity rating. The models were trained using creative techniques with the validation data and other classification data sets to train a model which predicted severity of comments.\n",
    "\n",
    "Since for our purposes we are only interested in the actual text comments themselves, I will only be using those columns from these datasources.\n",
    "\n",
    "I start by reading them all into pandas dataframes, isolating the text columns from each one, and stacking them all together so we have a single column of text when it is all said and done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7537 entries, 0 to 7536\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   comment_id  7537 non-null   int64 \n",
      " 1   text        7537 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 117.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "601                               silly little boy my ass\n",
       "2031    Is that so? Than why so many people questiong ...\n",
       "1928     Wow \\nThanks! You are SO amazing! I am in awe...\n",
       "468     \"\\n\\n Send this to User:Bumpusmills1 \\n\\nPass ...\n",
       "6351    \"\\n\\nthat is your opinion. And what is your pr...\n",
       "748      man its all about captain morgans spiced rum....\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = pd.read_csv('comments_to_score.csv')\n",
    "data1.info()\n",
    "\n",
    "## Isolate only text column\n",
    "data1 = data1['text']\n",
    "\n",
    "data1.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We can see the comments to score was the test file which contained only comments and their corresponding id's***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28422 entries, 0 to 28421\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   worker      28422 non-null  int64 \n",
      " 1   less_toxic  28422 non-null  object\n",
      " 2   more_toxic  28422 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 666.3+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-89246e0b6f04>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data2 = pd.read_csv('validation_data.csv',error_bad_lines=False, engine=\"python\")\n",
      "Skipping line 28424: unexpected end of data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23403</th>\n",
       "      <td>352</td>\n",
       "      <td>\"\\n\\nChip killed Ciji, but Val confessed becau...</td>\n",
       "      <td>Kristof the problem here is a lag from the old...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>173</td>\n",
       "      <td>He is also responsible for the llama vandalism...</td>\n",
       "      <td>racist ==\\nyou are a western racist who edits ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15809</th>\n",
       "      <td>52</td>\n",
       "      <td>\"\\n\\nSmash Lab, Part II\\nYou really are a piec...</td>\n",
       "      <td>Wow... \\n\\nThis page is so awful. It has some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8426</th>\n",
       "      <td>733</td>\n",
       "      <td>Insult me once more and I swear you go down. U...</td>\n",
       "      <td>\"\\n\\nSockpuppetry case\\n \\nYou have been accus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22092</th>\n",
       "      <td>699</td>\n",
       "      <td>STOP \\n\\nI traced your vandalism edits on the...</td>\n",
       "      <td>A Bisexual, like a homosexual or a heterosexua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13835</th>\n",
       "      <td>481</td>\n",
       "      <td>\"\\n\\n The Ouse \\n\\nYou reverted my edit with a...</td>\n",
       "      <td>This is some BULL S.....VSmith, when does my b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       worker                                         less_toxic  \\\n",
       "23403     352  \"\\n\\nChip killed Ciji, but Val confessed becau...   \n",
       "15795     173  He is also responsible for the llama vandalism...   \n",
       "15809      52  \"\\n\\nSmash Lab, Part II\\nYou really are a piec...   \n",
       "8426      733  Insult me once more and I swear you go down. U...   \n",
       "22092     699   STOP \\n\\nI traced your vandalism edits on the...   \n",
       "13835     481  \"\\n\\n The Ouse \\n\\nYou reverted my edit with a...   \n",
       "\n",
       "                                              more_toxic  \n",
       "23403  Kristof the problem here is a lag from the old...  \n",
       "15795  racist ==\\nyou are a western racist who edits ...  \n",
       "15809   Wow... \\n\\nThis page is so awful. It has some...  \n",
       "8426   \"\\n\\nSockpuppetry case\\n \\nYou have been accus...  \n",
       "22092  A Bisexual, like a homosexual or a heterosexua...  \n",
       "13835  This is some BULL S.....VSmith, when does my b...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = pd.read_csv('validation_data.csv',error_bad_lines=False, engine=\"python\")\n",
    "data2.info()\n",
    "\n",
    "data2.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-64cbe9b74f8b>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data4 = pd.read_csv('jigsaw-toxic-comment-train.csv',error_bad_lines=False, engine=\"python\")\n",
      "Skipping line 29405: unexpected end of data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29403 entries, 0 to 29402\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   id             29403 non-null  object\n",
      " 1   comment_text   29403 non-null  object\n",
      " 2   toxic          29403 non-null  int64 \n",
      " 3   severe_toxic   29403 non-null  int64 \n",
      " 4   obscene        29403 non-null  int64 \n",
      " 5   threat         29403 non-null  int64 \n",
      " 6   insult         29403 non-null  int64 \n",
      " 7   identity_hate  29403 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 1.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17452</th>\n",
       "      <td>2e0f60bf77130180</td>\n",
       "      <td>Shit \\n\\nGeorge tan031993 you're a bitch!!!\\n\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15220</th>\n",
       "      <td>282f3fa2114875a3</td>\n",
       "      <td>THIS IS FAR LEFTWING UAF CRANK who deletes/dis...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18744</th>\n",
       "      <td>31766569d8350a7c</td>\n",
       "      <td>stop sending me this shit...\\ni'm not doing an...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28413</th>\n",
       "      <td>4b3945d021a8af3c</td>\n",
       "      <td>\"\\n\\nThis needed to be clarified since there h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23391</th>\n",
       "      <td>3dc5c117b22178ea</td>\n",
       "      <td>hello \\n\\nI edited the page Jacob because the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3561</th>\n",
       "      <td>09914ace666e9c1f</td>\n",
       "      <td>heY TOdds! Quick Q? \\n\\nWhy Are You So Gay</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                       comment_text  \\\n",
       "17452  2e0f60bf77130180  Shit \\n\\nGeorge tan031993 you're a bitch!!!\\n\\...   \n",
       "15220  282f3fa2114875a3  THIS IS FAR LEFTWING UAF CRANK who deletes/dis...   \n",
       "18744  31766569d8350a7c  stop sending me this shit...\\ni'm not doing an...   \n",
       "28413  4b3945d021a8af3c  \"\\n\\nThis needed to be clarified since there h...   \n",
       "23391  3dc5c117b22178ea  hello \\n\\nI edited the page Jacob because the ...   \n",
       "3561   09914ace666e9c1f         heY TOdds! Quick Q? \\n\\nWhy Are You So Gay   \n",
       "\n",
       "       toxic  severe_toxic  obscene  threat  insult  identity_hate  sum  \n",
       "17452      1             1        1       0       1              0    4  \n",
       "15220      1             0        0       0       0              0    1  \n",
       "18744      1             0        1       0       1              0    3  \n",
       "28413      0             0        1       0       0              0    1  \n",
       "23391      1             0        1       0       0              0    2  \n",
       "3561       1             0        0       0       0              0    1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4 = pd.read_csv('jigsaw-toxic-comment-train.csv',error_bad_lines=False, engine=\"python\")\n",
    "data4 = data4.dropna(how='all')\n",
    "data4.info()\n",
    "\n",
    "## Sum across labels to filter out clean comments\n",
    "data4['sum'] = data4.loc[:, 'toxic':].sum(axis=1)\n",
    "\n",
    "## Keep only comments with some type of label\n",
    "data4 = data4[data4['sum'] > 0]\n",
    "\n",
    "data4.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27531"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-79f711889c2d>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data5 = pd.read_csv('jigsaw-unintended-bias-train.csv',error_bad_lines=False, engine=\"python\")\n",
      "Skipping line 8257: unexpected end of data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8255 entries, 0 to 8254\n",
      "Data columns (total 45 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   id                                   8255 non-null   int64  \n",
      " 1   comment_text                         8255 non-null   object \n",
      " 2   toxic                                8255 non-null   float64\n",
      " 3   severe_toxicity                      8255 non-null   float64\n",
      " 4   obscene                              8255 non-null   float64\n",
      " 5   identity_attack                      8255 non-null   float64\n",
      " 6   insult                               8255 non-null   float64\n",
      " 7   threat                               8255 non-null   float64\n",
      " 8   asian                                1795 non-null   float64\n",
      " 9   atheist                              1795 non-null   float64\n",
      " 10  bisexual                             1795 non-null   float64\n",
      " 11  black                                1795 non-null   float64\n",
      " 12  buddhist                             1795 non-null   float64\n",
      " 13  christian                            1795 non-null   float64\n",
      " 14  female                               1795 non-null   float64\n",
      " 15  heterosexual                         1795 non-null   float64\n",
      " 16  hindu                                1795 non-null   float64\n",
      " 17  homosexual_gay_or_lesbian            1795 non-null   float64\n",
      " 18  intellectual_or_learning_disability  1795 non-null   float64\n",
      " 19  jewish                               1795 non-null   float64\n",
      " 20  latino                               1795 non-null   float64\n",
      " 21  male                                 1795 non-null   float64\n",
      " 22  muslim                               1795 non-null   float64\n",
      " 23  other_disability                     1795 non-null   float64\n",
      " 24  other_gender                         1795 non-null   float64\n",
      " 25  other_race_or_ethnicity              1795 non-null   float64\n",
      " 26  other_religion                       1795 non-null   float64\n",
      " 27  other_sexual_orientation             1795 non-null   float64\n",
      " 28  physical_disability                  1795 non-null   float64\n",
      " 29  psychiatric_or_mental_illness        1795 non-null   float64\n",
      " 30  transgender                          1795 non-null   float64\n",
      " 31  white                                1795 non-null   float64\n",
      " 32  created_date                         8255 non-null   object \n",
      " 33  publication_id                       8255 non-null   int64  \n",
      " 34  parent_id                            4170 non-null   float64\n",
      " 35  article_id                           8255 non-null   int64  \n",
      " 36  rating                               8255 non-null   object \n",
      " 37  funny                                8255 non-null   int64  \n",
      " 38  wow                                  8255 non-null   int64  \n",
      " 39  sad                                  8255 non-null   int64  \n",
      " 40  likes                                8255 non-null   int64  \n",
      " 41  disagree                             8255 non-null   int64  \n",
      " 42  sexual_explicit                      8255 non-null   float64\n",
      " 43  identity_annotator_count             8255 non-null   int64  \n",
      " 44  toxicity_annotator_count             8255 non-null   int64  \n",
      "dtypes: float64(32), int64(10), object(3)\n",
      "memory usage: 2.8+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-79f711889c2d>:25: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  data5['sum'] = data5.loc[:, 'toxic':].sum(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>240848</td>\n",
       "      <td>Are you suggesting that people should not have...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>273136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5626</th>\n",
       "      <td>248402</td>\n",
       "      <td>Travel Oregon: I need $250k a year for five ye...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>43464.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6379</th>\n",
       "      <td>249332</td>\n",
       "      <td>If nano guy works hard and does something good...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>293582.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7556</th>\n",
       "      <td>250819</td>\n",
       "      <td>There is also a question that has been raised ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>297622.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7775</th>\n",
       "      <td>251136</td>\n",
       "      <td>Since she has basically announced that she onl...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>48524.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>248990</td>\n",
       "      <td>Winning is the all-time greatest cure for wart...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>292787.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                       comment_text  toxic  \\\n",
       "628   240848  Are you suggesting that people should not have...    0.0   \n",
       "5626  248402  Travel Oregon: I need $250k a year for five ye...    0.0   \n",
       "6379  249332  If nano guy works hard and does something good...    0.0   \n",
       "7556  250819  There is also a question that has been raised ...    0.0   \n",
       "7775  251136  Since she has basically announced that she onl...    0.2   \n",
       "6100  248990  Winning is the all-time greatest cure for wart...    0.0   \n",
       "\n",
       "      severe_toxicity  obscene  identity_attack  insult  threat  asian  \\\n",
       "628               0.0      0.0              0.0     0.0     0.0    0.0   \n",
       "5626              0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "6379              0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "7556              0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "7775              0.0      0.0              0.2     0.0     0.0    0.0   \n",
       "6100              0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "\n",
       "      atheist  ...    rating  funny  wow  sad  likes  disagree  \\\n",
       "628       0.0  ...  approved      0    0    0      2         0   \n",
       "5626      NaN  ...  approved      0    0    0      4         0   \n",
       "6379      NaN  ...  approved      0    0    0      1         0   \n",
       "7556      NaN  ...  approved      0    0    0      0         0   \n",
       "7775      0.0  ...  approved      0    0    0      0         0   \n",
       "6100      NaN  ...  approved      0    0    0      0         0   \n",
       "\n",
       "      sexual_explicit  identity_annotator_count  toxicity_annotator_count  \\\n",
       "628               0.0                         4                         4   \n",
       "5626              0.0                         0                         4   \n",
       "6379              0.0                         0                         4   \n",
       "7556              0.0                         0                         4   \n",
       "7775              0.1                         4                        10   \n",
       "6100              0.0                         0                         4   \n",
       "\n",
       "           sum  \n",
       "628   273136.0  \n",
       "5626   43464.0  \n",
       "6379  293582.0  \n",
       "7556  297622.0  \n",
       "7775   48524.5  \n",
       "6100  292787.0  \n",
       "\n",
       "[6 rows x 46 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5 = pd.read_csv('jigsaw-unintended-bias-train.csv',error_bad_lines=False, engine=\"python\")\n",
    "data5.info()\n",
    "\n",
    "# drop_columns = [\n",
    "#     'created_date',\n",
    "#     'publication_id',\n",
    "#     'parent_id',\n",
    "#     'article_id', \n",
    "#     'rating', \n",
    "#     'funny', \n",
    "#     'wow', \n",
    "#     'sad', \n",
    "#     'likes', \n",
    "#     'disagree', \n",
    "#     'identity_annotator_count', \n",
    "#     'toxicity_annotator_count'\n",
    "# ]\n",
    "\n",
    "# data5 = data5.drop(columns=drop_columns, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Sum across labels to filter out clean comments\n",
    "data5['sum'] = data5.loc[:, 'toxic':].sum(axis=1)\n",
    "\n",
    "## Keep only comments with some type of label\n",
    "data5 = data5[data5['sum'] > 0]\n",
    "\n",
    "data5.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***This was the data provided to validate the models performance during training. The three columns are workers(annotators) and the other two are text columns which we will use both to train our generative model with.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>313</td>\n",
       "      <td>This article sucks \\n\\nwoo woo wooooooo</td>\n",
       "      <td>WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>\"And yes, people should recognize that but the...</td>\n",
       "      <td>Daphne Guinness \\n\\nTop of the mornin' my fav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>Western Media?\\n\\nYup, because every crime in...</td>\n",
       "      <td>\"Atom you don't believe actual photos of mastu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>347</td>\n",
       "      <td>And you removed it! You numbskull! I don't car...</td>\n",
       "      <td>You seem to have sand in your vagina.\\n\\nMight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539</td>\n",
       "      <td>smelly vagina \\n\\nBluerasberry why don't you ...</td>\n",
       "      <td>hey \\n\\nway to support nazis, you racist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   worker                                         less_toxic  \\\n",
       "0     313            This article sucks \\n\\nwoo woo wooooooo   \n",
       "1     188  \"And yes, people should recognize that but the...   \n",
       "2      82   Western Media?\\n\\nYup, because every crime in...   \n",
       "3     347  And you removed it! You numbskull! I don't car...   \n",
       "4     539   smelly vagina \\n\\nBluerasberry why don't you ...   \n",
       "\n",
       "                                          more_toxic  \n",
       "0  WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...  \n",
       "1   Daphne Guinness \\n\\nTop of the mornin' my fav...  \n",
       "2  \"Atom you don't believe actual photos of mastu...  \n",
       "3  You seem to have sand in your vagina.\\n\\nMight...  \n",
       "4           hey \\n\\nway to support nazis, you racist  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all columns into a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-ce653e639800>:7: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data3 = pd.read_csv('validation_data.csv',error_bad_lines=False, engine=\"python\")\n",
      "Skipping line 28424: unexpected end of data\n"
     ]
    }
   ],
   "source": [
    "## Isolate text column\n",
    "data2 = data2['more_toxic']\n",
    "data4 = data4['comment_text']\n",
    "data5 = data5['comment_text']\n",
    "\n",
    "## Isolate text column\n",
    "data3 = pd.read_csv('validation_data.csv',error_bad_lines=False, engine=\"python\")\n",
    "data3 = data3['less_toxic']\n",
    "\n",
    "text_column = pd.concat([data1, data2, data3, data4, data5], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "You are not sorry one damned bit.  You have yet to refute what I have written.  All you do is pass the insults as if it were salt on the dinner table.  This is on every article in which we disagree.  If you have something useful and constructive to say, then don't be a harpy troll.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       19\n",
       "this irishtom guy is turning every article into an ad for islam                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  19\n",
       " sorry i jumped to conclusions \\n\\non christian terrorism article man, I don't agree with you, and I want you to go and listen to 'prophet of doom' (now in audio format) as it is good. But I was wrong to be so rude. It is not the Southern European way.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     19\n",
       " This article is completely fake. Pyrrhus was ILLIRYAN, NOT GREEK.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               16\n",
       "\"\\n\\nHas he called himself a Nazi, or is that just what an opponent called him? Look, Untwirl, I have been a Socialist my entire life, but I will not put down someone I don't like as \"\"right-wing.\"\" life to more complex than that. Sometimes I like those right wingers better than my political \"\"friends\"\", who often act like jerks.   \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  16\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ..\n",
       "The fundamental point is, the corporate agencies of \"State\" are NOT the government of People and have no right to dictate policy to an established Commonwealth (state as a body politic) once is a state has entered the Union. Say what you will... Folks like you violate the law of land and the most fundamental Right of the People everyday. That's the job, to subvert the meaning and intent of original documents. Pledged to a foreign entity, and part of monolithic monopoly with stranglehold on actual law. The ABA replaced the common law with statutory Roman law. The ABA defies EVERY aspect the Sherman Anti-Trust Act, negates the separation of powers and perpetuates a conflict of interest fraud and profiteering while constructing unlawful obfuscations and deny full disclosure of pertinent fact to the public to protect itself. Of course you are going disagree with me at every twist and turn. But the showdown is on. What will not be addressed by law will revert to common law Right.     1\n",
       "The United States District Courts created by Congress (28 U.S.C. 132(a)) and doing business throughout the Union are not Article III, but Article IV courts and have usurped exercise of territorial and personal jurisdiction over property belonging to the Americans living there.\\n\\nWe the People of the Continental Republics of the several states are the source and authority of OUR own law and as a result, are the final arbiters. Not those owned by, pledged to, or operating in cahoots with ANY foreign interest. \\n\\nThis fraud against the People will end.                                                                                                                                                                                                                                                                                                                                                                                                                                                     1\n",
       "Carl you will be missed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          1\n",
       "First VQ, now this. My poor heart. RIP Sewickleys, serving me from ages 17 through 35. May your patron saint Tonya Harding bless you for all time immemorial. And also, way to ruin Battlestar Galactica for me. GFY, Sackhoff family.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            1\n",
       "actually...while the rest of the nation pulled out of recession in the 1980's, Oregon lost a decade because Oregon was internationally known as Tax He11, USA. It was when liberal communities turned to Reaganomics 101, and invested millions of dollars in developing sister city relationships with Asia, granting property tax relief to corporations offshoring jobs to Oregon, our economy became so smoking hot in the 90's we created more jobs than we could fill, which saw our wages double in the decade. Today liberals only grant property tax relief to crony corporations that suit their political agenda. What you're failing to understand is we have the highest corporate tax rates in the 1st world.                                                                                                                                                                                                                                                                                                       1\n",
       "Length: 24760, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_column.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like between the data provided for the competition there are many duplicates. However we can see that some comments are reused many more times than other comments. For example the most used comments were repeated `19` times in the datasets while others only `2` times. \n",
    "\n",
    "Since the duplications are not balanced if we left the data like this I am afraid we would be biasing the model towards the comments which were present more in the data. \n",
    "\n",
    "I will remove all duplicate comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total numer of comments in text data = 75695\n",
      "Numer of unique comments in text data = 24760\n",
      "Duplicate comments dropped\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total numer of comments in text data = {len(text_column)}\")\n",
    "print(f\"Numer of unique comments in text data = {len(text_column.unique())}\")\n",
    "\n",
    "text_column = text_column.drop_duplicates()\n",
    "print(\"Duplicate comments dropped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the toxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>noun_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>93.760000</td>\n",
       "      <td>16.430000</td>\n",
       "      <td>21.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>138.589075</td>\n",
       "      <td>26.114842</td>\n",
       "      <td>30.023163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>46.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>106.500000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>25.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>888.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>201.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_count  verb_count  noun_count\n",
       "count  100.000000  100.000000  100.000000\n",
       "mean    93.760000   16.430000   21.060000\n",
       "std    138.589075   26.114842   30.023163\n",
       "min      4.000000    0.000000    0.000000\n",
       "25%     23.000000    4.000000    4.000000\n",
       "50%     46.500000    8.000000   10.500000\n",
       "75%    106.500000   21.000000   25.250000\n",
       "max    888.000000  184.000000  201.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "data['text'] = text_column\n",
    "data = data.sample(100)\n",
    "\n",
    "# Function to calculate word count\n",
    "def count_words(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "# Function to calculate verb count\n",
    "def count_verbs(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    verb_count = len([word for word, tag in tagged_words if tag.startswith('V')])\n",
    "    return verb_count\n",
    "\n",
    "# Function to calculate noun count\n",
    "def count_nouns(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    noun_count = len([word for word, tag in tagged_words if tag.startswith('N')])\n",
    "    return noun_count\n",
    "\n",
    "# Add word count column\n",
    "data['word_count'] = data['text'].apply(count_words)\n",
    "\n",
    "# Add verb count column\n",
    "data['verb_count'] = data['text'].apply(count_verbs)\n",
    "\n",
    "# Add noun count column\n",
    "data['noun_count'] = data['text'].apply(count_nouns)\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABCh0lEQVR4nO3dd5xU1d348c936i5lQYoKopRQlCYgIoIQS2zRBH0iilFBsfz0sURjCnkSjTGmmMfYotHH2JCo2A22WLALioBEQEUBFwWRXha2TPv+/rhnlmGZ3Z3dndky+32/dl8zc+695545c+d+55x777miqhhjjDH14WvqAhhjjGm5LIgYY4ypNwsixhhj6s2CiDHGmHqzIGKMMabeLIgYY4ypt1YfRETkbhG5Jkt5HSAiO0TE716/KSIXZCNvl99LIjIlW/nVYb03iMhGEfm2sdedpizFIvK9pi5HcyMiD4rIDU1djuZORK4TkX82dTnySV4HEbfDKROREhHZKiJzRORiEal836p6sar+PsO8atx5qepXqtpOVeNZKPseG7uqnqiq0xuadx3LcQBwNTBQVfdNM32ZiJyR8nqsiGiatBIRCTRCeUeJyIvu894sIvNE5LxGWG/GPxhEZLSI7BSRdmmmfSQil2W/hHusJ+S2sS9cWYpF5H4R6ZXj9R4pIqtzuY76cmVTEfl7lfR3ReTcJiiPiMgVIrLEfUarReQJERmS4/X2cvWQ0fc1r4OI8wNVbQ/0BP4M/BK4L9sraYwdZBM5ANikquurmf42MD7l9XjgszRpc1U1lulK61OfInI48DrwFtAX6AxcApxY17xySVXfB1YDp6Wmi8hgYCDwaF3yS7Z86+hJ4IfAj4EOwMHAAuCYeuSVT3YC5+Q6mGboNuAnwBVAJ6A/8CxwUhOWaU+qmrf/QDHwvSppo4AEMNi9fhC4wT3vAjwPbAU2A+/gBdoZbpkyYAfwC6AXoMD5wFd4O9NkWsDl9ybwJ2AesB34F9DJTTsSWJ2uvMAJQASIuvX9JyW/C9xzH/AbYBWwHngI6OCmJcsxxZVtI/DrGuqpg1t+g8vvNy7/77n3nHDleDDNsucAi1NevwicmybtN+75D4Glro7fBA6q8v5/CXwMVAABl/8qYBPw63Sfacry7wJ31rJNXAgsd5/vLKB7lToLpMybWt/nuvxvArYAXwInuml/AOJAuaunOzLYNv8HeL1K2l+AZ9zzA4FXXTmXAaenzPcgcJer153uc3oQuNstU4IXSHtWs+7k57p/DeXr7upns6uvC6us/4aU10eSsi27z+hn7nPcBjwGFABtq2xPO5L1X0tdTQNWuPf1CXBqyrRqPxc3vberixJXN3cA/6xmPUfiBfe/AQ9U2a7OzeB7t1s9VN0HAdcBj7tlSvC+ByOrKUs/t02Nquv3NmVd/0yZtxd77pt+D7znyvIK0MVN+8rNm/yMDq/x86ntA2zJ/1Szw3GVdEnVLwTeDv9uIOj+xwGSLq+UD+Uh9+UorOaDWgMMdvM8lfxgM9zg/lll+pvs2qlNxfty9wHaAU8DM6qU7R+uXAfj7ZQPqqaeHsILcO3dsp8D51dXzirL9sTbKXTC+4Ktd+v8OiVtG15rpD/eTu9YV7+/cO8hlPL+FwH7uzwGuo14PBAGbgZi1XymbfC+dEfVUNaj8QLqCJff34C3033J0tT3uXhB/ULAj9fC+YZd20flvBlum/u797K/e+3D24Gd4raVr4Hz8ALpcFfugSnb7DZgrFuuwKWVpNTVbcC71az7z8BbtZTvbeDvLu9heDuqo6t+Z9JtI+5znIcXiDoBnwIXZ7I9VVOWiS4vH3CG24a6Zfi5zHXbTdjVTQm1B5F98X70DXDpqUGkpu/dHu+NPb/T5cD3XVn/BLxfTVkuBlbVUi81fW+vo/YgsgLvO1noXv+5uu9CTf+toTsrnW/wNu6qokA3vF9wUVV9R12t1uA6Vd2pqmXVTJ+hqktUdSdwDXB6PbsfqjoLuFlVV6rqDuBXwKQq3UC/U9UyVf0P8B+8YLIbV5ZJwK9UtURVi4G/4rUAaqWqq/CC8jiX/xeuLt5LSQsBH+DtAF5Q1VdVNYr367EQGJOS5e2q+rXL4zTgeVV9W1Ur8OovUU1R9sLbyaytobhnAfer6kKX36+Aw+vQdbFKVf+h3jGv6Xjbyj4ZLrsbVf0a74ubrOdj8HZ0LwAnA8Wq+oCqxlT1I7wfIBNTsviXqr6nqglVLXdpL6TU1a/de9s/zeo7U0M9uWXGAr9U1XJVXQTcC0yuw1u8XVW/UdXNwHN4gaheVPUJl1dCVR8DvsDrUUhK+7m443mHAteoaoWqvu3KUtv6vsX7MXl9msmZfO9q8q6qvujKOoM030mnts+oQd9b5wFV/dx91x6nnp9Raw0i++E106v6X7xfGa+IyEoRmZZBXl/XYfoqvF/gXTIqZc26u/xS8w6w+04t9WyqUrxfTlV1cWWqmtd+dShL8rjIeLwuQPB+vSXT5rkd225lVtUEXv2kriu1vrqnvnaBeFM1ZdiCF2C61VDOquvf4fLL9L1W1qeqlrqn6eo0U9PZ9aU/B5jpgmtP4DB3csBWEdmKt/NKPbEh3XaXWlc78Lbx7mnm20Tt9bRZVUtS0uq6TWSy7WVERCaLyKKUuhjM7t+h6j6X7sAWt90kpW7nNbkROF5Equ7kM/ne1aRqvRRUE4Bq+4yy8b3NymfU6oKIiByKV9HvVp3mIvrVqtoHr+/+pyKSPNBYXYuktpZK6i/BA/BaOxvxmuRtUsrlB7rWId9v8HY2qXnHgHW1LFfVRlemqnmtqUMeySAyjl1B5J2UtLfTlVlEBK9+UteV+r7XklJ/ItIG7xfaHtzOYy7woxrKWXX9bV1+a/A+D0j5TNh9p12b2j6vdJ4GeojIUcB/4QUV8ILBW6raMeW/napeUsv6UuuqHV5r+5s0870GjBKRHtWU6xugk4i0T0lL3SZ223bJYT2JSE+8btnLgM6q2hFYAkgGi68F9nKfc9IBGRVSdRNwK95xg1Q1fe9q+07XxWy8bWNkNdNr+9422mfUaoKIiBSJyMnATLy+wsVp5jlZRPq6nds2vD72ZPfJOrx+0Lo6W0QGuh3g9cCTrin7Od6vkJNEJIh3UCycstw6oFfq6chVPApcJSK93Q7jj8BjWoczoABcWR4H/iAi7d2X9qdAXc6lfxuv3348XjcWwGK8g5pHsSuIPA6cJCLHuPd8Nd6xmjnV5PskcLKIHCEiIbz6q2mb/QVwroj8XEQ6A4jIwSIy001/FDhPRIaJSBivzj5Q1WJV3YD3BTxbRPwiMhX4Th3qYI/tw532e111C7hfyE8CD+B1ycx3k54H+ovIOSISdP+HishBtZTh+yl19Xu8/vY9Wiyq+hreQeZnROQQEQm4z/5iEZnqlpkD/ElECkRkKN4JJMltYpFbVycR2Re4spZypVoHdBaRDsmE5Km11czfFm+ntsHNex5eS6RWrqt1PvA7d0rzEcAP6lDWm/G6WlPrvabvXW3f6Yyp6hd4x6QedfUTcp/FJBGZlsH3dhEwXrxr1zrgdbtlagPefi+j/V1rCCLPiUgJ3q+7X+NtGNVdN9AP71faDrxftX9X1TfctD8Bv3FN6p/VYf0z8A5Efot3kPIKAFXdBvw3Xl9z8pdw6vnzT7jHTSKyME2+97u838Y7I6UcuLwO5Up1uVv/SrwW2iMu/4yo6ud4G963qrrVpSXwDq4W4YKEqi4DzsY7oL0R7wv9A1WNVJPvUuBSV561eF1W1V5joKpz8A6eHw2sFJHNwD14ZzEld57X4B1fWIsXJCalZHEh8HO8roRBVB/c0rkNOE1EtojI7S5tf3YF1epMx/s1+VDK+ygBjnNl+wZv27mR2ndIjwC/xevGOgSvrqtzGl69PIb3g2kJMBJv+wc4E+8A6zfAM8BvXf2Bt939B++g8Ssuj4yo6md4O+KV7rvUHa+e0ta1qn6C19c/Fy8ADaH2Ok31Y+AwvDr5LSn1nEFZt+OdMZd6/LTa710G3+m6ugLvbLI78c5mXAGcyq7jOtV+b1X1VbzP5WO8U7efz3SlrlX/B+A99xmNrmn+5BkMxpgscl1Fj6vqmFpnbuVE5F7gCVV9uanLYurOgogxxph6aw3dWcYYY3LEgogxxph6syBijDGm3vJ10MDddOnSRXv16tXUxTDGmBZlwYIFG1W1xmtdWkUQ6dWrF/Pnz699RmOMMZVEpNYr/K07yxhjTL1ZEDHGGFNvFkSMMcbUW6s4JmKMaRmi0SirV6+mvLy89plN1hQUFNCjRw+CwWCdl7UgYoxpNlavXk379u3p1asX3jioJtdUlU2bNrF69Wp69+5d5+WtO8sY02yUl5fTuXNnCyCNSETo3LlzvVt/FkSMMc2KBZDG15A6tyBisiceg/kPQNT6s41pLSyImOxZ9DA8fyXMu6epS2JMvVx11VXceuutla+PP/54LrjggsrXV199NTfffHO98n7zzTc5+eST006bN28e48ePZ8CAAQwfPpwLLriA0tLStPPW14MPPsg336S70WXDWBAx2bNz/e6PxrQwY8eOZc4c7/5YiUSCjRs3snTp0srpc+bMYcyYzG4RE4/HM5pv3bp1TJw4kRtvvJFly5bx0UcfccIJJ1BSUlL7wnVgQcQ0fxH3yyla1rTlMKaexowZw9y5cwFYunQpgwcPpn379mzZsoWKigo+/fRTRowYwezZsxk+fDhDhgxh6tSpVFRUAN4QS7/85S8ZMWIETzzxBP/+97858MADGTFiBE8//XTadd55551MmTKFww8/vDLttNNOY5999mHz5s2ccsopDB06lNGjR/Pxxx8DcN1113HTTTdVzj948GCKi4spLi7moIMO4sILL2TQoEEcd9xxlJWV8eSTTzJ//nzOOusshg0bRllZ9r6jdoqvyZ7yre5xe5MWw+SH3z23lE++ye62NLB7Eb/9waBqp3fv3p1AIMBXX33FnDlzOPzww1mzZg1z586lQ4cODBkyhEQiwbnnnsvs2bPp378/kydP5q677uLKK68EoHPnzixcuJDy8nL69evH66+/Tt++fTnjjDPSrnPJkiVMmTIl7bTf/va3DB8+nGeffZbXX3+dyZMns2jRohrf4xdffMGjjz7KP/7xD04//XSeeuopzj77bO644w5uuukmRo4cmVFdZcpaIiZ7yrZ4j5GdTVsOYxpgzJgxzJkzpzKIHH744ZWvx44dy7Jly+jduzf9+/cHYMqUKbz99tuVyyeDxWeffUbv3r3p168fIsLZZ9d0y/v03n33Xc455xwAjj76aDZt2sT27TUH1t69ezNs2DAADjnkEIqLi+u83rqwlojJnvJt3mMku325pnWqqcWQS8njIosXL2bw4MHsv//+/PWvf6WoqIjzzjuv1uXbtm1bp/UNGjSIBQsWMGHChIyXCQQCJBKJytep13iEw+HK536/P6tdV+lYS8RkT8zrF7aWiGnJxowZw/PPP0+nTp3w+/106tSJrVu3MnfuXMaMGcOAAQMoLi5m+fLlAMyYMYPvfve7e+Rz4IEHUlxczIoVKwB49NFH067vsssuY/r06XzwwQeVaU8//TTr1q1j3LhxPPzww4B3dleXLl0oKiqiV69eLFy4EICFCxfy5Zdf1vq+2rdvn/WD9WBBxGRTzP0aqtjRtOUwpgGGDBnCxo0bGT169G5pHTp0oEuXLhQUFPDAAw8wceJEhgwZgs/n4+KLL94jn4KCAu655x5OOukkRowYwd577512ffvssw8zZ87kZz/7GQMGDOCggw7i5Zdfpn379lx33XUsWLCAoUOHMm3aNKZPnw7Aj370IzZv3sygQYO44447KrvWanLuuedy8cUXZ/3Auqhq1jJrrkaOHKl2U6pGcNdYWLcEinrAT5fWPr8xVXz66accdNBBTV2MVild3YvIAlWt8Ui8tURM9iRbIhFriRjTWlgQMdmTPCaSfDTG5D0LIiZ7ki2ReAW0gm5SY4wFEZNNyRaIJiARa9qyGGMahQURkz2xCvC5S49iNpKvMa1BToOIiJwgIstEZLmITEszPSwij7npH4hIL5d+rIgsEJHF7vHolGXedHkucv/pz5szjUvV68Yq6OC9tuMixrQKOQsiIuIH7gROBAYCZ4rIwCqznQ9sUdW+wC3AjS59I/ADVR0CTAFmVFnuLFUd5v5tyNjmIBk0KoOItURMy3PUUUfx8ssv75Z26623cskll2Scx5FHHkkmlxS05OHfU+WyJTIKWK6qK1U1AswEql7XPwGY7p4/CRwjIqKqH6lq8p0vBQpFJIxpvpJBI1zkXltLxLQ8Z555JjNnztwtbebMmZx55pkZLd9ahn9Plcsgsh/wdcrr1S4t7TyqGgO2AZ2rzPMjYKGqpu6VHnBdWddINfd1FJGLRGS+iMzfsGFDQ96HycQeLRELIqblOe2003jhhReIRCIAFBcX88033zBu3DheeeUVDj/8cEaMGMHEiRPZscO7Hqrq8O/gDYUybNgwBg8ezLx58/ZYT0sf/j1Vsx6AUUQG4XVxHZeSfJaqrhGR9sBTwDnAQ1WXVdV7gHvAu2K9EYrbuiVbItadZbLlpWnw7eLs5rnvEDjxz9VO7tSpE6NGjeKll15iwoQJzJw5k9NPP51NmzZxww038Nprr9G2bVtuvPFGbr75Zq699lpg1/DvAHfffTelpaUsWrSIt99+m6lTp7JkyZLd1tPSh39PlcuWyBpg/5TXPVxa2nlEJAB0ADa51z2AZ4DJqroiuYCqrnGPJcAjeN1mpqlZS8TkidQurWRX1vvvv88nn3zC2LFjGTZsGNOnT2fVqlWVy1S9V0iy+2v8+PFs376drVu3Zrz+ljD8e6pctkQ+BPqJSG+8YDEJ+HGVeWbhHTifC5wGvK6qKiIdgReAaar6XnJmF2g6qupGEQkCJwOv5fA9mExZS8RkWw0thlyaMGECV111FQsXLqS0tJRDDjmE5557jmOPPbbakXirDv9etZe96uuWPvx7qpy1RNwxjsuAl4FPgcdVdamIXC8iP3Sz3Qd0FpHlwE+B5GnAlwF9gWurnMobBl4WkY+BRXjB6R+5eg+mDqwlYvJEu3btOOqoo5g6dWpli2L06NG89957lcO/79y5k88//7zaPB577DHAa1V06NCBDh067Da9pQ//niqnx0RU9UXgxSpp16Y8LwcmplnuBuCGarI9JJtlNFkS9w5EEm7vXlsQMS3XmWeeyamnnlrZrdW1a1cefPBBzjzzzMr7qd9www3VDsFeUFDA8OHDiUaj3H///XtMTx3+ff369fh8PsaPH88JJ5zAddddx9SpUxk6dCht2rTZbfj3hx56iEGDBnHYYYfVafj3wsJC5s6dS2FhYX2rpFo2FLzJjpVvwkMT4Id3wKzL4NR74OD095Q2pjo2FHzTsaHgTdNKjpUVauM9Jlsmxpi8ZkHEZEfcBZGgO8BoQcSYVsGCiMmORNR7DLo+13i06cpijGk0FkRMdiSDRshaIsa0JhZETHYk3JhBlS0RCyLGtAYWREx2WHeWMa2SBRGTHcmg4Q+BL2gtEdNiiQhXX3115eubbrqJ6667LufrvemmmzjwwAMZNmwYhx56KA89tMeQgA2ydetW/v73v2c1T7AgYrIl2RLxBb1AYkHEtFDhcJinn36ajRs3Nto67777bl599VXmzZvHokWLmD17Ntm+hs+CiGneksdEfAHwB607y7RYgUCAiy66iFtuuWWPacXFxRx99NEMHTqUY445hq+++grwrgx/8sknK+dr164d4A1bcuSRR3Laaadx4IEHctZZZ6UNDn/84x+56667KCry7sdTVFRUOcrv7NmzGT58OEOGDGHq1KmVV8z36tWrMtDNnz+fI488EqDyivcjjzySPn36cPvttwMwbdo0VqxYwbBhw/j5z3+ejaoCmvlQ8KYFqezOClhLxGTFjfNu5LPNn2U1zwM7HcgvR/2y1vkuvfRShg4dyi9+8Yvd0i+//HKmTJnClClTuP/++7niiit49tlna8zro48+YunSpXTv3p2xY8fy3nvvccQRR1RO3759OyUlJfTp02ePZcvLyzn33HOZPXs2/fv3Z/Lkydx1111ceeWVNa7zs88+44033qCkpIQBAwZwySWX8Oc//5klS5bUOqx8XVlLxGTHHt1Z1hIxLVdRURGTJ0+u/BWfNHfuXH78Y28w8nPOOYd333231rxGjRpFjx498Pl8DBs2rE7DtC9btozevXtXjpM1ZcoU3n777VqXO+mkkwiHw3Tp0oW9996bdevWZbzOurKWiMkOd8X6b+f9iT5t/EyxlohpoExaDLl05ZVXMmLECM4777xa500dpj2RSFTeGRH2HKY9FovttmxRURHt2rVj5cqVaVsjmawzdVj4TNaZTdYSMdmRiLFDhKdXPMtNhWrdWabF69SpE6effjr33XdfZdqYMWMqR/Z9+OGHGTduHOAdn1iwYAEAs2bNIhqtW0v8V7/6FZdeemnlzad27NjBQw89xIABAyguLq4cgn7GjBl897vf3WOdTz31VK3ryNWw8BZETHYkoqwK7fr1E7X7iZg8cPXVV+92ltbf/vY3HnjgAYYOHcqMGTO47bbbALjwwgt56623OPjgg5k7d+4eN6mqzSWXXMJRRx3FoYceyuDBgxk3bhw+n4+CggIeeOABJk6cyJAhQ/D5fFx88cWAdxvdn/zkJ4wcORK/31/rOjp37szYsWMZPHhwVg+s21DwJjteuYZ3P36QS7p2BOB133foes6zTVok0/LYUPBNx4aCN00rEWOLP1j5cnPCbo9rTGtgQcRkRyLG5sCuJvVWu7OhMa2CBRGTHfEo21L6ZbeoHVg39dMautibm4bUuQURkx2JKGW+XUGkJGHXiZi6KygoYNOmTRZIGpGqsmnTJgoKCuq1vF0nYrIjHqPM56MwUEhZrIxSjTd1iUwL1KNHD1avXs2GDRuauiitSkFBAT169KjXshZETHYkYpSK0KmgE2t2rGGn5u7iJpO/gsEgvXv3bupimDqw7iyTHYkoZSK0DbalEB87NdHUJTLGNAILIiY74jHKfEJhoJC2EmAn1p1lTGtgQcRkRyJKmUBhoJA2EqAUa4kY0xpYEDHZkYhRhhdE2voC7BQ7u8aY1sCCiMmOeEpLxBdiZ1OXxxjTKHIaRETkBBFZJiLLRWRamulhEXnMTf9ARHq59GNFZIGILHaPR6csc4hLXy4it4uI5PI9mAwlYpShriUSolQAO9ffmLyXsyAiIn7gTuBEYCBwpogMrDLb+cAWVe0L3ALc6NI3Aj9Q1SHAFGBGyjJ3ARcC/dz/Cbl6D6YO4lHKxAURf4idPtl1y1xjTN7KZUtkFLBcVVeqagSYCUyoMs8EYLp7/iRwjIiIqn6kqt+49KVAoWu1dAOKVPV99S5pfQg4JYfvwWRIE9HKlkiBL0S5iN1TxJhWIJdBZD/g65TXq11a2nlUNQZsAzpXmedHwEJVrXDzr64lTwBE5CIRmS8i8+3q19yrSMRQoE2wDWELIsa0Gs36wLqIDMLr4vp/dV1WVe9R1ZGqOrJr167ZL5zZTbm7p3qBv4BCf4gKEbvPujGtQC6DyBpg/5TXPVxa2nlEJAB0ADa51z2AZ4DJqroiZf7UAV7S5WmaQFS9gBHyhwj7w1T4fCRidk8RY/JdLoPIh0A/EektIiFgEjCryjyz8A6cA5wGvK6qKiIdgReAaar6XnJmVV0LbBeR0e6srMnAv3L4HkyGIm7AxaAvSEHAGw20ImIn+hqT73IWRNwxjsuAl4FPgcdVdamIXC8iP3Sz3Qd0FpHlwE+B5GnAlwF9gWtFZJH739tN+2/gXmA5sAJ4KVfvwWQukvAGXAz5QxT4vXutl0d3NGWRjDGNIKej+Krqi8CLVdKuTXleDkxMs9wNwA3V5DkfGJzdkpqGiiZiQMBaIsa0Ms36wLppOaK6qyUSDhQCUB61IGJMvrMgYrIi6oZ+D/qCFFoQMabVsJtSmaxIHlgP+UNoMojESpuySMaYRmBBxGRFNLHr7CwJtgWgPGpBxJh8Z0HENJwqETf0e9AfJOBaIhWxsqYslTGmEVgQMQ2XiJG8Nj3oC+ILeS2RMuvOMibvWRAxDRePEnEj8od8IfyuO6vCrlg3Ju9ZEDENl4gSTQYRf4hQqD0A5dadZUzes1N8TcMl4pVBJOgLEg61A6A8VtGUpTLGNAILIqbhUruz/CEKQkUAlMetO8uYfGfdWabhElGi7ibFQV+QYMiHX5WKuLVEjMl3FkRMw6W0RIL+IIifAlXKLIgYk/csiJiGS8SJ4gWRgATAD2FVKhJ2Uypj8p0FEdNwCa8lEpIA4lokhaqU2+1xjcl7dmDdNFzcOyYSFH9lUliFcmuJGJP3LIiYhkvEiIoQ8u1q2IaBCnejKmNM/rIgYhrOXScSTAkiBQjlai0RY/KdBRHTcO6YSFBSg4iPcmuJGJP3LIiYhnMDMIb8wcqkMEKFu8eIMSZ/WRAxDRdP0xIRPxVYEDEm31kQMQ3njons3hLxUeZumWuMyV8WREzDubOzgr5dQaTAF6ACCyLG5DsLIqbhElEiAqHUICIBKlSbsFDGmMZgQcQ0nGuJBFK7syRAOYpaIDEmr2UURETkaRE5SUQs6Jg9JeJEEEK+UGVSgT+ACkQSNvSJMfks06Dwd+DHwBci8mcRGZDDMpmWJu7d2TDoTz0m4j0vt1vkGpPXMgoiqvqaqp4FjACKgddEZI6InCciwZqXNnkvEfMGYPSFK5PCrlVi9xQxJr9l3D0lIp2Bc4ELgI+A2/CCyqs1LHOCiCwTkeUiMi3N9LCIPOamfyAivZLrEpE3RGSHiNxRZZk3XZ6L3P/emb4HkyOJmDcAoz+1O8t7bi0RY/JbRkPBi8gzwABgBvADVV3rJj0mIvOrWcYP3AkcC6wGPhSRWar6Scps5wNbVLWviEwCbgTOAMqBa4DB7r+qs1Q17XpNE0gOwOjf1RIpcC0Ru0WuMfkt05bIP1R1oKr+KRlARCQMoKojq1lmFLBcVVeqagSYCUyoMs8EYLp7/iRwjIiIqu5U1Xfxgolp7pLXiQRSgogLKBUx684yJp9lGkRuSJM2t5Zl9gO+Tnm92qWlnUdVY8A2oHMG5XnAdWVdI8m7IFUhIheJyHwRmb9hw4YMsjT1Fo8SQXbrzgq7IGItEWPyW43dWSKyL96OvlBEhgPJHXYR0CbHZavOWaq6RkTaA08B5wAPVZ1JVe8B7gEYOXKkXayQQ+puShXyF1SmhQPe8/JYWVMVyxjTCGo7JnI83sH0HsDNKeklwP/UsuwaYP+U1z1cWrp5VotIAOgAbKopU1Vd4x5LROQRvG6zPYKIaTyxRAwVIZhyTKTQBZGKqAURY/JZjUFEVacD00XkR6r6VB3z/hDoJyK98YLFJLxrTVLNAqbgdY2dBryuNVzi7AJNR1Xd6E4tPhl4rY7lMlkWdV1WoZRjImF/IQDl0R1NUiZjTOOorTvrbFX9J9BLRH5adbqq3pxmseS0mIhcBrwM+IH7VXWpiFwPzFfVWcB9wAwRWQ5sxgs0yXUX43WbhUTkFOA4YBXwsgsgfrwA8o86vF+TA1F3L/XdBmAMekGkIrKzScpkjGkctXVntXWP7eqTuaq+CLxYJe3alOflwMRqlu1VTbaH1KcsJncicW9ok1DqgfVAsiVS2iRlMsY0jtq6s/7PPf6ucYpjWqKoCyK7t0S83x/lMWuJGJPPMh2A8S8iUiQiQRGZLSIbROTsXBfOtAyRZHdW6ii+Ae/kPTuwbkx+y/Q6keNUdTvegexioC/w81wVyrQs6VoivmCYUEIpj1l3ljH5LNMgkuz2Ogl4QlW35ag8pgVKtkRSh4LHH6ZAEzZ2ljF5LqOxs4DnReQzoAy4RES6YkOSGCeaiAG7d2fhD1GgSoUFEWPyWqZDwU8DxgAjVTUK7GTPcbBMKxVN1xIJhAirWkvEmDyXaUsE4EC860VSl7ErxU1lSyT1FF/8LojY2FnG5LVMh4KfAXwHWATEXbJiQcQAkWR3lm/37qzChNpNqYzJc5m2REYCA2saksS0XlFNc0wkEHYtEbvHujH5LNOzs5YA++ayIKblSt8S8YJIRcKCiDH5LNOWSBfgExGZB1T2T6jqD3NSKtOiRDTNMZFgAQWqrLPuLGPyWqZB5LpcFsK0bNFEHHxVWiKBQgpUKXdnbhlj8lNGQURV3xKRnkA/VX1NRNrgjaJrDFH1zrXY7RRf1xKpsCBiTF7LdOysC/Hugf5/Lmk/4Nkclcm0MMkgsvuB9ULCCaXcHS8xxuSnTA+sXwqMBbYDqOoXwN65KpRpWSLpWiI+H2ERyjVezVLGmHyQaRCpUNXK02zcBYd2uq8BIEoCgIBv997RQgJESRBPWCAxJl9lGkTeEpH/AQpF5FjgCeC53BXLtCSRRIIggojslh52QcUuODQmf2UaRKYBG4DFwP/Du1vhb3JVKNOyREkQQvZID7uztWzoE2PyV6ZnZyVE5FngWVXdkNsimZYmogmCsuemVOgLAhEqYtYSMSZf1dgSEc91IrIRWAYsc3c1vLam5UzrEiNBKM2mFHYXH1pLxJj8VVt31lV4Z2UdqqqdVLUTcBgwVkSuynnpTIsQQQnKnptSgS8M2DERY/JZbUHkHOBMVf0ymaCqK4Gzgcm5LJhpObzurDQtkYBridg9RYzJW7UFkaCqbqya6I6LBNPMb1qhKBBMM4BBgb8QsO4sY/JZbUGkpiFYbXhWA3jdWSFJE0QCrjvLDqwbk7dqOzvrYBHZniZdgIIclMe0QFFRQr49g0g40Aawlogx+azGIKKqNsiiqVUUCKZriQRdd5YdEzEmb2V6saEx6SUSRKSaIOJaInZ2ljH5K6dBREROEJFlIrJcRKalmR4Wkcfc9A9EpJdL7ywib4jIDhG5o8oyh4jIYrfM7VJ1rA3TuBIxogghX5qLDUPtACiLljZ2qYwxjSRnQURE/MCdwInAQOBMERlYZbbzgS2q2he4BbjRpZcD1wA/S5P1XcCFQD/3f0L2S28ylogREUl/xXqwLQA7IyWNXSpjTCPJZUtkFLBcVVe6EYBnAhOqzDMBmO6ePwkcIyKiqjtV9V28YFJJRLoBRar6vqoq8BBwSg7fg6lNIkpE0rdE/KG2FCYS7KzY1gQFM8Y0hlwGkf2Ar1Ner3ZpaedR1RiwDehcS56ra8nTNKZEnKhAyJfmsqFAAW0SSmlkR+OXyxjTKPL2wLqIXCQi80Vk/oYNNmZkziRiREUIpjnFl1A72mqCnZF0Z4kbY/JBLoPIGmD/lNc9XFraedyNrjoAm2rJs0cteQKgqveo6khVHdm1a9c6Ft1kLO51ZwXTtURCba0lYkyey2UQ+RDoJyK9RSQETAJmVZlnFjDFPT8NeN0d60hLVdcC20VktDsrazLwr+wX3WSs8uysNEEk3I42mqA0trPxy2WMaRQZ3U+kPlQ1JiKXAS8DfuB+VV0qItcD81V1FnAfMENElgOb8QINACJSDBQBIRE5BThOVT8B/ht4ECgEXnL/poloLELEJwT9oT0nhtrRNqFsslN8jclbOQsiAKr6It5dEFPTrk15Xg5MrGbZXtWkzwcGZ6+UpiFibkiTkC9dEGlLm0SCr2NljVwqY0xjydsD66ZxRF0rI+QGW9xNqC1tVSm1K9aNyVsWREyDRFwrI5i2JdKONokEOxMWRIzJVxZETINEoy6I+KtpiSSU0kSUGs6XMMa0YBZETIMkWyJpu7P8IdogKFBmx0WMyUsWREyDRNww72lbIiK0daf+lsbsDC1j8pEFEdMg0eTZWYH09yhr4/OCy86oXStiTD6yIGIaJBqrJYi4bq5Su1bEmLxkQcQ0SNTdPz0YKEw7va3fS98RtaFPjMlHFkRMg0TiyWMi6Vsi7YPe3Q1L7J4ixuQlCyKmQSLJlkgwfRDpEGwPwHYbydeYvGRBxDRI1F2NHnL3U6+qKOSCSIUFEWPykQUR0yCRRASAUDD9MZF2oSJElW0Ru7uhMfnIgohpkEg82Z2VPoj4CoooSqi1RIzJUxZETIPE4smWSNv0MxR0oCgRZ7vdZ92YvGRBxDRIJB4DIFjNMRHCRRQlEmwrr+mGlcaYlsqCiGmQqDsmEvSnubMhuJZIgpLyrY1XKGNMo7EgYhokkuzOSndnQ4CCIjrEE3aKrzF5yoKIaZCIuu6sdPdYh8qWyLaIXbFuTD6yIGIaJJKIElAl4KvmTsvumMj2WKndU8SYPGRBxDRIeSJKuKbYUNCBDvEEcRI2kq8xeciCiGmQSCJGGKl+hoIiOiYSAGyp2NJIpTLGNBYLIqZByhMx0tyOapdwEZ3jcQA2ldlpvsbkGwsipkEqtJaWiM9PF583OKMFEWPyjwUR0yAVmiBcy2bUOdAOgI1lGxujSMaYRmRBxDSI1xKpeTPaK9weATbZVevG5B0LIqZByjVBWGrejIIFHdlLfdYSMSYPWRAxDRKh9iBCuIhOasdEjMlHFkRMg5RrnLBUc6FhUkEHusQTbCy3logx+SanQUREThCRZSKyXESmpZkeFpHH3PQPRKRXyrRfufRlInJ8SnqxiCwWkUUiMj+X5Te1q0AJV3e1elJBEZ1jUWuJGJOHavn215+I+IE7gWOB1cCHIjJLVT9Jme18YIuq9hWRScCNwBkiMhCYBAwCugOviUh/VY275Y5SVftZ2wx4QaSacbMcLejI3pFy1peuJ6EJfLV1fxljWoxcfptHActVdaWqRoCZwIQq80wAprvnTwLHiIi49JmqWqGqXwLLXX6mmakpiGwvj/KHFz7hpnc30i0aJZqIcs6Dr/HRV3blujH5IpdBZD/g65TXq11a2nlUNQZsAzrXsqwCr4jIAhG5qLqVi8hFIjJfROZv2LChQW/EVK9CoMC35zDwy9fvYMId73Hfu1/SdZ/udI95o/1+uuErfnTXHG577QsbkNGYPJCz7qwcOkJV14jI3sCrIvKZqr5ddSZVvQe4B2DkyJG2t8oBVaUCCFW5IdXqLaWcde/7xBPKzIsOZ1QsxOePez2R15yyL28t7M4tr33O+pJyfj9hMD5fDVe8G2OatVy2RNYA+6e87uHS0s4jIgGgA7CppmVVNfm4HngG6+ZqMtF4BBWhwL9r9KzyaJzzH5xPWSTOwxeMZlTvTlDYqbIlsiWynlvOGMbF3/0OD3/wFX99dVlTFd8YkwW5DCIfAv1EpLeIhPAOlM+qMs8sYIp7fhrwunp9HLOASe7srd5AP2CeiLQVkfYAItIWOA5YksP3YGpQ4YZ2D6UEkT+9+CnL1pXwtx+PYMC+7b3ENp1op0p7fwFrd6xFRPjlCQM4c9QB3PnGCh6f/3W67I0xLUDOurNUNSYilwEvA37gflVdKiLXA/NVdRZwHzBDRJYDm/ECDW6+x4FPgBhwqarGRWQf4Bnv2DsB4BFV/Xeu3oOpWUWFd8vbAr83wOL84s1Mn7uKqWN7893+XXfN2KYTAN38bVi7cy0AIsLvJwzi682lXPPsEobs14GDuhU17hswxjRYTo+JqOqLwItV0q5NeV4OTKxm2T8Af6iSthI4OPslNfVR5oJIOFBAPKFc+6+ldO9QwM+O77/7jOEi8AXo7guzZuc3lckBv49bJw3jxNve4dJHFvLcZUfQNtwSD9MZ03rZCfum3kortgHQNtiWR+d9xSdrt/M/Jx1Em1CVQCAChXvRQ/2sLlm921lZXdqFuW3SML7cuJNr/mU9k8a0NBZETL0lWyIBXyE3v/o5o/t04qQh3dLP3KYzPeNQFitjQ9nup1yP+U4XLj+qL08vXMOs/3yTfnljTLNkQcTUW2nECyKffRNl884I0048CHe8ak+FnegZjQCwavuqPSZfcUw/RhzQkV8/s5jVW0pzVmZjTHZZEDH1VhYpAeCDL8s4akBXhu3fsfqZ23SiV5l3Nlfx9uI9Jgf8Pm6bNBxVuOqxRcTiiRyU2BiTbRZETL2VRnYAsL0iyJXf61/zzIV7sc/OLYT9YVZt27MlArB/pzb8/pRBfFi8hb+/uSLbxTXG5IAFEVNvW8q87qzB3btxcE2tEIC2XfCVbuKA9gek7c5KOnV4DyYM685ts79gwSobY8uY5s6CiKm3JWu+BWDiqIG1z9xuX0hE6dW2W9rurFS/P2Uw3ToUcOVjH1FSHs1CSY0xuWJBxNRLSXmUFW5gy0EH7F/L3ED7fQDoGd6L1SWriSaqDw5FBUFuPWMYa7aU8dt/Lc1KeY0xuWFBxNTLjPdXEddyChMJfOEMrjRv5wWRPv52xDTGV9u/qnH2kb06cfnR/Xj6ozX8a1HVIdeMMc2FBRFTZ6WRGPe+8yVFbRO0SSgE29S+kAsi/aUQgGWbax948fKj+3JIz734zTNL+HqznfZrTHNkQcTU2SMffMXmnRE6toM2msgsiLTfF4A+sShBX5DPtnxW6yIBv49bzxgGAhc+NJ8dFbEGltwYk20WREydlEfj/N/bKzm8T2fivihtVcCXwWYUaguh9gR3bKRvx758vvnzjNa3f6c23PnjEXyxfgdXPPoR8YTdGsaY5sSCiKmTx+d/zYaSCi4/pi/b4+V0oA43lGq/D+z4lv579eezzbW3RJLG9+/K7344iNc/W8/1zy21OyIa04xYEDEZK4/GuevNFYzsuReH9+nM9niEIvyZZ9BuXyhZx4GdDmRT+SbWl67PeNGzR/fkgiN6M33uKv735WUWSIxpJiyImIz98/1VrN1WzlXH9kdE2KZRiiRY+4JJ7feFkm8YvvdwABasW1Cn9f/P9w/izFEH8Pc3V3CL3aPdmGbBgojJyPbyKHe8sZzx/bsytm8XVJXtGqPIH8o8k716wrbVDOjYl3bBdnz47Yd1KoPPJ/zhlMFMPKQHt8/+gt8994kdIzGmidkdgExG/u+tFWwtjfLLEwYAUB4vJypQ5G+beSYde0IiRmDHOkbsM4J5386rczl8PuHGHw2lQ2GQe9/9knXby/nr6QfveQ8TY0yjsJaIqdU3W8u4790vmTCsO4O6dwBgu7uXSFGofeYZ7dXLe9xSzBH7HcGq7av4YssXdS6Pzyf85uSB/Oakg/j30m859c45rNiwo875GGMazoKIqdVvZy1FEH5+/IDKtK0VWwHoGO6YeUZ79fQetxRzXM/j8Iufl758qd7lumBcH6afN4r1JeVMuOM9nv1ojR0nMaaRWRAxNXpl6be8+sk6rvxeP3rsteuiwg071gKwd2HXzDMr6gHihy2r6FzYmcO7H86zy58lEo/Uu3zj+3flhSvG0X+fdlz52CIufGgB67aX1zs/Y0zdWBAx1dpaGuHafy3lwH3bM/WI3rtN2+CGc+/adt/MM/QHoOP+sHklAJMHTmZD2QaeW/Fcg8rZvWMhT1w8hl9//yDe+WIDx978Fg++9yVRu7GVMTlnQcSkpar84smP2bSzgv897WCC/t03lfXbvwagS/v96pZx14Ng/ScAjO42moGdB3Lv4nuJxhs25LvfJ1w4vg8v/WQcg/frwHXPfcLxt77N7E/XWReXMTlkQcSkde87X/LKJ+v45QkHMqRHhz2mbyhZQ4d4nHDHA+qW8b5DYOPnEC1DRLh02KWs3rGap794Oivl7tO1HQ9fcBj3Th4JwPnT5/Nfd82xYGJMjlgQMXt4cfFa/vjSp5w4eF/Or9KNlbRmxzd0i8WhqHvdMt93MGgC1n8KwLj9xjFi7xHcvehOSr9+v6FFB0BE+N7AfXj5yvH84dTBbCip4Pzp8/n+7e/y5ILVlEfjWVmPMcaCiKnipcVruXLmIkYcsBe3nDEMkfRjYxWXr6d3NArt6xhEuh3sPa6eD3g7/Ct7fp+NFVt45MmJ8OpvG1L83QT9Ps46rCdv/OxIbpp4MNF4gp898R8O++Nsfv/8JyxfX5K1dRnTWlkQMYB3DOTed1Zy6SMLGdKjA/dPOZSCYPpxscpj5ayJ7aCXvw0E6nDFOnjXiuzVG1bM9l7HIgx/8zaOrEhwf6fObJt7O3z2YsPeTBVBv4/TDunBq1eN55ELD+OIvl2YPqeY7938Nife9g5/f3O53a/EmHqyy3wNKzfs4HfPfcJbn2/guIH7cOukYTVeAb5442IUGNQ2g9viptP3e7DoYSjfBnP/DuuXcvkPb+a0xbdxX/fv8NN/XQrd53hjbX32ghdwen8XBk6AlJZRNBHlw7Ufsm7rCvYqnktXfyGJgRPYEQhSGCjk4K4H79aSEhHGfKcLY77ThfUl5Tz/n7U8//E3/OXfy/jLv5cxYJ/2jO/fhfH9u3Jor07VBlFjzC45DSIicgJwG+AH7lXVP1eZHgYeAg4BNgFnqGqxm/Yr4HwgDlyhqi9nkqfJTCKhLPxqC4/M+4pZi74hHPBx/YRBnDO6Z7VdWEnvFr+KT5Vh3UfXb+XDz4YP/wFPnAsr34KhZ9B/xPn8oHQl//zyRY4nyqBHJ0GoHax6F/whmH8/DJ0EP7gNggUsXLeQ6+dez4ptK3bPe90rlU8P7nowfzjiD/Qs6rlHEfZuX8DUI3oz9YjefL25lJeWrOWtzzcwfc4q/vHOl4QCPgZ1L+LgHh0ZfkBHBnUvYv9ObQgHLLAYk0pydcaKiPiBz4FjgdXAh8CZqvpJyjz/DQxV1YtFZBJwqqqeISIDgUeBUUB34DWgv1usxjzTGTlypM6fPz+r768lUFXKowm2lUXZWhbh681lrNywg8VrtjF3xSY27YzQLhzgRyP247Kj+9G1fbjWPNfsWMOkZyYwrGQLfzv16V3HOOrqxV/AvP/zlp/yHBR0YEv5Fs54/gwqKrZx3bffMi4RIvDdX8CIKfDuLeibf2R194P5Z7/DePSrl+kWbM9V365mkBSy7fgbWB/bgf+tv9CuvISVw0/nlg1zicajTB0ylYn9J9KlsEutxSqNxPhg5WbmrNjIf77exuI12yhzB+J9Aj32akOvLm3ZtyhM53ZhOrcN0bFNiFDAR8jvIxQQRIRILEEklqDCPZZF45RH45RGYpRFvNdlkRhl0TilkeS0OGXROJFYgqA/mZ/3Xxj0Uxjy0ybkp00o4B5rfl4Y8tM27KdN0HseCmS/9zqeUKLxBLGEprzneOV7r3CvK6rUR+o86V97jyLsVg+pz8MBP+GAj4Kgn4Kg97og6L2umh5OSQ/5fbX+UDIeEVmgqiNrnCeHQeRw4DpVPd69/hWAqv4pZZ6X3TxzRSQAfAt0Baalzpuczy1WY57p1DeIXDD9Q77cuBOAylrSXQ/JuktOUwV1r5LVmlq9Gc2/2zLppqXJo0pa8klF3PsiVrVPUZjD+3RmXL+unDB4X/6y4Pd8tGYOiR3rSKAk8Jp/Cah8nUzbKVCoyj/DB9L3x0+lrbeMbV8L7fYG365f98Xbirn89csp3l5M2B+mY7gjIX+IingFpRXb2BGvwK/KaSU7+OnmrbTZfzRMfLDy9ruUfOu1cL6ay7pQAX/p0pVXwt4OY68EtEHwA35lz9tpFRRBm86VL4O+IDNPepxl60r4Yt0OVm7cyZcbd/Llxh1sKKlg044IsTqOIuwTKnfwhUFvh1/gHguDfgpC3o4uFtfKHWsknqA8mqA04gWh0kicskicnZEYdfn6Bv3i1hkg4M98J6rqBYtYIkE07oJGXIkmEnVaf038PvF28AFf5WPI70OhMjhF3PYciSXqXO+pRKgMMuGAD58IApWBRcT9I+7RmyYAKa9biheuOKLeLehMgkguu7P2A75Oeb0aOKy6eVQ1JiLbgM4u/f0qyyavaqstTwBE5CLgIoADDqjjtQxOz85tvcp320tys6nc2NjVRZ86rXLzqpyWsnFWzrdrWuXzKgvWOn+V8lSdLxgQOhQGK//361hIny7t6NBm93uAdGvbjR3te+KvKMeH96Xy4cMvPgTxHsV77OAL8YNuY9n/sMtrrb9aFXXbI6lXh148PeFp3vjqDRZvXMzWiq1E4hHC/jAFgQL6FOzNuNKd7BdPQLdh0OfI3Y6T0H5fOPdFWDGbfYrf4a87N7K8YjPvxLbytVZQToIYSpw0O6HCvaFjr8qXfp+fgN/HoO4dKgeeTKWqbC+PsbU0QjSeIBLzdrBxVUJ+HwVBHyG/v7IlURDK7q9gVaUilmBnRayyFbOzIuYCTLLV4z0vi8TcozdPvI57/4BPCPh9BH1C0O/znvuFgM9HwC8E/ULI7yMc9LtHX9rXyTrZNd17DPjr1kpKJLSylVMeTVAe9Vo75a7Fl3xemRZLUJEmvSKaIKFa+eNNUdwfulv6rtfpNp3mTPb8uZRVeXtgXVXvAe4BryVSnzyuOXlgVsvUXF188MVQz16pXAj6ghzX6ziO63Vc/TLw+aDfsd4/0Nf9Z5vIriDdFETEddn46Vz77HnF5xOvNReyY1RNLZen+K4BUk/f6eHS0s7jurM64B1gr27ZTPI0xhjTSHIZRD4E+olIbxEJAZOAWVXmmQVMcc9PA15Xr4N/FjBJRMIi0hvoB8zLME9jjDGNJGfdWe4Yx2XAy3in496vqktF5HpgvqrOAu4DZojIcmAzXlDAzfc48AkQAy5V1ThAujxz9R6MMcbULGdnZzUnrfUUX2OMaYhMzs6yYU+MMcbUmwURY4wx9WZBxBhjTL1ZEDHGGFNvreLAuohsAFalJHUBNjZRcerKypobVtbcsLLmTlOUt6eqdq1phlYRRKoSkfm1nXHQXFhZc8PKmhtW1txpruW17ixjjDH1ZkHEGGNMvbXWIHJPUxegDqysuWFlzQ0ra+40y/K2ymMixhhjsqO1tkSMMcZkgQURY4wx9Zb3QURErhORNSKyyP1/P2Xar0RkuYgsE5HjU9JPcGnLRWRaI5b1f0XkMxH5WESeEZGOLr2XiJSlvIe7U5Y5REQWu7LeLk10386mqrMayrO/iLwhIp+IyFIR+YlLr/P20EjlLXaf4yIRme/SOonIqyLyhXvcy6WL+6yXu21lRCOWc0BK3S0Ske0icmVzqVcRuV9E1ovIkpS0OtejiExx838hIlPSrStHZW15+wBVzet/vHuz/yxN+kDgP0AY6A2swBte3u+e9wFCbp6BjVTW44CAe34jcKN73gtYUs0y84DReHfLfQk4sQnquMnqrIYydQNGuOftgc/dZ16n7aERy1sMdKmS9hdgmns+LWV7+L77rMV99h80UR37gW+Bns2lXoHxwIjU70td6xHoBKx0j3u553s1Ullb3D4g71siNZgAzFTVClX9ElgOjHL/y1V1papGgJlu3pxT1VdUNeZevo9358ZqiUg3oEhV31dva3oIOCW3pUyryeqsOqq6VlUXuuclwKfAfjUsUt320JQmANPd8+ns+mwnAA+p532go9sWGtsxwApVXVXDPI1ar6r6Nt69iaqWoS71eDzwqqpuVtUtwKvACY1R1pa4D2gtQeQy1zy8P9mUxduhfJ0yz2qXVl16Y5uK96siqbeIfCQib4nIOJe2H175kpqqrM2lztISkV7AcOADl1SX7aGxKPCKiCwQkYtc2j6qutY9/xbYxz1v6rImTQIeTXndHOsV6l6PzaHM0EL2AXkRRETkNRFZkuZ/AnAX8B1gGLAW+GszLmtynl/j3dHxYZe0FjhAVYcDPwUeEZGixi99yyMi7YCngCtVdTvNbHtIcYSqjgBOBC4VkfGpE92vzGZzPr54t6f+IfCES2qu9bqb5laP1WlJ+4Cc3R63Manq9zKZT0T+ATzvXq4B9k+Z3MOlUUN6g9VWVhE5FzgZOMZt8KhqBVDhni8QkRVAf1eu1OZuVstaBzXVZZMRkSBeAHlYVZ8GUNV1KdMz3R5yTlXXuMf1IvIMXpfPOhHppqprXbfF+uZQVudEYGGyPptrvTp1rcc1wJFV0t9shHICLW8fkBctkZpU6Ss+FUieCTELmCQiYRHpDfTDO0D1IdBPRHq7X1uT3LyNUdYTgF8AP1TV0pT0riLid8/7uLKudE307SIy2p2RMRn4V2OUtYomq7PquPq4D/hUVW9OSa/r9tAYZW0rIu2Tz/EOri5xZUqeGTSFXZ/tLGCyO7toNLAtpbumsZxJSldWc6zXFHWtx5eB40RkL9ctd5xLy7kWuQ9ozKP4TfEPzAAWAx/jbTTdUqb9Gu9skWWknNGAd9bG527arxuxrMvx+mIXuf+7XfqPgKUubSHwg5RlRuJ9YVcAd+BGIWiCem6SOquhPEfgdVt8nFKf36/P9tAIZe2DdwbTf9zn/GuX3hmYDXwBvAZ0cukC3OnKuhgY2ch12xbYBHRISWsW9YoX2NYCUbzjA+fXpx7xjkcsd//nNWJZW9w+wIY9McYYU295351ljDEmdyyIGGOMqTcLIsYYY+rNgogxxph6syBijDGm3iyIGJMlIrKviMwUkRVu+JIXRaR/FvM/UkTGZCs/Y7LBgogxWeAu9HoGeFNVv6OqhwC/Ytc4TdlwJGBBxDQrFkSMyY6jgKiqVt7nQVX/A7wr3j0ilrh7PpwBla2K5NAgiMgdbriL5L1FficiC90yB7pBJC8GrhLvfhLjMKYZyIuxs4xpBgYDC9Kk/xfeoIQHA12AD0Xk7Qzy26iqI0Tkv/Hu03GBeDci2qGqN2Wr0MY0lLVEjMmtI4BHVTWu3iCFbwGHZrDc0+5xAd4NiYxpliyIGJMdS4FD6jB/jN2/fwVVple4xzjWY2CaMQsixmTH60A45YZSiMhQYCtwhoj4RaQr3i1R5wGrgIFudNuOeHcJrE0J3q1+jWk27BeOMVmgqioipwK3isgvgXK8+6ZfCbTDG6FXgV+o6rcAIvI43uirXwIfZbCa54An3Q3MLlfVd7L9PoypKxvF1xhjTL1Zd5Yxxph6syBijDGm3iyIGGOMqTcLIsYYY+rNgogxxph6syBijDGm3iyIGGOMqbf/D900x0PNNSGkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = data['word_count'].plot(kind='kde')\n",
    "data['verb_count'].plot(kind='kde', ax=ax)\n",
    "data['noun_count'].plot(kind='kde', ax=ax)\n",
    "\n",
    "ax.legend(['Word Count', 'Verb Count', 'Noun Count'])\n",
    "ax.set_title('Distribution of Word Count, Verb Count, and Noun Count')\n",
    "ax.set_xlabel('Count')\n",
    "ax.set_ylabel('Density')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these comments on average are quite short in length and contain more nouns than verbs on average.\n",
    "\n",
    "Since we have not done any cleaning of the data yet these distributions are not exact as the nltk package is not currently looking for misspelled words or different versions of word spellings which are used online sometimes.\n",
    "\n",
    "For example if a user knows that the platform they are on has limitations on language than they may spell a profane word to try to fool any auto detecting systems such as `Fuck==>Fxck, F*ck, Fukk, Fuuu*uukk`, etc.\n",
    "\n",
    "Therefore these counts will not detect all nouns and verbs but should give a decent sample.\n",
    "\n",
    "Knowing the underlying distributions of some of these features is important because after the synthetic data is generated we would most likely want it to follow the same distributions for these attributes of the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the most common N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(('', ''), 56)</td>\n",
       "      <td>((., If, you), 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((,, and), 43)</td>\n",
       "      <td>(((, talk, )), 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>((., I), 43)</td>\n",
       "      <td>((,, and, I), 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>((``, ''), 41)</td>\n",
       "      <td>((., '', ''), 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((of, the), 24)</td>\n",
       "      <td>((Answere, :, -), 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>((in, the), 20)</td>\n",
       "      <td>(('', '', and), 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>((., The), 20)</td>\n",
       "      <td>(('', and, ``), 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>((I, have), 20)</td>\n",
       "      <td>(('', '', .), 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>((,, I), 19)</td>\n",
       "      <td>(('', '', ''), 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>((,, but), 19)</td>\n",
       "      <td>((Communist, Party, of), 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>((., ''), 16)</td>\n",
       "      <td>((., It, 's), 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>((you, are), 16)</td>\n",
       "      <td>((., Also, ,), 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>((to, do), 15)</td>\n",
       "      <td>((., The, fact), 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>((., It), 15)</td>\n",
       "      <td>((and, ``, ''), 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>((to, be), 15)</td>\n",
       "      <td>((the, word, ``), 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>((to, the), 15)</td>\n",
       "      <td>((word, ``, ''), 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>((going, to), 14)</td>\n",
       "      <td>((to, contact, me), 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>((., He), 14)</td>\n",
       "      <td>((., '', Lynn), 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>((it, is), 14)</td>\n",
       "      <td>(('', Lynn, Porter), 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>((., If), 14)</td>\n",
       "      <td>((Lynn, Porter, ''), 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>((do, n't), 14)</td>\n",
       "      <td>((!, !, !), 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>((should, be), 12)</td>\n",
       "      <td>((I, do, n't), 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>((you, .), 12)</td>\n",
       "      <td>((., I, have), 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>((is, not), 12)</td>\n",
       "      <td>((Party, of, Australia), 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>((,, you), 12)</td>\n",
       "      <td>(('', '', Answere), 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>((I, was), 12)</td>\n",
       "      <td>(('', Answere, :), 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>((it, 's), 11)</td>\n",
       "      <td>((I, was, the), 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>((., And), 11)</td>\n",
       "      <td>((it, 's, not), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>((,, the), 11)</td>\n",
       "      <td>((,, but, the), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>((for, the), 11)</td>\n",
       "      <td>((If, you, want), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>((., i), 11)</td>\n",
       "      <td>((you, want, to), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>((in, a), 10)</td>\n",
       "      <td>((., How, is), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>((is, a), 10)</td>\n",
       "      <td>((,, as, is), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>((the, same), 10)</td>\n",
       "      <td>((as, is, the), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>((and, the), 10)</td>\n",
       "      <td>((not, prevented, from), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>((I, am), 10)</td>\n",
       "      <td>((prevented, from, discussing), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>((and, I), 10)</td>\n",
       "      <td>((from, discussing, or), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>((you, have), 10)</td>\n",
       "      <td>((discussing, or, proposing), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>((,, it), 9)</td>\n",
       "      <td>((or, proposing, changes), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>((it, .), 9)</td>\n",
       "      <td>(('', '', ,), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>((on, the), 9)</td>\n",
       "      <td>((he, blocked, me), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>((that, the), 9)</td>\n",
       "      <td>((``, '', the), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>((that, I), 9)</td>\n",
       "      <td>((be, able, to), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>((If, you), 9)</td>\n",
       "      <td>((also, do, n't), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>((., This), 9)</td>\n",
       "      <td>((I, think, the), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>((have, a), 9)</td>\n",
       "      <td>((you, ., i), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>((., You), 9)</td>\n",
       "      <td>((WANK, THE, DOG), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>((does, n't), 8)</td>\n",
       "      <td>((a, lot, of), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>((this, is), 8)</td>\n",
       "      <td>((n't, have, a), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>((want, to), 8)</td>\n",
       "      <td>((have, a, problem), 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bigrams                            trigrams\n",
       "0       (('', ''), 56)                   ((., If, you), 8)\n",
       "1       ((,, and), 43)                   (((, talk, )), 8)\n",
       "2         ((., I), 43)                    ((,, and, I), 7)\n",
       "3       ((``, ''), 41)                    ((., '', ''), 6)\n",
       "4      ((of, the), 24)                ((Answere, :, -), 6)\n",
       "5      ((in, the), 20)                  (('', '', and), 5)\n",
       "6       ((., The), 20)                  (('', and, ``), 5)\n",
       "7      ((I, have), 20)                    (('', '', .), 5)\n",
       "8         ((,, I), 19)                   (('', '', ''), 5)\n",
       "9       ((,, but), 19)         ((Communist, Party, of), 5)\n",
       "10       ((., ''), 16)                    ((., It, 's), 4)\n",
       "11    ((you, are), 16)                   ((., Also, ,), 4)\n",
       "12      ((to, do), 15)                 ((., The, fact), 4)\n",
       "13       ((., It), 15)                  ((and, ``, ''), 4)\n",
       "14      ((to, be), 15)                ((the, word, ``), 4)\n",
       "15     ((to, the), 15)                 ((word, ``, ''), 4)\n",
       "16   ((going, to), 14)              ((to, contact, me), 4)\n",
       "17       ((., He), 14)                  ((., '', Lynn), 4)\n",
       "18      ((it, is), 14)             (('', Lynn, Porter), 4)\n",
       "19       ((., If), 14)             ((Lynn, Porter, ''), 4)\n",
       "20     ((do, n't), 14)                      ((!, !, !), 4)\n",
       "21  ((should, be), 12)                   ((I, do, n't), 4)\n",
       "22      ((you, .), 12)                   ((., I, have), 4)\n",
       "23     ((is, not), 12)         ((Party, of, Australia), 4)\n",
       "24      ((,, you), 12)              (('', '', Answere), 4)\n",
       "25      ((I, was), 12)               (('', Answere, :), 4)\n",
       "26      ((it, 's), 11)                  ((I, was, the), 4)\n",
       "27      ((., And), 11)                  ((it, 's, not), 3)\n",
       "28      ((,, the), 11)                  ((,, but, the), 3)\n",
       "29    ((for, the), 11)                ((If, you, want), 3)\n",
       "30        ((., i), 11)                ((you, want, to), 3)\n",
       "31       ((in, a), 10)                   ((., How, is), 3)\n",
       "32       ((is, a), 10)                    ((,, as, is), 3)\n",
       "33   ((the, same), 10)                  ((as, is, the), 3)\n",
       "34    ((and, the), 10)         ((not, prevented, from), 3)\n",
       "35       ((I, am), 10)  ((prevented, from, discussing), 3)\n",
       "36      ((and, I), 10)         ((from, discussing, or), 3)\n",
       "37   ((you, have), 10)    ((discussing, or, proposing), 3)\n",
       "38        ((,, it), 9)       ((or, proposing, changes), 3)\n",
       "39        ((it, .), 9)                    (('', '', ,), 3)\n",
       "40      ((on, the), 9)              ((he, blocked, me), 3)\n",
       "41    ((that, the), 9)                  ((``, '', the), 3)\n",
       "42      ((that, I), 9)                 ((be, able, to), 3)\n",
       "43      ((If, you), 9)                ((also, do, n't), 3)\n",
       "44      ((., This), 9)                ((I, think, the), 3)\n",
       "45      ((have, a), 9)                    ((you, ., i), 3)\n",
       "46       ((., You), 9)               ((WANK, THE, DOG), 3)\n",
       "47    ((does, n't), 8)                   ((a, lot, of), 3)\n",
       "48     ((this, is), 8)                 ((n't, have, a), 3)\n",
       "49     ((want, to), 8)             ((have, a, problem), 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text into words\n",
    "data['words'] = data['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Get bigrams and trigrams for each row\n",
    "data['bigrams']   = data['words'].apply(lambda x: list(ngrams(x, 2)))\n",
    "data['trigrams']  = data['words'].apply(lambda x: list(ngrams(x, 3)))\n",
    "# data['quadgrams'] = data['words'].apply(lambda x: list(ngrams(x, 4)))\n",
    "\n",
    "# Count the occurrences of bigrams and trigrams\n",
    "bigram_counts   = Counter([gram for grams in data['bigrams'] for gram in grams])\n",
    "trigram_counts  = Counter([gram for grams in data['trigrams'] for gram in grams])\n",
    "# quadgram_counts = Counter([gram for grams in data['quadgrams'] for gram in grams])\n",
    "\n",
    "# Get the most common bigrams, trigrams, and quadgrams\n",
    "most_common_bigrams   = bigram_counts.most_common(50)\n",
    "most_common_trigrams  = trigram_counts.most_common(50)\n",
    "# most_common_quadgrams = quadgram_counts.most_common(50)\n",
    "\n",
    "df_common_grams = pd.DataFrame()\n",
    "df_common_grams['bigrams']   = most_common_bigrams\n",
    "df_common_grams['trigrams']  = most_common_trigrams\n",
    "# df_common_grams['quadgrams'] = most_common_quadgrams\n",
    "\n",
    "# # Display the results\n",
    "# print('Most common bigrams:')\n",
    "# for bigram, count in most_common_bigrams:\n",
    "#     print(' '.join(bigram), count)\n",
    "\n",
    "# print('\\nMost common trigrams:')\n",
    "# for trigram, count in most_common_trigrams:\n",
    "#     print(' '.join(trigram), count)\n",
    "    \n",
    "# print('\\nMost common quadgrams:')\n",
    "# for quadgram, count in most_common_quadgrams:\n",
    "#     print(' '.join(quadgram), count)\n",
    "\n",
    "\n",
    "df_common_grams.iloc[:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the initial 10 or so most common bi-grams and tri-grams are repetitive punctuation marks.\n",
    "\n",
    "Traditionally these would be cleaned and removed when training models for NLP tasks, however due to the nature of this work many of these traditional techniques will limit the models ability to predict toxicity as well as with clean text.\n",
    "\n",
    "I happened to have competed in this competition and one thing all of us learned was that leaving capital letters and punctuation improved the models ability to infer toxicity and especially levels of toxicity. \n",
    "\n",
    "For example a phrase such as:\n",
    "\n",
    "`Are you kidding?`\n",
    "\n",
    "Conveys a much different meaning than the same words but put this way:\n",
    "\n",
    "`ARE YOU KIDDING!!!??`\n",
    "\n",
    "Traditional NLP techniques would have us convert all characters to lower case and remove punctuation so the model will interpret both of those texts the exact same way.\n",
    "\n",
    "When training sentiment based models or models where feeling and emotion is being conveyed in some way such as toxicity of comments, it is more than just the raw content of the words alone which gives the meaning. The puncuation and capitalizations are very expressive forms of language and as such for these problems do better left in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "* First we need load in our text column as tensorflow formatted dataset\n",
    "\n",
    "* Next we shuffle the data to avoid any patterns which may have been present\n",
    "\n",
    "* We then slice the data into batches for processing\n",
    "\n",
    "* Vectorize the text which will be used to create a corpus of vocabulary used when training and act as vector representations of our text\n",
    "\n",
    "* Create the corpus of vocabulary which is used to train and evaluate throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = 100000  ## Only consider the top 20k words\n",
    "maxlen = 80  ## Max sequence length\n",
    "batch_size = 128  ## Data loading batch sizes\n",
    "\n",
    "# Create a dataset from the pandas column\n",
    "text_column = text_column.astype(str)  # Convert all elements to strings\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(text_column)\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "text_ds = text_ds.shuffle(buffer_size=128)\n",
    "text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "# def custom_standardization(input_string):\n",
    "#     \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
    "#     lowercased = tf.strings.lower(input_string)\n",
    "#     stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "#     return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
    "\n",
    "\n",
    "## Create a vectorization layer and adapt it to the text\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=None,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  ## To get words back from token indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Labels\n",
    "\n",
    "Since we are building a generative auto-regressive model, we must train it to predict the next word by looking backwards and using the previous tokens to predict the highest probability for the next token.\n",
    "\n",
    "This is fairly easy to create labels for because we simply shuffle the `TRUE` data be one token and then when training the model compares the predicted text with the next indexed word.\n",
    "\n",
    "We can inspect what these samples and labels look like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to create target column\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
    "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "\" I already gave you the source for all my edits The Slur database, so I will not continue to play your little game. If you weren't so lazy and intent on harassment, you could use Google to search for [UNK] and [UNK] You get more than 400 hits including white supremacist sites and an academic paper dating to 1996. It's obviously a real slur with some usage. Nice try at being obdurate though. I'm sure there's a slur that\n",
      "\n",
      "Target Sequence:\n",
      "I already gave you the source for all my edits The Slur database, so I will not continue to play your little game. If you weren't so lazy and intent on harassment, you could use Google to search for [UNK] and [UNK] You get more than 400 hits including white supremacist sites and an academic paper dating to 1996. It's obviously a real slur with some usage. Nice try at being obdurate though. I'm sure there's a slur that describes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "\" I'd say Zivo [UNK] is not heavy metal but hard rock, although since there are no good Croatian heavy metal bands it's not an important mistake. In my opinion, they are really fucking great. The songs [UNK] [UNK] \"\"Kill Yourself\"\" and [UNK] are my favourites. \"                                 \n",
      "\n",
      "Target Sequence:\n",
      "I'd say Zivo [UNK] is not heavy metal but hard rock, although since there are no good Croatian heavy metal bands it's not an important mistake. In my opinion, they are really fucking great. The songs [UNK] [UNK] \"\"Kill Yourself\"\" and [UNK] are my favourites. \"                                  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "3RR RULE That's FOUR for you today already at Saudi Arabia, Yuber. I'm assuming good faith and not going to report you on it yet. Knock it off.                                                    \n",
      "\n",
      "Target Sequence:\n",
      "RULE That's FOUR for you today already at Saudi Arabia, Yuber. I'm assuming good faith and not going to report you on it yet. Knock it off.                                                     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "I confess to having complete (and apparently blissful) ignorance of Jordan, but I've glanced at the article. Is this a woman or a soap opera!?. I don't think there was much to change in terms of the description of the various diseases. It is mentioned that she is famous for the size of her breasts: am I correct in assuming this is because they are grotesquely large rather than vanishingly small? [UNK] 11 Jul 2003 (UTC)    \n",
      "\n",
      "Target Sequence:\n",
      "confess to having complete (and apparently blissful) ignorance of Jordan, but I've glanced at the article. Is this a woman or a soap opera!?. I don't think there was much to change in terms of the description of the various diseases. It is mentioned that she is famous for the size of her breasts: am I correct in assuming this is because they are grotesquely large rather than vanishingly small? [UNK] 11 Jul 2003 (UTC)     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "Please just let us know what you found. I'm not asking for details. Just let u know if the same IP address was used or what ever else you found that caused you to determien that he was usign sockpuppets. No details, jsut what was it? I am one of [UNK] most hated enemies, so I think I'm the perfect person to ask this. Vandalism from an IP address used by Deskana was revealed using checkuser adn he was able\n",
      "\n",
      "Target Sequence:\n",
      "just let us know what you found. I'm not asking for details. Just let u know if the same IP address was used or what ever else you found that caused you to determien that he was usign sockpuppets. No details, jsut what was it? I am one of [UNK] most hated enemies, so I think I'm the perfect person to ask this. Vandalism from an IP address used by Deskana was revealed using checkuser adn he was able to\n"
     ]
    }
   ],
   "source": [
    "## Select samples from the training data set to inspect\n",
    "sample = text_ds.take(5) \n",
    "\n",
    "## Display some samples\n",
    "for x, y in sample:\n",
    "    # Convert token indices back to words\n",
    "    input_words  = [vocab[i] for i in x[0].numpy()]\n",
    "    target_words = [vocab[i] for i in y[0].numpy()]\n",
    "\n",
    "    print(\"\\n\\n\\n\\nInput Sequence:\")\n",
    "    print(\" \".join(input_words))\n",
    "    print(\"\\nTarget Sequence:\")\n",
    "    print(\" \".join(target_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***We can see that the target or label sequence is merely our ground truth text sequence we have just shifted by `1` token. This is what our model will use to evaluate during training.***\n",
    "\n",
    "* ***Cell below was for loading in and preprocessing the IMBD movie quotes dataset. This is the dataset I tested this approach on first.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Transformer Block and Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Creates a mask for causal (auto-regressive) self-attention. The returned mask has the shape \n",
    "    [batch_size, n_dest, n_src], where each entry at position (i, j, k) will be 1 if j >= k and 0 otherwise. \n",
    "    This is used to prevent the attention mechanism from attending to future positions during the forward pass.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of sequences in each batch.\n",
    "        n_dest (int): Number of destination attention heads.\n",
    "        n_src (int): Number of source attention heads.\n",
    "        dtype (tf.DType): Type of the output tensor.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: A tensor of shape [batch_size, n_dest, n_src] representing the mask.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create two range tensors i and j, where i has shape [n_dest, 1] and j has shape [n_src]\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "\n",
    "    # Create a mask where entry (i, j) is True if i >= j - n_src + n_dest and False otherwise\n",
    "    m = i >= j - n_src + n_dest\n",
    "\n",
    "    # Cast the mask to the desired data type\n",
    "    mask = tf.cast(m, dtype)\n",
    "\n",
    "    # Reshape the mask to have shape [1, n_dest, n_src]\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "\n",
    "    # Create a tensor with shape [2] that represents the multiples for tiling\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "\n",
    "    # Tile the mask tensor to have shape [batch_size, n_dest, n_src]\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    A Transformer block that includes multi-head self-attention and a feed-forward neural network.\n",
    "    Each of these two components has a residual connection and is followed by layer normalization.\n",
    "\n",
    "    Attributes:\n",
    "        att (layers.MultiHeadAttention): Multi-head self-attention layer.\n",
    "        ffn (keras.Sequential): Feed-forward neural network.\n",
    "        layernorm1 (layers.LayerNormalization): Layer normalization after the self-attention.\n",
    "        layernorm2 (layers.LayerNormalization): Layer normalization after the feed-forward network.\n",
    "        dropout1 (layers.Dropout): Dropout layer after the self-attention.\n",
    "        dropout2 (layers.Dropout): Dropout layer after the feed-forward network.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1,**kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer block.\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int): Dimensionality of the input embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            ff_dim (int): Number of units in the hidden layer of the feed-forward network.\n",
    "            rate (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer block.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): Input tensor of shape [batch_size, seq_len, embed_dim].\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor of shape [batch_size, seq_len, embed_dim].\n",
    "        \"\"\"\n",
    "        # Compute the shapes\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "\n",
    "        # Create the causal mask for the multi-head self-attention\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "\n",
    "        # Compute the output of the multi-head self-attention\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "\n",
    "        # Apply dropout to the attention output\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "\n",
    "        # Add the attention output to the inputs (residual connection) and normalize the result\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "\n",
    "        # Compute the output of the feed-forward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "\n",
    "        # Apply dropout to the feed-forward output\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "\n",
    "        # Add the feed-forward output to the previous output (residual connection) and normalize the result\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self): # 5\n",
    "        config = super().get_config()\n",
    "        # save constructor args\n",
    "        config['embed_dim'] = self.embed_dim\n",
    "        config['num_heads'] = self.num_heads\n",
    "        config['ff_dim'] = self.ff_dim\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Embedding layer\n",
    "\n",
    "***Create two separate embedding layers:***\n",
    "\n",
    "1) One for tokens \n",
    "\n",
    "2) One for token indices(positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    \"\"\"\n",
    "    Layer for combining token and positional embeddings. Token embeddings provide the model\n",
    "    with understanding of the meaning of each token, while positional embeddings provide\n",
    "    information about the position of each token in the sequence.\n",
    "\n",
    "    Attributes:\n",
    "        token_emb (layers.Embedding): Token embedding layer.\n",
    "        pos_emb (layers.Embedding): Position embedding layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, name=None, **kwargs):\n",
    "        super(TokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        \n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the TokenAndPositionEmbedding layer.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): Input tensor of shape [batch_size, seq_len].\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor of shape [batch_size, seq_len, embed_dim], resulting from\n",
    "            adding token embeddings and position embeddings.\n",
    "        \"\"\"\n",
    "        # Compute the maximum sequence length\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "\n",
    "        # Create a range tensor representing positions\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "\n",
    "        # Compute the position embeddings\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        # Compute the token embeddings\n",
    "        x = self.token_emb(x)\n",
    "\n",
    "        # Add the token embeddings and position embeddings\n",
    "        return x + positions\n",
    "    \n",
    "    def get_config(self): # 5\n",
    "        config = super().get_config()\n",
    "        # save constructor args\n",
    "        config['maxlen'] = self.maxlen\n",
    "        config['vocab_size'] = self.vocab_size\n",
    "        config['embed_dim'] = self.embed_dim\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Mini GPT\n",
    "\n",
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vocab_size = 30000  # Only consider the top 20k words\n",
    "maxlen = 80  # Max sequence size\n",
    "embed_dim = 256  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow_addons.optimizers import AdamW\n",
    "from keras.optimizers import SGD\n",
    "def MiniGPT():\n",
    "    \"\"\"\n",
    "    Constructs a mini version of the GPT model. The architecture is comprised of a\n",
    "    token and position embedding layer followed by a single Transformer block. The final\n",
    "    layer is a dense layer with softmax activation for prediction. \n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Mini GPT model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input layer expects inputs of shape (maxlen,) with type int32\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "\n",
    "    # Create the token and position embedding layer and compute the embeddings\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "\n",
    "    # Create the Transformer block and compute its output\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "\n",
    "    # Final dense layer with size equal to the vocabulary size\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "\n",
    "    # Construct the Keras model\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "\n",
    "    # Loss function for the training \n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Model compilation: use Adam optimizer and the defined loss function\n",
    "    # Note that we specify `None` for the second loss to not optimize based on the Transformer block's output\n",
    "    sgd_optimizer = SGD(lr=0.001)\n",
    "    model.compile(sgd_optimizer, loss=[loss_fn, None])\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {
    "a896bbaf-c33d-4700-b0b2-dc7aabd2e598.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "***Shout out to this amazing app: https://netron.app/***\n",
    "***You simply drag and drop your tensorflow or pytorch model into the app and it generates the diagram for you!***\n",
    "\n",
    "![Screen Shot 2023-06-25 at 6.55.38 PM.png](attachment:a896bbaf-c33d-4700-b0b2-dc7aabd2e598.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Text Generator Callback Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A callback to generate text from a trained model at the end of each epoch. It uses the model's \n",
    "    predictions to sample a token, add it to the input, and generate subsequent tokens.\n",
    "\n",
    "    Attributes:\n",
    "        max_tokens (int): The number of tokens to be generated after the prompt.\n",
    "        start_tokens (list): The token indices for the starting prompt.\n",
    "        index_to_word (list): Mapping from token indices to words, obtained from the TextVectorization layer.\n",
    "        k (int): Number of token predictions to consider for sampling the next token.\n",
    "        print_every (int): Frequency of print for the generated text (in number of epochs).\n",
    "    \"\"\"\n",
    "    def __init__(self, max_tokens, start_tokens, index_to_word, top_k=20, print_every=1,**kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the TextGenerator callback.\n",
    "\n",
    "        Args:\n",
    "            max_tokens (int): Maximum number of tokens to be generated.\n",
    "            start_tokens (list): List of integers representing the starting tokens.\n",
    "            index_to_word (list): List of strings representing the mapping from indices to words.\n",
    "            top_k (int, optional): Number of top token predictions to sample from. Defaults to 10.\n",
    "            print_every (int, optional): Frequency of print (in number of epochs). Defaults to 1.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.k = top_k\n",
    "        self.print_every = print_every\n",
    "        self.generated_texts = [] # for qualitative validation set\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        \"\"\"\n",
    "        Sample a token index from the token predictions based on their probabilities.\n",
    "\n",
    "        Args:\n",
    "            logits (tf.Tensor): The token predictions (logits) of the model.\n",
    "\n",
    "        Returns:\n",
    "            int: The sampled token index.\n",
    "        \"\"\"\n",
    "        # Select top-k logits and their indices\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "\n",
    "        # Apply softmax to transform logits into probabilities\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "\n",
    "        # Randomly select an index according to the probability distribution\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        \"\"\"\n",
    "        Convert a token index into the corresponding word.\n",
    "\n",
    "        Args:\n",
    "            number (int): The token index.\n",
    "\n",
    "        Returns:\n",
    "            str: The corresponding word.\n",
    "        \"\"\"\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        At the end of each epoch, generate text and print it.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): The current epoch number.\n",
    "            logs (dict, optional): Dictionary of metrics from the epoch. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Create a copy of start tokens for generation\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "\n",
    "        # Only generate text at specified frequency\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "\n",
    "        # Generate tokens until max tokens reached\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "\n",
    "            # Adjust padding based on length of start tokens\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "\n",
    "            x = np.array([x])\n",
    "\n",
    "            # Use the model to predict the probabilities for the next token\n",
    "            y, _ = self.model.predict(x)\n",
    "\n",
    "            # Sample a token from the model's output distribution\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "\n",
    "            # Append the token to the list of generated tokens\n",
    "            tokens_generated.append(sample_token)\n",
    "\n",
    "            # Add the token to the start tokens for the next generation\n",
    "            start_tokens.append(sample_token)\n",
    "\n",
    "            # Increase the number of tokens generated by 1\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "\n",
    "        # Convert the tokens into actual words and join them into a string\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        \n",
    "        self.generated_texts.append((epoch, txt)) # Store for evalutation after training\n",
    "\n",
    "\n",
    "        # Print the generated text\n",
    "        print(f\"generated text:\\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word/Index Mapping Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Callback Object\n",
    "\n",
    "***We also need to supply a starting prompt to act as a qualitative validation set to evaluate the models performance from a 'does it make more sense' per epoch. It will generate(predict) a text sequence continuation from the starting prompt at the end of every epoch to inspect.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_prompt = \"I would have\"\n",
    "\n",
    "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "num_tokens_generated = 42\n",
    "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "***I apologize for the scrolling your about to do. I wanted to generate text at each epoch so that along with loss there would be some qualitative evaluation on the models performance throughout training but I could not find a way to remove the progress bars for each step inside the epochs... If anyone reading this knows a way please comment.***\n",
    "\n",
    "***Until about `25` epochs many of the generations depending on the satrting prompt during training had nonsensical outputs. So we will use `25` to get a good baseline model to evaluate.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 80)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 80, 256)          25620480  \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " transformer_block (Transfor  (None, 80, 256)          658688    \n",
      " merBlock)                                                       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 80, 100000)        25700000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 51,979,168\n",
      "Trainable params: 51,979,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 181ms/step loss: 8.0328 - dense_2_loss: 8.03\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "generated text:\n",
      "I would have                                           \n",
      "\n",
      "194/194 [==============================] - 29s 140ms/step - loss: 8.0328 - dense_2_loss: 8.0328\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 23ms/step- loss: 5.9694 - dense_2_loss: 5.96\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "generated text:\n",
      "I would have                                           \n",
      "\n",
      "194/194 [==============================] - 27s 141ms/step - loss: 5.9694 - dense_2_loss: 5.9694\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 0s 23ms/step- loss: 5.9186 - dense_2_loss: 5.91\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "generated text:\n",
      "I would have                                           \n",
      "\n",
      "194/194 [==============================] - 27s 140ms/step - loss: 5.9186 - dense_2_loss: 5.9186\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 22ms/step- loss: 5.8817 - dense_2_loss: 5.88\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "generated text:\n",
      "I would have                                           \n",
      "\n",
      "194/194 [==============================] - 27s 140ms/step - loss: 5.8817 - dense_2_loss: 5.8817\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 23ms/step- loss: 5.8437 - dense_2_loss: 5.84\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "generated text:\n",
      "I would have                                           \n",
      "\n",
      "194/194 [==============================] - 27s 140ms/step - loss: 5.8437 - dense_2_loss: 5.8437\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 22ms/step- loss: 5.8014 - dense_2_loss: 5.80\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "generated text:\n",
      "I would have                                           \n",
      "\n",
      "194/194 [==============================] - 27s 140ms/step - loss: 5.8014 - dense_2_loss: 5.8014\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 22ms/step- loss: 5.7544 - dense_2_loss: 5.75\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "generated text:\n",
      "I would have                                           \n",
      "\n",
      "194/194 [==============================] - 27s 141ms/step - loss: 5.7544 - dense_2_loss: 5.7544\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 24ms/step- loss: 5.7032 - dense_2_loss: 5.70\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "generated text:\n",
      "I would have                                           \n",
      "\n",
      "194/194 [==============================] - 27s 141ms/step - loss: 5.7032 - dense_2_loss: 5.7032\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 22ms/step- loss: 5.6483 - dense_2_loss: 5.64\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "generated text:\n",
      "I would have                                           \n",
      "\n",
      "194/194 [==============================] - 27s 140ms/step - loss: 5.6483 - dense_2_loss: 5.6483\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 24ms/step- loss: 5.5908 - dense_2_loss: 5.59\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "generated text:\n",
      "I would have the                                          \n",
      "\n",
      "194/194 [==============================] - 27s 140ms/step - loss: 5.5908 - dense_2_loss: 5.5908\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 22ms/step- loss: 5.5317 - dense_2_loss: 5.53\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "generated text:\n",
      "I would have                                           \n",
      "\n",
      "194/194 [==============================] - 27s 140ms/step - loss: 5.5317 - dense_2_loss: 5.5317\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 22ms/step- loss: 5.4720 - dense_2_loss: 5.47\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "generated text:\n",
      "I would have [UNK]                                          \n",
      "\n",
      "194/194 [==============================] - 27s 141ms/step - loss: 5.4720 - dense_2_loss: 5.4720\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 23ms/step- loss: 5.4136 - dense_2_loss: 5.41\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "generated text:\n",
      "I would have the the [UNK] a the                                      \n",
      "\n",
      "194/194 [==============================] - 27s 141ms/step - loss: 5.4136 - dense_2_loss: 5.4136\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 22ms/step- loss: 5.3581 - dense_2_loss: 5.35\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "generated text:\n",
      "I would have the the is                                        \n",
      "\n",
      "194/194 [==============================] - 27s 140ms/step - loss: 5.3581 - dense_2_loss: 5.3581\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 0s 22ms/step- loss: 5.3065 - dense_2_loss: 5.30\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "generated text:\n",
      "I would have the the and a                                       \n",
      "\n",
      "194/194 [==============================] - 27s 140ms/step - loss: 5.3065 - dense_2_loss: 5.3065\n"
     ]
    }
   ],
   "source": [
    "model = MiniGPT()\n",
    "\n",
    "N_EPOCHS = 15\n",
    "history  = model.fit(text_ds, verbose=1, epochs=N_EPOCHS, callbacks=[text_gen_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Training Loss Per Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAGDCAYAAADtZ0xmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAA1H0lEQVR4nO3de3RkZ3nn+99TJZVKVVKVrn0pSX2zG9tdYrBJBzDJSswwmRMIA1ln4AwckgGSWYxZMxAmMAxwVoDJyszKmcmZSRwSCBMCZA0hmQEMTgLE3HHCtTG2UbttbPdV6pturftdz/mjtqSSWt2WurVr1+X7WUurdu29q/Soli39/Pp539fcXQAAAAC2LhZ1AQAAAEClIUQDAAAA20SIBgAAALaJEA0AAABsEyEaAAAA2CZCNAAAALBNhGgAKGNm9kUze8NO37vNGu4xs/6dfl8AqGR1URcAANXGzCaLnqYkzUlaCp7/a3f/5Fbfy91fFsa9AICbQ4gGgB3m7k0rx2Z2WtK/cvevbLzPzOrcfbGUtQEAdgbtHABQIittEWb2H8zsoqSPmVmrmf2NmQ2a2Whw3F30mm+Y2b8Kjt9oZn9vZr8X3HvKzF52g/ceNLNvmdmEmX3FzP7IzP7nFn+OO4LvdcXMjpvZK4uuvdzMHg/ed8DM3hmc7wh+titmNmJmD5kZf4MAVCx+gQFAae2R1CZpv6Q3q/B7+GPB832SZiR98Dqvf6GkJyV1SPovkj5qZnYD9/6FpO9Lapf0AUm/upXizaxe0l9LelDSLklvlfRJM7stuOWjKrSsNEvqlfS14Pw7JPVL6pS0W9J7JflWvicAlCNCNACU1rKk97v7nLvPuPuwu3/G3afdfULSf5L089d5/Rl3/x/uviTpE5L2qhBKt3yvme2T9NOS3ufu8+7+95Ie2GL9L5LUJOl3g9d+TdLfSHpdcH1B0hEzy7j7qLs/XHR+r6T97r7g7g+5OyEaQMUiRANAaQ26++zKEzNLmdmfmNkZMxuX9C1JLWYWv8brL64cuPt0cNi0zXtzkkaKzknSuS3Wn5N0zt2Xi86dkdQVHP9zSS+XdMbMvmlmdwfn/6ukpyU9aGYnzezdW/x+AFCWCNEAUFobR1/fIek2SS9094yknwvOX6tFYydckNRmZqmicz1bfO15ST0b+pn3SRqQJHf/gbu/SoVWj89J+l/B+Ql3f4e7H5L0Skm/aWYvvbkfAwCiQ4gGgGg1q9AHfcXM2iS9P+xv6O5nJB2T9AEzSwSjxf9siy//nqRpSe8ys3ozuyd47V8G7/V6M8u6+4KkcRXaV2RmrzCzW4Oe7DEVlvxb3vQ7AEAFIEQDQLR+X1KjpCFJ35X0pRJ939dLulvSsKTfkfRXKqxnfV3uPq9CaH6ZCjX/saR/6e5PBLf8qqTTQWvKvcH3kaTDkr4iaVLSdyT9sbt/fcd+GgAoMWNeBwDAzP5K0hPuHvpIOABUA0aiAaAGmdlPm9ktZhYzs1+U9CoVepgBAFvAjoUAUJv2SPqsCutE90t6i7v/KNqSAKBy0M4BAAAAbBPtHAAAAMA2EaIBAACAbaq4nuiOjg4/cOBA1GUAAACgyv3whz8ccvfOza5VXIg+cOCAjh07FnUZAAAAqHJmduZa12jnAAAAALaJEA0AAABsEyEaAAAA2KaK64kGAABA+BYWFtTf36/Z2dmoSwldMplUd3e36uvrt/waQjQAAACu0t/fr+bmZh04cEBmFnU5oXF3DQ8Pq7+/XwcPHtzy62jnAAAAwFVmZ2fV3t5e1QFaksxM7e3t2x5xJ0QDAABgU9UeoFfcyM9JiAYAAEDZGR4e1p133qk777xTe/bsUVdX1+rz+fn567722LFjetvb3hZqffREAwAAoOy0t7frkUcekSR94AMfUFNTk975zneuXl9cXFRd3eZR9ujRozp69Gio9TESDQAAgIrwxje+Uffee69e+MIX6l3vepe+//3v6+6779Zdd92lF7/4xXryySclSd/4xjf0ile8QlIhgP/ar/2a7rnnHh06dEj33XffjtTCSDQAAACu6z/+9XE9fn58R9/zSC6j9/+z/LZf19/fr29/+9uKx+MaHx/XQw89pLq6On3lK1/Re9/7Xn3mM5+56jVPPPGEvv71r2tiYkK33Xab3vKWt2xrObvNEKK3YGZ+Sd8/PaLn7G7S3mxj1OUAAADUrNe85jWKx+OSpLGxMb3hDW/QU089JTPTwsLCpq/5pV/6JTU0NKihoUG7du3SpUuX1N3dfVN1EKK3YGhyTm/4s+/rd365V7/yov1RlwMAAFBSNzJiHJZ0Or16/Fu/9Vt6yUteovvvv1+nT5/WPffcs+lrGhoaVo/j8bgWFxdvuo5Qe6LN7N+Z2XEz6zOzT5lZcsP1BjP7KzN72sy+Z2YHwqznRnW3NirbWK/j58eiLgUAAACBsbExdXV1SZI+/vGPl/R7hxaizaxL0tskHXX3XklxSa/dcNuvSxp191sl/XdJ/29Y9dwMM1NvV0bHd7gXCAAAADfuXe96l97znvforrvu2pHR5e0wdw/njQsh+ruSnidpXNLnJN3n7g8W3fN3kj7g7t8xszpJFyV1+nWKOnr0qB87diyUmq/nP3/hhD7+D6d1/Lf/D9XHWdQEAABUtxMnTuiOO+6IuoyS2eznNbMfuvuma+WFlgbdfUDS70k6K+mCpLHiAB3oknQuuH9R0pik9o3vZWZvNrNjZnZscHAwrJKvK5/LaH5pWU9fnozk+wMAAKB8hNnO0SrpVZIOSspJSpvZr9zIe7n7R9z9qLsf7ezs3Mkytyyfy0qS+gboiwYAAKh1YfYl/BNJp9x90N0XJH1W0os33DMgqUeSgnaOrKThEGu6YQc70kol4vRFAwAAINQQfVbSi8wsZWYm6aWSTmy45wFJbwiOXy3pa9frh45SPGY6sjfDCh0AAKBmlGks23E38nOG2RP9PUmflvSwpB8H3+sjZvbbZvbK4LaPSmo3s6cl/aakd4dVz07I5zJ6/Py4lpdr4x8oAABQu5LJpIaHh6s+SLu7hoeHlUwmn/3mIqFutuLu75f0/g2n31d0fVbSa8KsYSflu7L6xHfO6PTwlA51NkVdDgAAQGi6u7vV39+vqBZ1KKVkMrntHQzZsXAb8rmMJKnv/DghGgAAVLX6+nodPHgw6jLKFgseb8PhXc1KxGM6zgodAAAANY0QvQ2Juphu29PMCh0AAAA1jhC9TflcRn3nx6q+yR4AAADXRojepnxXVlemF3R+bDbqUgAAABARQvQ2rU4upC8aAACgZhGit+mOPRnFTEwuBAAAqGGE6G1qTMR1664mJhcCAADUMEL0Dcjnsupj+28AAICaRYi+AflcRpfG5zQ4MRd1KQAAAIgAIfoG5HNZSdJxRqMBAABqEiH6BhwJVuigLxoAAKA2EaJvQLaxXvvbU4xEAwAA1ChC9A3K5zLqG2AkGgAAoBYRom9QPpfV2ZFpjc0sRF0KAAAASowQfYNWdi58nL5oAACAmkOIvkGs0AEAAFC7CNE3qLO5QXsySVboAAAAqEGE6JtQmFzISDQAAECtIUTfhHxXVs8MTmpmfinqUgAAAFBChOibkM9ltOzSiYu0dAAAANQSQvRN6O0KJhfS0gEAAFBTCNE3IZdNqjVVz+RCAACAGkOIvglmpnwuqz6WuQMAAKgphOiblO/K6CcXJzW/uBx1KQAAACgRQvRNyueyml9a1lOXJ6IuBQAAACVCiL5JvcH238cH6IsGAACoFYTom3SgPa10Is723wAAADWEEH2TYjHTkVxGfazQAQAAUDMI0Tsgn8vqxIVxLS171KUAAACgBAjROyCfy2h6fkmnhqaiLgUAAAAlQIjeAas7F9IXDQAAUBMI0Tvg1l1NStTF2LkQAACgRhCid0B9PKbb9zSrb4CRaAAAgFpAiN4h+VxWx8+Py53JhQAAANWOEL1D8rmMxmYW1D86E3UpAAAACBkheocwuRAAAKB2EKJ3yO17mhWPGZMLAQAAagAheock6+O6tbOJyYUAAAA1gBC9g/JdGUaiAQAAagAhegflc1ldnpjT5fHZqEsBAABAiAjRO6g3l5EkRqMBAACqHCF6Bx1ZDdH0RQMAAFQzQvQOak7W60B7Sn0DjEQDAABUM0L0Dst3ZXX8AiPRAAAA1Sy0EG1mt5nZI0Vf42b29g333GNmY0X3vC+sekoln8vo3MiMxqYXoi4FAAAAIakL643d/UlJd0qSmcUlDUi6f5NbH3L3V4RVR6n15tZ2LnzxrR0RVwMAAIAwlKqd46WSnnH3MyX6fpHJs0IHAABA1StViH6tpE9d49rdZvaomX3RzPKb3WBmbzazY2Z2bHBwMLwqd0B7U4P2ZpPqY4UOAACAqhV6iDazhKRXSvrfm1x+WNJ+d3+epD+U9LnN3sPdP+LuR939aGdnZ2i17pR8LstINAAAQBUrxUj0yyQ97O6XNl5w93F3nwyOvyCp3swqvpE4n8vomcFJTc8vRl0KAAAAQlCKEP06XaOVw8z2mJkFxy8I6hkuQU2h6u3Kyl06cYHRaAAAgGoUaog2s7SkX5D02aJz95rZvcHTV0vqM7NHJd0n6bXu7mHWVApMLgQAAKhuoS1xJ0nuPiWpfcO5Dxcdf1DSB8OsIQp7s0m1pRPqG2ByIQAAQDVix8IQmJnyuQwj0QAAAFWKEB2SfC6rn1ya0NziUtSlAAAAYIcRokPS25XRwpLrqUuTUZcCAACAHUaIDkm+aPtvAAAAVBdCdEj2t6XU1FCnvgH6ogEAAKoNIToksZjpSC7DSDQAAEAVIkSHKJ/L6PEL41parvilrwEAAFCEEB2i3lxWswvLOjnI5EIAAIBqQogOUb6LnQsBAACqESE6RLd2NqmhLsbOhQAAAFWGEB2iunhMt+9l50IAAIBqQ4gOWT6XUd/5MbkzuRAAAKBaEKJD1pvLamJ2UedGZqIuBQAAADuEEB2yfG5lciF90QAAANWCEB2y2/Y0Kx4z9RGiAQAAqgYhOmTJ+rgO72piciEAAEAVIUSXQD6XVd8AkwsBAACqBSG6BHq7MhqanNflibmoSwEAAMAOIESXQD6XlcTkQgAAgGpBiC6BI8EKHX0D9EUDAABUA0J0CTQ11OlQR5qRaAAAgCpBiC6RI7kMI9EAAABVghBdIr1dWQ1cmdHo1HzUpQAAAOAmEaJLZGXnwscvMBoNAABQ6QjRJbKyQkffAH3RAAAAlY4QXSJt6YS6WhrZuRAAAKAKEKJL6Eguoz5W6AAAAKh4hOgS6s1ldWpoSlNzi1GXAgAAgJtAiC6hfC4jd+kEkwsBAAAqGiG6hHq7mFwIAABQDQjRJbQ706COpgSTCwEAACocIbqEzExHcln1EaIBAAAqGiG6xHpzGT11aUJzi0tRlwIAAIAbRIgusXwuq8Vl108uTkZdCgAAAG4QIbrEersK23+zXjQAAEDlIkSX2L62lJqTdTpOiAYAAKhYhOgSMzMd2ZtR3wCTCwEAACoVIToCvV1ZnbgwrsWl5ahLAQAAwA0gREcgn8tobnFZJ4emoi4FAAAAN4AQHQF2LgQAAKhshOgIHOpIK1kfY+dCAACACkWIjkBdPKbb92QYiQYAAKhQhOiI9HZl9Pj5cS0ve9SlAAAAYJsI0RHJ57KamFvUudHpqEsBAADANhGiI9KbW5lcSF80AABApQktRJvZbWb2SNHXuJm9fcM9Zmb3mdnTZvaYmT0/rHrKzXP2NKkuZuxcCAAAUIHqwnpjd39S0p2SZGZxSQOS7t9w28skHQ6+XijpQ8Fj1Wuoi+vw7mb1sUIHAABAxSlVO8dLJT3j7mc2nH+VpD/3gu9KajGzvSWqKXK9uYyOD4zJncmFAAAAlaRUIfq1kj61yfkuSeeKnvcH52pCPpfR8NS8Lo3PRV0KAAAAtiH0EG1mCUmvlPS/b+I93mxmx8zs2ODg4M4VFzF2LgQAAKhMpRiJfpmkh9390ibXBiT1FD3vDs6t4+4fcfej7n60s7MzpDJL7469GZmJnQsBAAAqTClC9Ou0eSuHJD0g6V8Gq3S8SNKYu18oQU1lId1Qp4MdafWxQgcAAEBFCW11Dkkys7SkX5D0r4vO3StJ7v5hSV+Q9HJJT0ualvSmMOspR725rI6dHom6DAAAAGxDqCHa3acktW849+GiY5f0b8Ksodzlcxk98Oh5jUzNqy2diLocAAAAbAE7FkZsZXIhm64AAABUDkJ0xPK5jCQmFwIAAFQSQnTEWlIJdbU0sswdAABABSFEl4Hergwj0QAAABWEEF0G8rmsTg1NaWJ2IepSAAAAsAWE6DLQ21Xoiz5xYSLiSgAAALAVhOgykM+xQgcAAEAlIUSXgV3NDepoalDfAH3RAAAAlYAQXQbMLJhcyEg0AABAJSBEl4l8LqOnLk9qdmEp6lIAAADwLAjRZaI3l9XSsuvJi0wuBAAAKHeE6DKxNrmQvmgAAIByR4guEz1tjWpO1qmPvmgAAICyR4guE2am3lxWx9n+GwAAoOwRostIPpfRiYsTWlhajroUAAAAXAchuoz0dmU1v7isZwYnoy4FAAAA10GILiP5XGH77+NsugIAAFDWCNFl5FBnk5L1MSYXAgAAlDlCdBmJx0xH9mYYiQYAAChzhOgyk89l9fiFcS0ve9SlAAAA4BoI0WWmtyujyblFnRmZjroUAAAAXAMhusys7VxIXzQAAEC52lKINrO0mcWC4+eY2SvNrD7c0mrT4d1Nqo+b+uiLBgAAKFtbHYn+lqSkmXVJelDSr0r6eFhF1bKGuries7uZkWgAAIAyttUQbe4+Len/lPTH7v4aSfnwyqpt+VxGx8+Py53JhQAAAOVoyyHazO6W9HpJfxuci4dTEnq7shqZmteFsdmoSwEAAMAmthqi3y7pPZLud/fjZnZI0tdDq6rGre5ceJ6+aAAAgHJUt5Wb3P2bkr4pScEEwyF3f1uYhdWyO/ZmZCb1DYzpF47sjrocAAAAbLDV1Tn+wswyZpaW1CfpcTP79+GWVrtSiTrd0tnE5EIAAIAytdV2jiPuPi7plyV9UdJBFVboQEhWJhcCAACg/Gw1RNcH60L/sqQH3H1BEktHhKg3l9WFsVkNT85FXQoAAAA22GqI/hNJpyWlJX3LzPZLYpg0REwuBAAAKF9bCtHufp+7d7n7y73gjKSXhFxbTVvZ/ruPvmgAAICys9WJhVkz+29mdiz4+v9UGJVGSLKpevW0NTISDQAAUIa22s7xZ5ImJP1fwde4pI+FVRQK8nuzOj7ASDQAAEC52WqIvsXd3+/uJ4Ov/yjpUJiFQertyuj08LTGZxeiLgUAAABFthqiZ8zsZ1eemNnPSJoJpySsWOmLPkFLBwAAQFnZ0o6Fku6V9Odmlg2ej0p6QzglYUW+q7BCR9/5cb3wUHvE1QAAAGDFVrf9flTS88wsEzwfN7O3S3osxNpq3q7mpHY1N7BzIQAAQJnZajuHpEJ4DnYulKTfDKEebJDPZXR8gHYOAACAcrKtEL2B7VgVuKberqyeHpzU7MJS1KUAAAAgcDMhmm2/SyCfy2hp2fXExYmoSwEAAEDguj3RZjahzcOySWoMpSKss7pz4cCY7uxpibYYAAAASHqWEO3uzaUqBJvrbm1UtrGenQsBAADKyM20c6AEzKwwuZAVOgAAAMoGIboC9HZl9cSFCS0sLUddCgAAABRyiDazFjP7tJk9YWYnzOzuDdfvMbMxM3sk+HpfmPVUqnwuo/mlZT19eTLqUgAAAKCt71h4o/5A0pfc/dVmlpCU2uSeh9z9FSHXUdGKJxfesTcTcTUAAAAIbSQ62CL85yR9VJLcfd7dr4T1/arZwY60Uok4kwsBAADKRJjtHAclDUr6mJn9yMz+1MzSm9x3t5k9amZfNLP8Zm9kZm82s2NmdmxwcDDEkstTPGa6Yy+TCwEAAMpFmCG6TtLzJX3I3e+SNCXp3RvueVjSfnd/nqQ/lPS5zd7I3T/i7kfd/WhnZ2eIJZev3lxGj58f1/Iye9wAAABELcwQ3S+p392/Fzz/tAqhepW7j7v7ZHD8BUn1ZtYRYk0VK5/Lamp+SaeHp6IuBQAAoOaFFqLd/aKkc2Z2W3DqpZIeL77HzPaYmQXHLwjqGQ6rpkqW7ypMKOyjLxoAACByYa/O8VZJnwxW5jgp6U1mdq8kufuHJb1a0lvMbFHSjKTXujv9Cps4vKtZiXhMx8+P6ZXPy0VdDgAAQE0LNUS7+yOSjm44/eGi6x+U9MEwa6gWibqYnrOnSccHGIkGAACIGjsWVpDeXFZ958fEYD0AAEC0CNEVJJ/L6Mr0gs6PzUZdCgAAQE0jRFeQfNfazoUAAACIDiG6gtyxJ6OYiZ0LAQAAIkaIriCNibhu6WzScUaiAQAAIkWIrjC9XYXJhQAAAIgOIbrC5HMZXRqf0+DEXNSlAAAA1CxCdIXJ5wqTC48zGg0AABAZQnSFOZIrbP/N5EIAAIDoEKIrTLaxXvvaUoxEAwAARIgQXYF6uzLqY/tvAACAyBCiK1A+l9XZkWmNzSxEXQoAAEBNIkRXoHzQF/04fdEAAACRIERXIFboAAAAiBYhugJ1Njdod6aBFToAAAAiQoiuUL25rPrY/hsAACAShOgKlc9l9MzgpGbml6IuBQAAoOYQoitUviurZZdOXKSlAwAAoNQI0RUqz86FAAAAkSFEV6iulka1pOp1nL5oAACAkiNEVygzK0wuZJk7AACAkiNEV7B8LqOfXJzU/OJy1KUAAADUFEJ0Bct3ZTW/tKynLk9EXQoAAEBNIURXMCYXAgAARIMQXcEOtqeVTsSZXAgAAFBihOgKFouZjuQy6mMkGgAAoKQI0RUun8vqxIVxLS171KUAAADUDEJ0hcvnMpqeX9KpoamoSwEAAKgZhOgKl89lJUnHWS8aAACgZAjRFe7w7iYl4jFW6AAAACghQnSFq4/HdPveZvWxQgcAAEDJEKKrQD6X0fHz43JnciEAAEApEKKrQD6X1djMgvpHZ6IuBQAAoCYQoqsAOxcCAACUFiG6CtyxN6N4zFihAwAAoEQI0VUgWR/XrZ1NTC4EAAAoEUJ0lViZXAgAAIDwEaKrRL4rq8sTc7o8MRt1KQAAAFWPEF0lmFwIAABQOoToKnFkJUTTFw0AABA6QnSVyCTrdaA9pb4BRqIBAADCRoiuIvlcVscvMBINAAAQNkJ0Fcl3ZXRuZEZj0wtRlwIAAFDVCNFVJJ/LShKj0QAAACEjRFeR1RU66IsGAAAIVagh2sxazOzTZvaEmZ0ws7s3XDczu8/Mnjazx8zs+WHWU+06mhq0N5tUH9t/AwAAhKou5Pf/A0lfcvdXm1lCUmrD9ZdJOhx8vVDSh4JH3CB2LgQAAAhfaCPRZpaV9HOSPipJ7j7v7lc23PYqSX/uBd+V1GJme8OqqRbkc1k9Mzip6fnFqEsBAACoWmG2cxyUNCjpY2b2IzP7UzNLb7inS9K5ouf9wTncoHwuI3fpxIWJqEsBAACoWmGG6DpJz5f0IXe/S9KUpHffyBuZ2ZvN7JiZHRscHNzJGqtOb1ewQgd90QAAAKEJM0T3S+p39+8Fzz+tQqguNiCpp+h5d3BuHXf/iLsfdfejnZ2doRRbLfZmk2pLJ9TH9t8AAAChCW1iobtfNLNzZnabuz8p6aWSHt9w2wOS/q2Z/aUKEwrH3P1CWDXVAjNTPpfRF398UcOT8+pubVRPW0rdrY3qbk2ppzWlbKo+6jIBAAAqWtirc7xV0ieDlTlOSnqTmd0rSe7+YUlfkPRySU9Lmpb0ppDrqQn3/vwt+tg/nFb/6LS+d2pEk3PrJxk2J+uCQB0E67b1j00NYf9jAQAAUNnM3aOuYVuOHj3qx44di7qMiuHuGptZUP/ojM6NTBceR6fXPZ9ZWFr3mtZU/Wqo7mkNRrHb1kJ3sj4e0U8DAABQOmb2Q3c/utk1hhyrnJmpJZVQSyqxOumwmLtreGp+NVQXB+wnLkzoK49f1vzS8rrXdDQ1rI1ebxjNzrUk1VBHyAYAANWNEF3jzEwdTQ3qaGrQnT0tV11fXnYNTs6tjWIXjWY/eu6KvvjjC1pc9qL3k3Y3J68K2d3BqPbebFJ1cXabBwAAlY0QjeuKxUy7M0ntziR19MDV1xeXlnVpYvOQ/f1TI/r8IzMqytiKx0x7MsUhu9AusjuTVEdzQp1NDWpNJRSLWcl+RgAAgO0iROOm1MVj6mppVFdL46bX5xeXdXFsNmgTmda5kZnC4+iMHnpqUJfG5656TTxmak8n1NlcGCHvbG5Yd9zRlNCu5gZ1NiWVaayTGYEbAACUFiEaoUrUxbSvPaV97alNr88uLOn8lRkNTsxpcHJOQ6uP8xqcnNPgxJx+cmlCgxNz69pGVt8/HlNHU0IdzQ3qbNoYtosDeEJNDQRuAACwMwjRiFSyPq5DnU061Nl03fuWlwurjAwFwXqw6HElcF8Ym9VjA2ManpzTJnlbyfrYWrBualgN3sUBfOWxMcHkSAAAcG2EaFSEWMzUmk6oNZ3Q4d3N1713adk1Oj1fCNkTc6vBuziAnxme1rEzoxqZmt/0PdKJ+CZtJOuD9q5Mg9rTDUrUMVESAIBaQ4hG1YnH1lYcuWPv9e9dWFrWyNT8utHt1bAdHP/k0oT+4ekhjc8ubvoeral67WpOrobuXc1rbSSrz+nfBgCgqhCiUdPq47HV1UeezdzikoYm10a4ByfmdHlituh4TqeGpjQ4Oaf5xeWrXp+oi62NYq8L2esDeEcTo9sAAJQ7QjSwRQ118euuRLLC3TU+u6jBiVldLgrcxWH72dpJWlL1a0G7qUG7Msm1vu2iEJ5trGd0GwCACBCigR1mZso21ivbWK9bd12/f3t+cVnDU0G4Hi+aMFk0yv3Ds6O6PD6nuc1Gt+OxQr/2athuuGq0e0+2EMDZ5AYAgJ1DiAYilKiLaW+2UXuzzz66PTG3eFXYLm4n6R+d1o/Ojmp4k9HtmBW2a9+TLbSu7MkkNxw3aHcmqeZkfVg/KgAAVYUQDVQAM1MmWa9Msl63PMtygCuTJS+PF0L2pfE5XRyb0cXxWV0cn9PZ4cJukmMzC1e9Np2Ia3c2CNZB0N4YvDuaGhRnR0kAQI0jRANVZv1kyew175uZX9Kl8VldHJ8tPI6tP/7uyWFd3mSTm3jM1NnUEITtBu3JJNcF75XjdAO/XgAA1Yu/ckCNakzEdaAjrQMd6Wves7zsGpqa06WxuWAke1aXisL2ycEpffuZYU1ssvxfc7LuqraR9WG7QR3pBsUY1QYAVCBCNIBrisVMu5qT2tWc1HOvM6o9Nbe4bjS7OGxfHJ/TU5eGNDg5p6UNo9p1MdOu5obVcL0326hcS1JdLY3KBV8dTQlWIAEAlB1CNICblm6oe9bt25eWXUOTc1e1jVwMHp+8NKFvPDmomYWlda9L1MWCUJ1ULlsI1mshO6lcS6OS9WzTDgAoLUI0gJKIx2y1V/t517jH3TU2s6CBKzM6f2VW56/MaCD4On9lRt96alCXJ+bk6we01dGUKITq7Fq4XgnaXa2Nak8zmg0A2FmEaABlw8zUkkqoJZVQPrd5+8j84rIujc+uButC0C48f3pwUt96alDT81sfze5qbdTebJLRbADAthCiAVSURF1MPW0p9bSlNr2+2Wj2+Ssz6mc0GwCwgwjRAKrKzYxmn9/maHZ3a+NqoN+TSbJ+NgDUEEI0gJpzo6PZK/3Zm41m18VMuZZG9bQ1qqe18N7drY3qbk2pp61RnU0NjGQDQBUhRAPABlsdzT5/ZUbnRqd1bmRG/aPTOjc6o3Mj0/rKiUsamly//XqyPlYI1EXBeiVs97SmlGmsI2QDQAUhRAPADUjUxa67Wc30/KIGRtdC9rmRaZ0bnVb/6Ix+eGZU4xs2qGluqFN3WyFk97QVh+1C4E4l+HUNAOWE38oAEIJUok6Hdzfr8O7mTa+PzSzo3Mh0YQS7aCT71NCUvvXUoGYXltfd355OqDtoEekpGsnubi1MfGyoY3URACglQjQARCDbWK9sV1a9XVe3i7i7hibn17WIrITt4wNjevD4RS0srTVkm0m7m5PrgnV30CbS09aovdlGJj0CwA4jRANAmTEzdTY3qLO5QXfta73q+tKy69L4bNAiMrMasM+NTuu7J4d1YXx200mP+9tTha+2tPYFx/vaUrSKAMAN4DcnAFSYeBCKcy2NeuEm1+cXl3VhbGY1WJ8bmdbZ4OuvH72gsZmFdffvam4IAnV6NWjva0vpQHtaLal6JjwCwCYI0QBQZRJ1Me1vT2t/++aTHq9Mz+vM8LTOjEzr7PDU6vHfPz2ozzw8t+7e5mTdutHrA0Vhe08mqRhtIgBqFCEaAGrMyvJ9z+tpuerazPySzo1OF4L18JTOjkzr9PC0jp8f098dv6jF5bU+kURdTD2tjUFgT2l/W0r72wthu5vJjgCqHCEaALCqMRHXc3Y36zmbrCqyuLSsC2Ozwcj11GrQPjNc6MUu3uXRTMplG4vaQ9ZaRfa3p9XUwJ8fAJWN32IAgC2pi6/t9Piz6lh3bWVFkbMjUzo9VNQqMjKtB49f0vDU+s1n2tOJwuTGtpT2tad1oChsdzQl6MMGUPYI0QCAm1a8oshP7W+76vrE7ILODBcmNxYeCyPYPzg9qs8/en7daiLpRFz729M62FEYvT7QUTg+0E7ABlA+CNEAgNA1J+vVe411secWl9Q/OqOzQXvI6eDx8QvjV/VhNzXUrQXr9nQQsAstIu1pAjaA0iFEAwAi1VAX1y2dTbqls+mqawtLyxoYndGp4SmdGSoE7FNDU+obGNOX+i5qqShgNzfUrW7FfjDovV4ZxW5lqT4AO4wQDQAoW/Xx2Gow1m3rry0sLat/dEanh6Z0amhKp4cLj4+eu6K/fey8ivK1MskgYBeNXh9oLzxvTSdK+0MBqAqEaABARaqPx3QwGGl+yYZr84vLOjc6vRqwzwxP6/TwlB4+O6q/fmx9D3a2sX519Hpd0G5PK5uqL+nPBKByEKIBAFUnURe7ZovI3OKSzo1M6/TQ9Oro9enhqU0nObam6teCdXtaBzpShUmOHWllkgRsoJYRogEANaWhLq5bdzXr1l1Xr4U9u1AI2GvtIYVJjt87Oaz7fzSw7t62dEIH2lM62NGkQ52FkF0I2CmlEvx5Baod/5YDABBI1sd1eHezDm+y2czswpLOBBMbC6uIFEax/+HpIX3m4f519+7NJldbTYq/etpSqo/HSvXjAAgRIRoAgC1I1sd1255m3bbn6oA9Nbe4GqpPDU7pVHD8N49d0NjMwup98ZhpX1tq04C9J5NULMYKIkClIEQDAHCT0g11yueyyueuXgd7dGpeJ4emVic5nhqa0smhKX37mSHNLiyv3pesj+lAe1qHOleCdZMOdhTaRViiDyg/hGgAAELUmk7op9IJ/dT+1nXnl5ddlyZm10auBwsB+4kLE3rw+KV1m8xkG+t1sCOtQysj151ruzimG/hTDkSBf/MAAIhALGbam23U3myjXnxrx7prxWtgnxya0qmhSZ0amtJ3Tw7rsxsmOO7ONKyOXK+E7AMdae1rSylRR/81EBZCNAAAZeZ6a2DPzC/pzEhh5PpkUYvIg8cvanhqfvW+mEk9Rf3Xh1ZaRDrT2kv/NXDTQg3RZnZa0oSkJUmL7n50w/V7JH1e0qng1Gfd/bfDrAkAgErWmIjr9j0Z3b4nc9W1semFYFLj5GrIPj08pR+cGtHU/NLqfQ11sXWTGg91Nq0GbXZwBLamFCPRL3H3oetcf8jdX1GCOgAAqGrZVL3uTLXozp6WdefdXYMTc3pmcG1pvpODk3ry0oS+/Pj6/uuWVP1quL4lCNcr/deNiXiJfyKgfNHOAQBAlTMz7coktSuT1N23tK+7thj0X58amtIzg5Or7SHfeWZYn314ff91LptcHbVemeB4qCOtrpZG1bH+NWpM2CHaJT1oZi7pT9z9I5vcc7eZPSrpvKR3uvvxkGsCAACBunissLV5R1ovuX3XumvT84uroXpl9ZBnhqb0uUcGNDG7uHpffdy0vz29fgWRoE2koynB8nyoSmGH6J919wEz2yXpy2b2hLt/q+j6w5L2u/ukmb1c0uckHd74Jmb2ZklvlqR9+/aFXDIAAJCkVGLz9a/dXSNT86trXp8cXFtB5Js/GdT84tr6180NdatL8hW3iRzoSKuJ5flQwczdn/2unfhGZh+QNOnuv3ede05LOnq9HuqjR4/6sWPHdr5AAABw05aWXeevzBRWDgnaQ1ZWERm4MqPi2LGruSEYsV4ZxS6sHtLTyvJ8KA9m9sONC2OsCO0/Ac0sLSnm7hPB8T+V9Nsb7tkj6ZK7u5m9QFJM0nBYNQEAgHDFY6aetpR62lL6+ed0rrs2u7CkM8PTOjU0GYTsQsD+u+OXNFK0PF88ZuppbVxd8/pQ8HiwI61ctpHl+VAWwvz/KLsl3R/0QdVJ+gt3/5KZ3StJ7v5hSa+W9BYzW5Q0I+m1XqqhcQAAUFLJ+rhu29Os2/Y0X3XtyvT8Wv910CJycmhK3z05opmFteX5EnUxHWhP6UB7sHNj+1qbSGdzA/3XKJmStXPsFNo5AACoHe6uS+NzOjW0tjzfytfZ4WnNL631X6cT8dVJkoeCZflWgjbrX+NGRNLOAQAAcLPMTHuySe3JXr0830r/dXGwPjU0pb6BMX3xxxdUtPy1WlL1OtC+vjVkpV2ECY64EfxTAwAAKlJx//XPbei/nl9c1rnRaZ3eELC/e3JYn/3R+vWvO5sbVttCigP2/vaUkvVsMIPNEaIBAEDVSdTFdEtnk27pbLrq2sz8ks6MBGtfDxceTw9P6atPXNLQ5NoERzMpl12Z4JjSwY4mHQweu1sbVc8GMzWNEA0AAGpKYyKu2/dkdPuezFXXxmcXdGZoWieHJnV6qLCSyKnhaT3wyHmNF20wE4+Z9rWlCpMcgx7sfe1pHWhPsYNjjSBEAwAABDLJej23O6vndl+9wczo9EKwqUwhXJ8emt50BZF4zNTd2qj97Wntb0tpf3tK+4OA3dNGi0i1IEQDAAA8CzNTWzqhtnSbfmp/27pr7q7LE3M6Mzyt08NTOjM8pTPD0zozPK0fnR1dt0W6JO3JJINgXQjX+4Ml+/a1p5RJ1pfyx8JNIEQDAADcBDPT7kxSuzNJveDg1QH7yvSCzoxMr4br08OF5fm+9sSghib7193flk6stomstIeshO32dIJ1sMsIIRoAACAkZqbWdEKt6YTu7Gm56vrU3KLODE/r7MiUTgej12eGp/SD06P6/KPn122Tnk7EV0eu1x4Lx3szSXZyLDFCNAAAQETSDXU6ksvoSO7qSY5zi0vqH51Z1x5yZnhKT16c0FdOXNLC0lrCTtTF1NPauNoWUvzY1dKoRB0THXcaIRoAAKAMNdTFr7lM38pGM2dH1tpDTgdh+9vPDK+b6Bgzqau1Ufvb1kav97Wl1N1amOiYbaQP+0YQogEAACpM8UYzP3Nrx7pr7q7Bybl1o9crj3/74wu6Mr2w7v5Msk49bYVg3dOWUk9ro7rbUuppTam7tZHVRK6BEA0AAFBFzEy7mpPa1ZzUTx9ou+r62PSCzo5M69zotM6tPs7oyUsT+uoTlzW/uLzu/l3NDavhumclXLc1qqc1pb3ZZM2uiU2IBgAAqCHZVL2em7p6LWxJWl4ujGIXh+uV4x+cHtUDj57XctFkx7qYaW9LUj2thXDd09a4OkLe05pSR1P1rihCiAYAAIAkKRZbW67v6Caj2AtLy7pwZfaqUexzo9P66hOXNTQ5t+7+xvq4uldHsAuP3UVhu5LXxSZEAwAAYEvq4zHta09pX3tq0+sz80vqHy2E67PD0zo3ujKSPaMfnBrRxNz6jWeyjfWFQB1MciwO210t5d2PTYgGAADAjmhMxHV4d7MO726+6pq7a2xmYXXkeiv92LszDeppTekP/++7tDfbWKofY0sI0QAAAAidmakllVBLKrHtfuzmMmz7IEQDAAAgcs/Wj11uanNNEgAAAOAmEKIBAACAbSJEAwAAANtEiAYAAAC2iRANAAAAbBMhGgAAANgmQjQAAACwTYRoAAAAYJsI0QAAAMA2EaIBAACAbSJEAwAAANtEiAYAAAC2iRANAAAAbJO5e9Q1bIuZDUo6E9G375A0FNH3rnZ8tuHhsw0Pn204+FzDw2cbHj7b8ET52e53987NLlRciI6SmR1z96NR11GN+GzDw2cbHj7bcPC5hofPNjx8tuEp18+Wdg4AAABgmwjRAAAAwDYRorfnI1EXUMX4bMPDZxsePttw8LmGh882PHy24SnLz5aeaAAAAGCbGIkGAAAAtokQvQVm9otm9qSZPW1m7466nmphZj1m9nUze9zMjpvZb0RdU7Uxs7iZ/cjM/ibqWqqJmbWY2afN7AkzO2Fmd0ddU7Uws38X/D7oM7NPmVky6poqlZn9mZldNrO+onNtZvZlM3sqeGyNssZKdY3P9r8GvxMeM7P7zawlwhIr1mafbdG1d5iZm1lHFLVtRIh+FmYWl/RHkl4m6Yik15nZkWirqhqLkt7h7kckvUjSv+Gz3XG/IelE1EVUoT+Q9CV3v13S88RnvCPMrEvS2yQddfdeSXFJr422qor2cUm/uOHcuyV91d0PS/pq8Bzb93Fd/dl+WVKvu/8jST+R9J5SF1UlPq6rP1uZWY+kfyrpbKkLuhZC9LN7gaSn3f2ku89L+ktJr4q4pqrg7hfc/eHgeEKFINIVbVXVw8y6Jf2SpD+NupZqYmZZST8n6aOS5O7z7n4l0qKqS52kRjOrk5SSdD7ieiqWu39L0siG06+S9Ing+BOSfrmUNVWLzT5bd3/Q3ReDp9+V1F3ywqrANf65laT/LuldkspmMh8h+tl1STpX9LxfBL0dZ2YHJN0l6XsRl1JNfl+FXzjLEddRbQ5KGpT0saBV5k/NLB11UdXA3Qck/Z4KI00XJI25+4PRVlV1drv7heD4oqTdURZTxX5N0hejLqJamNmrJA24+6NR11KMEI3ImVmTpM9Ieru7j0ddTzUws1dIuuzuP4y6lipUJ+n5kj7k7ndJmhL/S3xHBP25r1LhP1RyktJm9ivRVlW9vLA8V9mM6lULM/t/VGhX/GTUtVQDM0tJeq+k90Vdy0aE6Gc3IKmn6Hl3cA47wMzqVQjQn3T3z0ZdTxX5GUmvNLPTKrQg/WMz+5/RllQ1+iX1u/vK/zX5tAqhGjfvn0g65e6D7r4g6bOSXhxxTdXmkpntlaTg8XLE9VQVM3ujpFdIer2zhvBOuUWF/7B+NPib1i3pYTPbE2lVIkRvxQ8kHTazg2aWUGGSywMR11QVzMxU6Cs94e7/Lep6qom7v8fdu939gAr/zH7N3RnR2wHuflHSOTO7LTj1UkmPR1hSNTkr6UVmlgp+P7xUTNrcaQ9IekNw/AZJn4+wlqpiZr+oQgvdK919Oup6qoW7/9jdd7n7geBvWr+k5we/iyNFiH4WwSSBfyvp71T4Zf6/3P14tFVVjZ+R9KsqjJI+Eny9POqigC14q6RPmtljku6U9J+jLac6BKP7n5b0sKQfq/A3qix3KqsEZvYpSd+RdJuZ9ZvZr0v6XUm/YGZPqTDy/7tR1liprvHZflBSs6QvB3/PPhxpkRXqGp9tWWLHQgAAAGCbGIkGAAAAtokQDQAAAGwTIRoAAADYJkI0AAAAsE2EaAAAAGCbCNEAUEHMbKloSchHzGzHdks0swNm1rdT7wcA1awu6gIAANsy4+53Rl0EANQ6RqIBoAqY2Wkz+y9m9mMz+76Z3RqcP2BmXzOzx8zsq2a2Lzi/28zuN7NHg6+V7bXjZvY/zOy4mT1oZo2R/VAAUMYI0QBQWRo3tHP8i6JrY+7+XBV2Tvv94NwfSvqEu/8jSZ+UdF9w/j5J33T350l6vqSVnVgPS/ojd89LuiLpn4f60wBAhWLHQgCoIGY26e5Nm5w/Lekfu/tJM6uXdNHd281sSNJed18Izl9w9w4zG5TU7e5zRe9xQNKX3f1w8Pw/SKp3998pwY8GABWFkWgAqB5+jePtmCs6XhJzZwBgU4RoAKge/6Lo8TvB8bclvTY4fr2kh4Ljr0p6iySZWdzMsqUqEgCqASMMAFBZGs3skaLnX3L3lWXuWs3sMRVGk18XnHurpI+Z2b+XNCjpTcH535D0ETP7dRVGnN8i6ULYxQNAtaAnGgCqQNATfdTdh6KuBQBqAe0cAAAAwDYxEg0AAABsEyPRAAAAwDYRogEAAIBtIkQDAAAA20SIBgAAALaJEA0AAABsEyEaAAAA2Kb/H3QgI3m9GE/jAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Training loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Validation Set Per Epoch\n",
    "\n",
    "***Since we don't have a quantitative validation set in this situation we can use a qualitative validation set. These would be the generated text from the end of each epoch. This can give us some clues along with the losses per epoch to see how the models performance progressed through training.***\n",
    "\n",
    "***Lets inspect what the first five generations look llike compared to the last five during training.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Generated Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I would have                                  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I would have                                  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I would have                                  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I would have                                  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I would have                                  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch                                     Generated Text\n",
       "0      0  I would have                                  ...\n",
       "1      1  I would have                                  ...\n",
       "2      2  I would have                                  ...\n",
       "3      3  I would have                                  ...\n",
       "4      4  I would have                                  ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Generated Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>I would have [UNK]                            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>I would have the the [UNK] a the              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>I would have the the is                       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>I would have the the and a                    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>here we the the                               ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch                                     Generated Text\n",
       "11     11  I would have [UNK]                            ...\n",
       "12     12  I would have the the [UNK] a the              ...\n",
       "13     13  I would have the the is                       ...\n",
       "14     14  I would have the the and a                    ...\n",
       "15      0  here we the the                               ..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for epoch, text in text_gen_callback.generated_texts:\n",
    "#     print(f\"Epoch: {epoch+1}\\nGenerated Text:\\n{text}\\n\")\n",
    "    # Create a DataFrame\n",
    "    \n",
    "df_val = pd.DataFrame(text_gen_callback.generated_texts, columns=['Epoch', 'Generated Text'])\n",
    "\n",
    "display(df_val.head(5));df_val.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***It appears to that with more iterations the model becomes more and more realistic in its generations. The model starts producing less unknown tokens over time during training iterations.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "***Now that we have trained the model to generate toxic comments from a starting prompt we can begin to generate our synthetic data.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(starting_prompt=''):\n",
    "    new_start_prompt = \"here we\"\n",
    "    new_start_tokens = [word_to_index.get(word, 1) for word in new_start_prompt.split()]\n",
    "\n",
    "    text_gen_callback.start_tokens = new_start_tokens\n",
    "    text_gen_callback.on_epoch_end(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "generated text:\n",
      "here we to the                                         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_text(\"you are\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## State of training data\n",
    "\n",
    "* I learned alot on this project. It is interesting to me that with all of the data in the world there is still a shortage of industrial sized, cleaned, organized, and labeled training data. \n",
    "\n",
    "* It would seem that allthough in academia the glory goes to the next new model or algorithm when really the training data is probably more important at this point. It does not matter how fancy an algorithm is(some of them are pretty fancy) or a model is, without good training data we are limited in the problems we can tackle effectively.\n",
    "\n",
    "* During this project I researched training data sets and where they come from. Most of the open source ones come from a handful of instituions namely universities with a few corporations willing to share the training datasets or models which would not effect their market share significantly releasing them.\n",
    "\n",
    "* Even the sensitive subjetcs such as the language used in this projects training data is very important to solve very prominent problems we have.\n",
    "\n",
    "## Decisions made along the way\n",
    "* I started off this project trying to adapt a GAN to acheive this tast but there is little work done in this area and I had a lot of trouble trying to implement one of the papers where they achieve this task.\n",
    "\n",
    "* I chose this method as this model is lightweight and quick to train and tune for any specific style of language. As I mentioned at the beginning I originally tested this capabillity on the movie lines IMDB dataset and had great results. \n",
    "\n",
    "* The dataset I used turned out to be admittedly smaller than probably needed to get better results but I am confident that once I wrange up more nasty online comments the model will do much better after retraining.\n",
    "\n",
    "* The feature dimensions in the attention head made a huge difference on training time and performance. I had better results at 512 than it is currently at (256) but the time to train 25 epochs grew exponentially. If I had more Kaggle GPU hours I would have done my final training round with 512 in the attention head.\n",
    "\n",
    "* I originally tried a custom learning rate optimizing function but because of the large differences in epochs I was trying for different experiments I switched over to just using ADAM for this task.\n",
    "\n",
    "* The starting prompt matters. I found out that after looking at the popular bigrams and trigrams if I chose those sequences the model would generate sensible text more easily. \n",
    "\n",
    "* The length of the starting prompt also effects how different each output is dramatically.\n",
    "\n",
    "## Whats Next\n",
    "\n",
    "* I am going to run this model and compile a set of generated texts which are known to be toxic and add them to the training set and retrain the BERT based model I used when competing in this competition to see what results I get with the added training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Toxic_MiniGPT_SGD.keras')\n",
    "\n",
    "\n",
    "with tf.keras.utils.custom_object_scope({\n",
    "    'TransformerBlock': TransformerBlock,\n",
    "    'TokenAndPositionEmbedding': TokenAndPositionEmbedding\n",
    "}):\n",
    "    loaded_model = tf.keras.models.load_model('Toxic_MiniGPT_SGD.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path where you want to save the vocab\n",
    "file_path = 'vocab.txt'\n",
    "\n",
    "# Write each word from the vocab array to the file\n",
    "with open(file_path, 'w') as file:\n",
    "    for word in vocab:\n",
    "        file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'vocab.txt'\n",
    "\n",
    "# Read each line from the file and append it to the vocab list\n",
    "loaded_vocab = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        loaded_vocab.append(line.strip())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
