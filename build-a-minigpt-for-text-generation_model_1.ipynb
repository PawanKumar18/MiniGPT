{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniGPT For Generating Synthetic Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Toxic comments online come in many forms and in many arenas. There are currently several ways to mitigate these comments(for those organizations who wish to do so). Some of these ways include human moderators, and training machine learning models to detect toxicity in online comments.\n",
    "\n",
    "The issue with human moderators is that some of these platforms have grown so large so quickly that there are not nearly enough moderators to achieve any sense of control for most of these comments. The shear volume of toxicity and bots online makes it unrealistic to think we could do this job with humans at this point.\n",
    "\n",
    "Many companies are employing machine learning to assist with identifying toxic comments online automatically. The problem with this approach is the lack of labeled training data to train the models on.\n",
    "\n",
    "This is the problem I am going to solve using generative deep learning techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 41.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2023.8.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
      "\u001b[K     |████████████████████████████████| 774 kB 144.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk) (4.64.1)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.8.8\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "## set seeds for repeatable conclusion\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data I will be using to train the generative model was released on Kaggle as part of an ongoing series of competitions sponsored by the [Google company Jigsaw](https://en.wikipedia.org/wiki/Jigsaw_(company)).\n",
    "\n",
    "The data consists of online comments with various severity levels of toxicity. There are versions of these comments labeled by human annotators wherein they label each comment as toxic or not, or other sets where they were labeled as different categories of toxic such as hatespeech, racist/sexist, obscene, etc. Although these are the labeled datasets we would be adding the synthetic data to in order to create more training data, for this task of simply generating similar text data we will only focus on the comments themselves.\n",
    "\n",
    "The data provided by this competition includes a total of `14,251` unique toxic comments. Theses are the comments I will use to train the generative model with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "The data came in two different files.\n",
    "\n",
    "1) Comments to score: This acts as a test dataset of comments for scoring after the model was trained.\n",
    "\n",
    "2) Validation data: This was the training data for the competition wherein there are two columns. One column labeled less toxic was a comment which human annotators labeled as less toxic than its more toxic counterpart in the other column. There was no actual training data where a comment was paired with its severity rating. The models were trained using creative techniques with the validation data and other classification data sets to train a model which predicted severity of comments.\n",
    "\n",
    "Since for our purposes we are only interested in the actual text comments themselves, I will only be using those columns from these datasources.\n",
    "\n",
    "I start by reading them all into pandas dataframes, isolating the text columns from each one, and stacking them all together so we have a single column of text when it is all said and done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Jigsaw Rate Severity of Toxic Comments'\n",
      " LoadModel.ipynb\n",
      "'Toxic comment'\n",
      " build-a-minigpt-for-text-generation.ipynb\n",
      " build-a-minigpt-for-text-generation_model_1.ipynb\n",
      " vocabulary.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7537 entries, 0 to 7536\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   comment_id  7537 non-null   int64 \n",
      " 1   text        7537 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 117.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "601                               silly little boy my ass\n",
       "2031    Is that so? Than why so many people questiong ...\n",
       "1928     Wow \\nThanks! You are SO amazing! I am in awe...\n",
       "468     \"\\n\\n Send this to User:Bumpusmills1 \\n\\nPass ...\n",
       "6351    \"\\n\\nthat is your opinion. And what is your pr...\n",
       "748      man its all about captain morgans spiced rum....\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = pd.read_csv('Jigsaw Rate Severity of Toxic Comments/comments_to_score.csv')\n",
    "data1.info()\n",
    "\n",
    "## Isolate only text column\n",
    "data1 = data1['text']\n",
    "\n",
    "data1.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We can see the comments to score was the test file which contained only comments and their corresponding id's***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30108 entries, 0 to 30107\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   worker      30108 non-null  int64 \n",
      " 1   less_toxic  30108 non-null  object\n",
      " 2   more_toxic  30108 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 705.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6105</th>\n",
       "      <td>576</td>\n",
       "      <td>I Can Make You a New Man Too, In Only 15 Minu...</td>\n",
       "      <td>\"\\n\\nSandy. I've paid the price of semi-retire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8302</th>\n",
       "      <td>347</td>\n",
       "      <td>\"\\n\\nHere are your displays of impartiality an...</td>\n",
       "      <td>You're such a slut.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13194</th>\n",
       "      <td>267</td>\n",
       "      <td>Must do better\\n\\nCome on. You must be able to...</td>\n",
       "      <td>unblock|unblock my restriction from editing yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>193</td>\n",
       "      <td>I say admit to it or else we have enough reaso...</td>\n",
       "      <td>VOMIT VOMIT VOMIT VOMIT VOMIT VOMIT VOMIT VOMI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19599</th>\n",
       "      <td>283</td>\n",
       "      <td>Possible vandalism by  \\n\\nLooks at all of hi...</td>\n",
       "      <td>There not disruptive posts douchebag. All I'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24554</th>\n",
       "      <td>144</td>\n",
       "      <td>Come on man......your being rediculous just be...</td>\n",
       "      <td>````What garbage.  He did to the porn.  And th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       worker                                         less_toxic  \\\n",
       "6105      576   I Can Make You a New Man Too, In Only 15 Minu...   \n",
       "8302      347  \"\\n\\nHere are your displays of impartiality an...   \n",
       "13194     267  Must do better\\n\\nCome on. You must be able to...   \n",
       "263       193  I say admit to it or else we have enough reaso...   \n",
       "19599     283   Possible vandalism by  \\n\\nLooks at all of hi...   \n",
       "24554     144  Come on man......your being rediculous just be...   \n",
       "\n",
       "                                              more_toxic  \n",
       "6105   \"\\n\\nSandy. I've paid the price of semi-retire...  \n",
       "8302                               You're such a slut.    \n",
       "13194  unblock|unblock my restriction from editing yo...  \n",
       "263    VOMIT VOMIT VOMIT VOMIT VOMIT VOMIT VOMIT VOMI...  \n",
       "19599  There not disruptive posts douchebag. All I'm ...  \n",
       "24554  ````What garbage.  He did to the porn.  And th...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = pd.read_csv('Jigsaw Rate Severity of Toxic Comments/validation_data.csv')\n",
    "data2.info()\n",
    "\n",
    "data2.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding: Windows-1254, Confidence: 0.56\n"
     ]
    }
   ],
   "source": [
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        encoding = result['encoding']\n",
    "        confidence = result['confidence']\n",
    "    return encoding, confidence\n",
    "\n",
    "file_path = 'jigsaw-toxic-comment-train.csv'\n",
    "encoding, confidence = detect_encoding(file_path)\n",
    "\n",
    "print(f\"Detected encoding: {encoding}, Confidence: {confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-8539062653f7>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data4 = pd.read_csv('Toxic comment/jigsaw-toxic-comment-train.csv',error_bad_lines=False, engine=\"python\")\n",
      "Skipping line 191893: unexpected end of data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 191891 entries, 0 to 191890\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             191891 non-null  object\n",
      " 1   comment_text   191891 non-null  object\n",
      " 2   toxic          191891 non-null  int64 \n",
      " 3   severe_toxic   191891 non-null  int64 \n",
      " 4   obscene        191891 non-null  int64 \n",
      " 5   threat         191891 non-null  int64 \n",
      " 6   insult         191891 non-null  int64 \n",
      " 7   identity_hate  191891 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 11.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20567</th>\n",
       "      <td>3646f0c345e89ef1</td>\n",
       "      <td>Seriously? \\n\\nThis pathetic need to restore l...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48014</th>\n",
       "      <td>8042fe551cb9134d</td>\n",
       "      <td>DeCausa you are such an idiot. I was talking a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157179</th>\n",
       "      <td>da0b1337eb74f190</td>\n",
       "      <td>Fuck off this is none of your concern its betw...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55002</th>\n",
       "      <td>92fd235ce6e7bd3b</td>\n",
       "      <td>\"\\n\\n=The authority of the user Node_ue in the...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43196</th>\n",
       "      <td>73494c696b96bbfd</td>\n",
       "      <td>By the way, sorry about that stupid  edit.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58260</th>\n",
       "      <td>9bf4f685fcd8880f</td>\n",
       "      <td>Oh Sh*t! I'm shaking in my boots, big bad admi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "20567   3646f0c345e89ef1  Seriously? \\n\\nThis pathetic need to restore l...   \n",
       "48014   8042fe551cb9134d  DeCausa you are such an idiot. I was talking a...   \n",
       "157179  da0b1337eb74f190  Fuck off this is none of your concern its betw...   \n",
       "55002   92fd235ce6e7bd3b  \"\\n\\n=The authority of the user Node_ue in the...   \n",
       "43196   73494c696b96bbfd         By the way, sorry about that stupid  edit.   \n",
       "58260   9bf4f685fcd8880f  Oh Sh*t! I'm shaking in my boots, big bad admi...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  sum  \n",
       "20567       1             0        0       0       0              0    1  \n",
       "48014       1             0        0       0       0              0    1  \n",
       "157179      1             1        1       0       1              0    4  \n",
       "55002       1             0        0       0       0              0    1  \n",
       "43196       1             0        1       0       0              0    2  \n",
       "58260       1             0        1       0       1              0    3  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4 = pd.read_csv('Toxic comment/jigsaw-toxic-comment-train.csv',error_bad_lines=False, engine=\"python\")\n",
    "data4 = data4.dropna(how='all')\n",
    "data4.info()\n",
    "\n",
    "## Sum across labels to filter out clean comments\n",
    "data4['sum'] = data4.loc[:, 'toxic':].sum(axis=1)\n",
    "\n",
    "## Keep only comments with some type of label\n",
    "data4 = data4[data4['sum'] > 0]\n",
    "\n",
    "data4.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174249"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 707301 entries, 0 to 707300\n",
      "Data columns (total 34 columns):\n",
      " #   Column                               Non-Null Count   Dtype  \n",
      "---  ------                               --------------   -----  \n",
      " 0   id                                   707301 non-null  int64  \n",
      " 1   comment_text                         707301 non-null  object \n",
      " 2   toxic                                707301 non-null  float64\n",
      " 3   severe_toxicity                      707301 non-null  float64\n",
      " 4   obscene                              707301 non-null  float64\n",
      " 5   identity_attack                      707301 non-null  float64\n",
      " 6   insult                               707301 non-null  float64\n",
      " 7   threat                               707301 non-null  float64\n",
      " 8   asian                                289784 non-null  float64\n",
      " 9   atheist                              289784 non-null  float64\n",
      " 10  bisexual                             289784 non-null  float64\n",
      " 11  black                                289784 non-null  float64\n",
      " 12  buddhist                             289784 non-null  float64\n",
      " 13  christian                            289784 non-null  float64\n",
      " 14  female                               289784 non-null  float64\n",
      " 15  heterosexual                         289784 non-null  float64\n",
      " 16  hindu                                289784 non-null  float64\n",
      " 17  homosexual_gay_or_lesbian            289784 non-null  float64\n",
      " 18  intellectual_or_learning_disability  289784 non-null  float64\n",
      " 19  jewish                               289784 non-null  float64\n",
      " 20  latino                               289784 non-null  float64\n",
      " 21  male                                 289784 non-null  float64\n",
      " 22  muslim                               289784 non-null  float64\n",
      " 23  other_disability                     289784 non-null  float64\n",
      " 24  other_gender                         289784 non-null  float64\n",
      " 25  other_race_or_ethnicity              289784 non-null  float64\n",
      " 26  other_religion                       289784 non-null  float64\n",
      " 27  other_sexual_orientation             289784 non-null  float64\n",
      " 28  physical_disability                  289784 non-null  float64\n",
      " 29  psychiatric_or_mental_illness        289784 non-null  float64\n",
      " 30  transgender                          289784 non-null  float64\n",
      " 31  white                                289784 non-null  float64\n",
      " 32  sexual_explicit                      707301 non-null  float64\n",
      " 33  sum                                  707301 non-null  float64\n",
      "dtypes: float64(32), int64(1), object(1)\n",
      "memory usage: 183.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79721</th>\n",
       "      <td>523990</td>\n",
       "      <td>A lifetime of slime and privilege wallowing in...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538237</th>\n",
       "      <td>5902299</td>\n",
       "      <td>Actually.....I didn't trust either one of them...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131207</th>\n",
       "      <td>684403</td>\n",
       "      <td>Gillam was directly involved in Pebble buying ...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491123</th>\n",
       "      <td>5751646</td>\n",
       "      <td>Oh Robert, do you have that gang of crooks peg...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394514</th>\n",
       "      <td>5425984</td>\n",
       "      <td>Shhh.  Don't let Gerald Butts hear this.  More...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13596</th>\n",
       "      <td>293670</td>\n",
       "      <td>You have a good point about the Alaska Constit...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                       comment_text     toxic  \\\n",
       "79721    523990  A lifetime of slime and privilege wallowing in...  0.166667   \n",
       "538237  5902299  Actually.....I didn't trust either one of them...  0.100000   \n",
       "131207   684403  Gillam was directly involved in Pebble buying ...  0.166667   \n",
       "491123  5751646  Oh Robert, do you have that gang of crooks peg...  0.300000   \n",
       "394514  5425984  Shhh.  Don't let Gerald Butts hear this.  More...  0.200000   \n",
       "13596    293670  You have a good point about the Alaska Constit...  0.200000   \n",
       "\n",
       "        severe_toxicity   obscene  identity_attack    insult  threat  asian  \\\n",
       "79721               0.0  0.000000              0.0  0.166667     0.0    NaN   \n",
       "538237              0.1  0.000000              0.0  0.000000     0.0    NaN   \n",
       "131207              0.0  0.000000              0.0  0.166667     0.0    NaN   \n",
       "491123              0.0  0.000000              0.0  0.300000     0.0    NaN   \n",
       "394514              0.0  0.044444              0.0  0.177778     0.0    NaN   \n",
       "13596               0.0  0.000000              0.2  0.200000     0.0    NaN   \n",
       "\n",
       "        atheist  ...  other_gender  other_race_or_ethnicity  other_religion  \\\n",
       "79721       NaN  ...           NaN                      NaN             NaN   \n",
       "538237      NaN  ...           NaN                      NaN             NaN   \n",
       "131207      NaN  ...           NaN                      NaN             NaN   \n",
       "491123      NaN  ...           NaN                      NaN             NaN   \n",
       "394514      NaN  ...           NaN                      NaN             NaN   \n",
       "13596       NaN  ...           NaN                      NaN             NaN   \n",
       "\n",
       "        other_sexual_orientation  physical_disability  \\\n",
       "79721                        NaN                  NaN   \n",
       "538237                       NaN                  NaN   \n",
       "131207                       NaN                  NaN   \n",
       "491123                       NaN                  NaN   \n",
       "394514                       NaN                  NaN   \n",
       "13596                        NaN                  NaN   \n",
       "\n",
       "        psychiatric_or_mental_illness  transgender  white  sexual_explicit  \\\n",
       "79721                             NaN          NaN    NaN         0.000000   \n",
       "538237                            NaN          NaN    NaN         0.100000   \n",
       "131207                            NaN          NaN    NaN         0.000000   \n",
       "491123                            NaN          NaN    NaN         0.000000   \n",
       "394514                            NaN          NaN    NaN         0.022222   \n",
       "13596                             NaN          NaN    NaN         0.000000   \n",
       "\n",
       "             sum  \n",
       "79721   0.666667  \n",
       "538237  0.600000  \n",
       "131207  0.666667  \n",
       "491123  1.200000  \n",
       "394514  0.888889  \n",
       "13596   1.200000  \n",
       "\n",
       "[6 rows x 34 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5 = pd.read_csv('Toxic comment/jigsaw-unintended-bias-train.csv')\n",
    "data5.info()\n",
    "\n",
    "# drop_columns = [\n",
    "#     'created_date',\n",
    "#     'publication_id',\n",
    "#     'parent_id',\n",
    "#     'article_id', \n",
    "#     'rating', \n",
    "#     'funny', \n",
    "#     'wow', \n",
    "#     'sad', \n",
    "#     'likes', \n",
    "#     'disagree', \n",
    "#     'identity_annotator_count', \n",
    "#     'toxicity_annotator_count'\n",
    "# ]\n",
    "\n",
    "# data5 = data5.drop(columns=drop_columns, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Sum across labels to filter out clean comments\n",
    "data5['sum'] = data5.loc[:, 'toxic':].sum(axis=1)\n",
    "\n",
    "## Keep only comments with some type of label\n",
    "data5 = data5[data5['sum'] > 0]\n",
    "\n",
    "data5.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***This was the data provided to validate the models performance during training. The three columns are workers(annotators) and the other two are text columns which we will use both to train our generative model with.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>313</td>\n",
       "      <td>This article sucks \\n\\nwoo woo wooooooo</td>\n",
       "      <td>WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>\"And yes, people should recognize that but the...</td>\n",
       "      <td>Daphne Guinness \\n\\nTop of the mornin' my fav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>Western Media?\\n\\nYup, because every crime in...</td>\n",
       "      <td>\"Atom you don't believe actual photos of mastu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>347</td>\n",
       "      <td>And you removed it! You numbskull! I don't car...</td>\n",
       "      <td>You seem to have sand in your vagina.\\n\\nMight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539</td>\n",
       "      <td>smelly vagina \\n\\nBluerasberry why don't you ...</td>\n",
       "      <td>hey \\n\\nway to support nazis, you racist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   worker                                         less_toxic  \\\n",
       "0     313            This article sucks \\n\\nwoo woo wooooooo   \n",
       "1     188  \"And yes, people should recognize that but the...   \n",
       "2      82   Western Media?\\n\\nYup, because every crime in...   \n",
       "3     347  And you removed it! You numbskull! I don't car...   \n",
       "4     539   smelly vagina \\n\\nBluerasberry why don't you ...   \n",
       "\n",
       "                                          more_toxic  \n",
       "0  WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...  \n",
       "1   Daphne Guinness \\n\\nTop of the mornin' my fav...  \n",
       "2  \"Atom you don't believe actual photos of mastu...  \n",
       "3  You seem to have sand in your vagina.\\n\\nMight...  \n",
       "4           hey \\n\\nway to support nazis, you racist  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all columns into a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'comment_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'comment_text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9db17b665b3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'more_toxic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata5\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m## Isolate text column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'comment_text'"
     ]
    }
   ],
   "source": [
    "## Isolate text column\n",
    "data2 = data2['more_toxic']\n",
    "data4 = data4['comment_text']\n",
    "data5 = data5['comment_text']\n",
    "\n",
    "## Isolate text column\n",
    "data3 = pd.read_csv('Jigsaw Rate Severity of Toxic Comments/validation_data.csv')\n",
    "data3 = data3['less_toxic']\n",
    "\n",
    "text_column = pd.concat([data1, data2, data3, data4, data5], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "so block me, stupid prick!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               7\n",
       " You are not cool \\n\\nYou are the most hated fool in the world! You are a load of %$#! Barry Dejay rocks!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                7\n",
       "Go to hell you ignorant asshole!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         6\n",
       "\"\\nWhat I was saying? You really think that my comment was more rude or an attack than \"\"I don't give a damn about Egyptians\"\"????  \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    5\n",
       "No I won't so shut up king bee, I did no wrong so shut up or you'll be reported                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          5\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ..\n",
       " Thank you SO MUCH for your Nazi-like oversight! \\n\\n...but you can shove your advice up your ass. I just like messing with self-important dickheads who think they know everything, and damned if Wikipedia isn't full of them. Signed, your number one dirty Jew fan, Mel Gibson.                                                                                                                                                                                                                                                                                                                                                                                                                                      1\n",
       "FUCK OFF YOU TOTAL CRETINOUS ASSHOLE 9-)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 1\n",
       " FUCK YOU  \\n\\nthe subject says it all bitch.\\n (24.23.87.244  )                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         1\n",
       "\"== October 2009 ==\\n\\nI saw your evil edit to Thomas and the Magic Railroad and you will be assigned to \"\"correct-your-editing\"\" school. Stop your stupid\\nedits or else you will be blocked from editing by a sysop.   \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               1\n",
       "We are in this together - the producers and citizens of the State.  Transparency is paramount, dispense with all the \"proprietary\" nonsense or kill the tax credits.  It cannot be the blind man's game we are in.  There is no confidence the citizens are even coming out even.  \\n“Should the oil industry remain a vital part of Alaska's economy, or should the government use it as a slush fund to balance their checkbook?” asked a newsletter published Thursday by the Support Industry Alliance, which represents businesses that work in oil and gas and mining.\"  Perhaps it's time to consider the alternative and look at NOT having them as part.  It is, after all, our survival too.  And we count.    1\n",
       "Length: 15487, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_column.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like between the data provided for the competition there are many duplicates. However we can see that some comments are reused many more times than other comments. For example the most used comments were repeated `19` times in the datasets while others only `2` times. \n",
    "\n",
    "Since the duplications are not balanced if we left the data like this I am afraid we would be biasing the model towards the comments which were present more in the data. \n",
    "\n",
    "I will remove all duplicate comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total numer of comments in text data = 17928\n",
      "Numer of unique comments in text data = 15488\n",
      "Duplicate comments dropped\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total numer of comments in text data = {len(text_column)}\")\n",
    "print(f\"Numer of unique comments in text data = {len(text_column.unique())}\")\n",
    "\n",
    "text_column = text_column.drop_duplicates()\n",
    "print(\"Duplicate comments dropped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the toxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>noun_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>106.710000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>25.73000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>152.475552</td>\n",
       "      <td>28.398099</td>\n",
       "      <td>45.45724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25.500000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>5.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>51.500000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>11.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>136.000000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>27.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>844.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>322.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_count  verb_count  noun_count\n",
       "count  100.000000  100.000000   100.00000\n",
       "mean   106.710000   17.850000    25.73000\n",
       "std    152.475552   28.398099    45.45724\n",
       "min      5.000000    0.000000     0.00000\n",
       "25%     25.500000    3.750000     5.00000\n",
       "50%     51.500000    9.000000    11.00000\n",
       "75%    136.000000   22.250000    27.00000\n",
       "max    844.000000  162.000000   322.00000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "data['text'] = text_column\n",
    "data = data.sample(100)\n",
    "\n",
    "# Function to calculate word count\n",
    "def count_words(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "# Function to calculate verb count\n",
    "def count_verbs(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    verb_count = len([word for word, tag in tagged_words if tag.startswith('V')])\n",
    "    return verb_count\n",
    "\n",
    "# Function to calculate noun count\n",
    "def count_nouns(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    noun_count = len([word for word, tag in tagged_words if tag.startswith('N')])\n",
    "    return noun_count\n",
    "\n",
    "# Add word count column\n",
    "data['word_count'] = data['text'].apply(count_words)\n",
    "\n",
    "# Add verb count column\n",
    "data['verb_count'] = data['text'].apply(count_verbs)\n",
    "\n",
    "# Add noun count column\n",
    "data['noun_count'] = data['text'].apply(count_nouns)\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = data['word_count'].plot(kind='kde')\n",
    "data['verb_count'].plot(kind='kde', ax=ax)\n",
    "data['noun_count'].plot(kind='kde', ax=ax)\n",
    "\n",
    "ax.legend(['Word Count', 'Verb Count', 'Noun Count'])\n",
    "ax.set_title('Distribution of Word Count, Verb Count, and Noun Count')\n",
    "ax.set_xlabel('Count')\n",
    "ax.set_ylabel('Density')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these comments on average are quite short in length and contain more nouns than verbs on average.\n",
    "\n",
    "Since we have not done any cleaning of the data yet these distributions are not exact as the nltk package is not currently looking for misspelled words or different versions of word spellings which are used online sometimes.\n",
    "\n",
    "For example if a user knows that the platform they are on has limitations on language than they may spell a profane word to try to fool any auto detecting systems such as `Fuck==>Fxck, F*ck, Fukk, Fuuu*uukk`, etc.\n",
    "\n",
    "Therefore these counts will not detect all nouns and verbs but should give a decent sample.\n",
    "\n",
    "Knowing the underlying distributions of some of these features is important because after the synthetic data is generated we would most likely want it to follow the same distributions for these attributes of the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the most common N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>((8===D~~penis, 8===D~~penis), 117)</td>\n",
       "      <td>((8===D~~penis, 8===D~~penis, 8===D~~penis), 116)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((user, :), 87)</td>\n",
       "      <td>((!, !, !), 42)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>((,, and), 66)</td>\n",
       "      <td>((Gamaliel, enjoys, making), 41)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>((!, !), 57)</td>\n",
       "      <td>((enjoys, making, love), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(('', ''), 50)</td>\n",
       "      <td>((making, love, to), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>((``, ''), 49)</td>\n",
       "      <td>((love, to, animals), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>((Gamaliel, enjoys), 41)</td>\n",
       "      <td>((to, animals, ,), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>((enjoys, making), 41)</td>\n",
       "      <td>((animals, ,, and), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>((love, to), 41)</td>\n",
       "      <td>((,, and, Bill), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>((,, you), 40)</td>\n",
       "      <td>((and, Bill, Maher), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>((you, are), 40)</td>\n",
       "      <td>((Bill, Maher, .), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>((making, love), 40)</td>\n",
       "      <td>((Maher, ., Link), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>((to, animals), 40)</td>\n",
       "      <td>((., Link, with), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>((animals, ,), 40)</td>\n",
       "      <td>((Link, with, picture), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>((and, Bill), 40)</td>\n",
       "      <td>((with, picture, showing), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>((Bill, Maher), 40)</td>\n",
       "      <td>((picture, showing, this), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>((Maher, .), 40)</td>\n",
       "      <td>((showing, this, :), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>((., Link), 40)</td>\n",
       "      <td>((this, :, just), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>((Link, with), 40)</td>\n",
       "      <td>((:, just, google), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>((with, picture), 40)</td>\n",
       "      <td>((just, google, meatspin), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>((picture, showing), 40)</td>\n",
       "      <td>((google, meatspin, .), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>((showing, this), 40)</td>\n",
       "      <td>((meatspin, ., Gamaliel), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>((this, :), 40)</td>\n",
       "      <td>((., Gamaliel, enjoys), 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>((:, just), 40)</td>\n",
       "      <td>((you, fuck, misterwiki), 37)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>((just, google), 40)</td>\n",
       "      <td>((fuck, misterwiki, .), 37)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>((google, meatspin), 40)</td>\n",
       "      <td>((misterwiki, ., you), 36)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>((meatspin, .), 40)</td>\n",
       "      <td>((., you, fuck), 36)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>((., Gamaliel), 40)</td>\n",
       "      <td>((,, you, are), 35)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>((you, fuck), 37)</td>\n",
       "      <td>((else, ,, you), 33)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>((fuck, misterwiki), 37)</td>\n",
       "      <td>((:, user, :), 33)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>((misterwiki, .), 37)</td>\n",
       "      <td>((user, :, user), 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>((., you), 37)</td>\n",
       "      <td>((user, :, precious), 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>((are, not), 35)</td>\n",
       "      <td>((:, precious, Roy), 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>((else, ,), 33)</td>\n",
       "      <td>((precious, Roy, go), 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>((:, user), 33)</td>\n",
       "      <td>((Roy, go, away), 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>((someone, else), 33)</td>\n",
       "      <td>((go, away, and), 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>((:, precious), 32)</td>\n",
       "      <td>((away, and, bother), 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>((precious, Roy), 32)</td>\n",
       "      <td>((and, bother, someone), 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>((Roy, go), 32)</td>\n",
       "      <td>((bother, someone, else), 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>((go, away), 32)</td>\n",
       "      <td>((someone, else, ,), 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>((away, and), 32)</td>\n",
       "      <td>((you, are, not), 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>((and, bother), 32)</td>\n",
       "      <td>((are, not, wanted), 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>((bother, someone), 32)</td>\n",
       "      <td>((not, wanted, here), 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>((not, wanted), 32)</td>\n",
       "      <td>((wanted, here, ,), 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>((wanted, here), 32)</td>\n",
       "      <td>((here, ,, stop), 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>((here, ,), 32)</td>\n",
       "      <td>((,, stop, stalking), 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>((,, stop), 32)</td>\n",
       "      <td>((stop, stalking, user), 31)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>((stop, stalking), 32)</td>\n",
       "      <td>((stalking, user, :), 31)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>((stalking, user), 31)</td>\n",
       "      <td>((2009, ``, ''), 15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>((., I), 28)</td>\n",
       "      <td>((., This, is), 7)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                bigrams  \\\n",
       "0   ((8===D~~penis, 8===D~~penis), 117)   \n",
       "1                       ((user, :), 87)   \n",
       "2                        ((,, and), 66)   \n",
       "3                          ((!, !), 57)   \n",
       "4                        (('', ''), 50)   \n",
       "5                        ((``, ''), 49)   \n",
       "6              ((Gamaliel, enjoys), 41)   \n",
       "7                ((enjoys, making), 41)   \n",
       "8                      ((love, to), 41)   \n",
       "9                        ((,, you), 40)   \n",
       "10                     ((you, are), 40)   \n",
       "11                 ((making, love), 40)   \n",
       "12                  ((to, animals), 40)   \n",
       "13                   ((animals, ,), 40)   \n",
       "14                    ((and, Bill), 40)   \n",
       "15                  ((Bill, Maher), 40)   \n",
       "16                     ((Maher, .), 40)   \n",
       "17                      ((., Link), 40)   \n",
       "18                   ((Link, with), 40)   \n",
       "19                ((with, picture), 40)   \n",
       "20             ((picture, showing), 40)   \n",
       "21                ((showing, this), 40)   \n",
       "22                      ((this, :), 40)   \n",
       "23                      ((:, just), 40)   \n",
       "24                 ((just, google), 40)   \n",
       "25             ((google, meatspin), 40)   \n",
       "26                  ((meatspin, .), 40)   \n",
       "27                  ((., Gamaliel), 40)   \n",
       "28                    ((you, fuck), 37)   \n",
       "29             ((fuck, misterwiki), 37)   \n",
       "30                ((misterwiki, .), 37)   \n",
       "31                       ((., you), 37)   \n",
       "32                     ((are, not), 35)   \n",
       "33                      ((else, ,), 33)   \n",
       "34                      ((:, user), 33)   \n",
       "35                ((someone, else), 33)   \n",
       "36                  ((:, precious), 32)   \n",
       "37                ((precious, Roy), 32)   \n",
       "38                      ((Roy, go), 32)   \n",
       "39                     ((go, away), 32)   \n",
       "40                    ((away, and), 32)   \n",
       "41                  ((and, bother), 32)   \n",
       "42              ((bother, someone), 32)   \n",
       "43                  ((not, wanted), 32)   \n",
       "44                 ((wanted, here), 32)   \n",
       "45                      ((here, ,), 32)   \n",
       "46                      ((,, stop), 32)   \n",
       "47               ((stop, stalking), 32)   \n",
       "48               ((stalking, user), 31)   \n",
       "49                         ((., I), 28)   \n",
       "\n",
       "                                             trigrams  \n",
       "0   ((8===D~~penis, 8===D~~penis, 8===D~~penis), 116)  \n",
       "1                                     ((!, !, !), 42)  \n",
       "2                    ((Gamaliel, enjoys, making), 41)  \n",
       "3                        ((enjoys, making, love), 40)  \n",
       "4                            ((making, love, to), 40)  \n",
       "5                           ((love, to, animals), 40)  \n",
       "6                              ((to, animals, ,), 40)  \n",
       "7                             ((animals, ,, and), 40)  \n",
       "8                                ((,, and, Bill), 40)  \n",
       "9                            ((and, Bill, Maher), 40)  \n",
       "10                             ((Bill, Maher, .), 40)  \n",
       "11                             ((Maher, ., Link), 40)  \n",
       "12                              ((., Link, with), 40)  \n",
       "13                        ((Link, with, picture), 40)  \n",
       "14                     ((with, picture, showing), 40)  \n",
       "15                     ((picture, showing, this), 40)  \n",
       "16                           ((showing, this, :), 40)  \n",
       "17                              ((this, :, just), 40)  \n",
       "18                            ((:, just, google), 40)  \n",
       "19                     ((just, google, meatspin), 40)  \n",
       "20                        ((google, meatspin, .), 40)  \n",
       "21                      ((meatspin, ., Gamaliel), 40)  \n",
       "22                        ((., Gamaliel, enjoys), 40)  \n",
       "23                      ((you, fuck, misterwiki), 37)  \n",
       "24                        ((fuck, misterwiki, .), 37)  \n",
       "25                         ((misterwiki, ., you), 36)  \n",
       "26                               ((., you, fuck), 36)  \n",
       "27                                ((,, you, are), 35)  \n",
       "28                               ((else, ,, you), 33)  \n",
       "29                                 ((:, user, :), 33)  \n",
       "30                              ((user, :, user), 32)  \n",
       "31                          ((user, :, precious), 32)  \n",
       "32                           ((:, precious, Roy), 32)  \n",
       "33                          ((precious, Roy, go), 32)  \n",
       "34                              ((Roy, go, away), 32)  \n",
       "35                              ((go, away, and), 32)  \n",
       "36                          ((away, and, bother), 32)  \n",
       "37                       ((and, bother, someone), 32)  \n",
       "38                      ((bother, someone, else), 32)  \n",
       "39                           ((someone, else, ,), 32)  \n",
       "40                              ((you, are, not), 32)  \n",
       "41                           ((are, not, wanted), 32)  \n",
       "42                          ((not, wanted, here), 32)  \n",
       "43                            ((wanted, here, ,), 32)  \n",
       "44                              ((here, ,, stop), 32)  \n",
       "45                          ((,, stop, stalking), 32)  \n",
       "46                       ((stop, stalking, user), 31)  \n",
       "47                          ((stalking, user, :), 31)  \n",
       "48                               ((2009, ``, ''), 15)  \n",
       "49                                 ((., This, is), 7)  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text into words\n",
    "data['words'] = data['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Get bigrams and trigrams for each row\n",
    "data['bigrams']   = data['words'].apply(lambda x: list(ngrams(x, 2)))\n",
    "data['trigrams']  = data['words'].apply(lambda x: list(ngrams(x, 3)))\n",
    "# data['quadgrams'] = data['words'].apply(lambda x: list(ngrams(x, 4)))\n",
    "\n",
    "# Count the occurrences of bigrams and trigrams\n",
    "bigram_counts   = Counter([gram for grams in data['bigrams'] for gram in grams])\n",
    "trigram_counts  = Counter([gram for grams in data['trigrams'] for gram in grams])\n",
    "# quadgram_counts = Counter([gram for grams in data['quadgrams'] for gram in grams])\n",
    "\n",
    "# Get the most common bigrams, trigrams, and quadgrams\n",
    "most_common_bigrams   = bigram_counts.most_common(50)\n",
    "most_common_trigrams  = trigram_counts.most_common(50)\n",
    "# most_common_quadgrams = quadgram_counts.most_common(50)\n",
    "\n",
    "df_common_grams = pd.DataFrame()\n",
    "df_common_grams['bigrams']   = most_common_bigrams\n",
    "df_common_grams['trigrams']  = most_common_trigrams\n",
    "# df_common_grams['quadgrams'] = most_common_quadgrams\n",
    "\n",
    "# # Display the results\n",
    "# print('Most common bigrams:')\n",
    "# for bigram, count in most_common_bigrams:\n",
    "#     print(' '.join(bigram), count)\n",
    "\n",
    "# print('\\nMost common trigrams:')\n",
    "# for trigram, count in most_common_trigrams:\n",
    "#     print(' '.join(trigram), count)\n",
    "    \n",
    "# print('\\nMost common quadgrams:')\n",
    "# for quadgram, count in most_common_quadgrams:\n",
    "#     print(' '.join(quadgram), count)\n",
    "\n",
    "\n",
    "df_common_grams.iloc[:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the initial 10 or so most common bi-grams and tri-grams are repetitive punctuation marks.\n",
    "\n",
    "Traditionally these would be cleaned and removed when training models for NLP tasks, however due to the nature of this work many of these traditional techniques will limit the models ability to predict toxicity as well as with clean text.\n",
    "\n",
    "I happened to have competed in this competition and one thing all of us learned was that leaving capital letters and punctuation improved the models ability to infer toxicity and especially levels of toxicity. \n",
    "\n",
    "For example a phrase such as:\n",
    "\n",
    "`Are you kidding?`\n",
    "\n",
    "Conveys a much different meaning than the same words but put this way:\n",
    "\n",
    "`ARE YOU KIDDING!!!??`\n",
    "\n",
    "Traditional NLP techniques would have us convert all characters to lower case and remove punctuation so the model will interpret both of those texts the exact same way.\n",
    "\n",
    "When training sentiment based models or models where feeling and emotion is being conveyed in some way such as toxicity of comments, it is more than just the raw content of the words alone which gives the meaning. The puncuation and capitalizations are very expressive forms of language and as such for these problems do better left in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "* First we need load in our text column as tensorflow formatted dataset\n",
    "\n",
    "* Next we shuffle the data to avoid any patterns which may have been present\n",
    "\n",
    "* We then slice the data into batches for processing\n",
    "\n",
    "* Vectorize the text which will be used to create a corpus of vocabulary used when training and act as vector representations of our text\n",
    "\n",
    "* Create the corpus of vocabulary which is used to train and evaluate throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = 100000  ## Only consider the top 20k words\n",
    "maxlen = 80  ## Max sequence length\n",
    "batch_size = 128  ## Data loading batch sizes\n",
    "\n",
    "# Create a dataset from the pandas column\n",
    "text_column = text_column.astype(str)  # Convert all elements to strings\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(text_column)\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "text_ds = text_ds.shuffle(buffer_size=128)\n",
    "text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "# def custom_standardization(input_string):\n",
    "#     \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
    "#     lowercased = tf.strings.lower(input_string)\n",
    "#     stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "#     return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
    "\n",
    "\n",
    "## Create a vectorization layer and adapt it to the text\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=None,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  ## To get words back from token indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Labels\n",
    "\n",
    "Since we are building a generative auto-regressive model, we must train it to predict the next word by looking backwards and using the previous tokens to predict the highest probability for the next token.\n",
    "\n",
    "This is fairly easy to create labels for because we simply shuffle the `TRUE` data be one token and then when training the model compares the predicted text with the next indexed word.\n",
    "\n",
    "We can inspect what these samples and labels look like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to create target column\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
    "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "\" I already gave you the source for all my edits The Slur database, so I will not continue to play your little game. If you weren't so lazy and intent on harassment, you could use Google to search for \"\"Gargamel\"\" and \"\"Jew\"\". You get more than 400 hits including white supremacist sites and an academic paper dating to 1996. It's obviously a real slur with some usage. Nice try at being obdurate though. I'm sure there's a slur that\n",
      "\n",
      "Target Sequence:\n",
      "I already gave you the source for all my edits The Slur database, so I will not continue to play your little game. If you weren't so lazy and intent on harassment, you could use Google to search for \"\"Gargamel\"\" and \"\"Jew\"\". You get more than 400 hits including white supremacist sites and an academic paper dating to 1996. It's obviously a real slur with some usage. Nice try at being obdurate though. I'm sure there's a slur that describes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "\" I'd say Zivo Blato is not heavy metal but hard rock, although since there are no good Croatian heavy metal bands it's not an important mistake. In my opinion, they are really fucking great. The songs \"\"Underage Girl\"\", \"\"Kill Yourself\"\" and \"\"Abortion\"\" are my favourites. \"                                 \n",
      "\n",
      "Target Sequence:\n",
      "I'd say Zivo Blato is not heavy metal but hard rock, although since there are no good Croatian heavy metal bands it's not an important mistake. In my opinion, they are really fucking great. The songs \"\"Underage Girl\"\", \"\"Kill Yourself\"\" and \"\"Abortion\"\" are my favourites. \"                                  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "3RR RULE That's FOUR for you today already at Saudi Arabia, Yuber. I'm assuming good faith and not going to report you on it yet. Knock it off.                                                    \n",
      "\n",
      "Target Sequence:\n",
      "RULE That's FOUR for you today already at Saudi Arabia, Yuber. I'm assuming good faith and not going to report you on it yet. Knock it off.                                                     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "I confess to having complete (and apparently blissful) ignorance of Jordan, but I've glanced at the article. Is this a woman or a soap opera!?. I don't think there was much to change in terms of the description of the various diseases. It is mentioned that she is famous for the size of her breasts: am I correct in assuming this is because they are grotesquely large rather than vanishingly small? 04:09 11 Jul 2003 (UTC)    \n",
      "\n",
      "Target Sequence:\n",
      "confess to having complete (and apparently blissful) ignorance of Jordan, but I've glanced at the article. Is this a woman or a soap opera!?. I don't think there was much to change in terms of the description of the various diseases. It is mentioned that she is famous for the size of her breasts: am I correct in assuming this is because they are grotesquely large rather than vanishingly small? 04:09 11 Jul 2003 (UTC)     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "Please just let us know what you found. I'm not asking for details. Just let u know if the same IP address was used or what ever else you found that caused you to determien that he was usign sockpuppets. No details, jsut what was it? I am one of Rob;s most hated enemies, so I think I'm the perfect person to ask this. Vandalism from an IP address used by Deskana was revealed using checkuser adn he was able\n",
      "\n",
      "Target Sequence:\n",
      "just let us know what you found. I'm not asking for details. Just let u know if the same IP address was used or what ever else you found that caused you to determien that he was usign sockpuppets. No details, jsut what was it? I am one of Rob;s most hated enemies, so I think I'm the perfect person to ask this. Vandalism from an IP address used by Deskana was revealed using checkuser adn he was able to\n"
     ]
    }
   ],
   "source": [
    "## Select samples from the training data set to inspect\n",
    "sample = text_ds.take(5) \n",
    "\n",
    "## Display some samples\n",
    "for x, y in sample:\n",
    "    # Convert token indices back to words\n",
    "    input_words  = [vocab[i] for i in x[0].numpy()]\n",
    "    target_words = [vocab[i] for i in y[0].numpy()]\n",
    "\n",
    "    print(\"\\n\\n\\n\\nInput Sequence:\")\n",
    "    print(\" \".join(input_words))\n",
    "    print(\"\\nTarget Sequence:\")\n",
    "    print(\" \".join(target_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***We can see that the target or label sequence is merely our ground truth text sequence we have just shifted by `1` token. This is what our model will use to evaluate during training.***\n",
    "\n",
    "* ***Cell below was for loading in and preprocessing the IMBD movie quotes dataset. This is the dataset I tested this approach on first.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# !tar -xf aclImdb_v1.tar.gz\n",
    "\n",
    "# batch_size = 128\n",
    "\n",
    "# # The dataset contains each review in a separate text file\n",
    "# # The text files are present in four different folders\n",
    "# # Create a list all files\n",
    "# filenames = []\n",
    "# directories = [\n",
    "#     \"aclImdb/train/pos\",\n",
    "#     \"aclImdb/train/neg\",\n",
    "#     \"aclImdb/test/pos\",\n",
    "#     \"aclImdb/test/neg\",\n",
    "# ]\n",
    "# for dir in directories:\n",
    "#     for f in os.listdir(dir):\n",
    "#         filenames.append(os.path.join(dir, f))\n",
    "\n",
    "# print(f\"{len(filenames)} files\")\n",
    "\n",
    "# # Create a dataset from text files\n",
    "# random.shuffle(filenames)\n",
    "# text_ds = tf.data.TextLineDataset(filenames)\n",
    "# text_ds = text_ds.shuffle(buffer_size=256)\n",
    "# text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "# def custom_standardization(input_string):\n",
    "#     \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
    "#     lowercased = tf.strings.lower(input_string)\n",
    "#     stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "#     return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
    "\n",
    "\n",
    "# # Create a vectorization layer and adapt it to the text\n",
    "# vectorize_layer = TextVectorization(\n",
    "#     standardize=custom_standardization,\n",
    "#     max_tokens=vocab_size - 1,\n",
    "#     output_mode=\"int\",\n",
    "#     output_sequence_length=maxlen + 1,\n",
    "# )\n",
    "# vectorize_layer.adapt(text_ds)\n",
    "# vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
    "\n",
    "# ## Functoin to create target column\n",
    "# def prepare_lm_inputs_labels(text):\n",
    "#     \"\"\"\n",
    "#     Shift word sequences by 1 position so that the target for position (i) is\n",
    "#     word at position (i+1). The model will use all words up till position (i)\n",
    "#     to predict the next word.\n",
    "#     \"\"\"\n",
    "#     text = tf.expand_dims(text, -1)\n",
    "#     tokenized_sentences = vectorize_layer(text)\n",
    "#     x = tokenized_sentences[:, :-1]\n",
    "#     y = tokenized_sentences[:, 1:]\n",
    "#     return x, y\n",
    "\n",
    "\n",
    "# text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
    "# text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Transformer Block and Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Creates a mask for causal (auto-regressive) self-attention. The returned mask has the shape \n",
    "    [batch_size, n_dest, n_src], where each entry at position (i, j, k) will be 1 if j >= k and 0 otherwise. \n",
    "    This is used to prevent the attention mechanism from attending to future positions during the forward pass.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of sequences in each batch.\n",
    "        n_dest (int): Number of destination attention heads.\n",
    "        n_src (int): Number of source attention heads.\n",
    "        dtype (tf.DType): Type of the output tensor.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: A tensor of shape [batch_size, n_dest, n_src] representing the mask.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create two range tensors i and j, where i has shape [n_dest, 1] and j has shape [n_src]\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "\n",
    "    # Create a mask where entry (i, j) is True if i >= j - n_src + n_dest and False otherwise\n",
    "    m = i >= j - n_src + n_dest\n",
    "\n",
    "    # Cast the mask to the desired data type\n",
    "    mask = tf.cast(m, dtype)\n",
    "\n",
    "    # Reshape the mask to have shape [1, n_dest, n_src]\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "\n",
    "    # Create a tensor with shape [2] that represents the multiples for tiling\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "\n",
    "    # Tile the mask tensor to have shape [batch_size, n_dest, n_src]\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    A Transformer block that includes multi-head self-attention and a feed-forward neural network.\n",
    "    Each of these two components has a residual connection and is followed by layer normalization.\n",
    "\n",
    "    Attributes:\n",
    "        att (layers.MultiHeadAttention): Multi-head self-attention layer.\n",
    "        ffn (keras.Sequential): Feed-forward neural network.\n",
    "        layernorm1 (layers.LayerNormalization): Layer normalization after the self-attention.\n",
    "        layernorm2 (layers.LayerNormalization): Layer normalization after the feed-forward network.\n",
    "        dropout1 (layers.Dropout): Dropout layer after the self-attention.\n",
    "        dropout2 (layers.Dropout): Dropout layer after the feed-forward network.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer block.\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int): Dimensionality of the input embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            ff_dim (int): Number of units in the hidden layer of the feed-forward network.\n",
    "            rate (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer block.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): Input tensor of shape [batch_size, seq_len, embed_dim].\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor of shape [batch_size, seq_len, embed_dim].\n",
    "        \"\"\"\n",
    "        # Compute the shapes\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "\n",
    "        # Create the causal mask for the multi-head self-attention\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "\n",
    "        # Compute the output of the multi-head self-attention\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "\n",
    "        # Apply dropout to the attention output\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "\n",
    "        # Add the attention output to the inputs (residual connection) and normalize the result\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "\n",
    "        # Compute the output of the feed-forward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "\n",
    "        # Apply dropout to the feed-forward output\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "\n",
    "        # Add the feed-forward output to the previous output (residual connection) and normalize the result\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Embedding layer\n",
    "\n",
    "***Create two separate embedding layers:***\n",
    "\n",
    "1) One for tokens \n",
    "\n",
    "2) One for token indices(positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    \"\"\"\n",
    "    Layer for combining token and positional embeddings. Token embeddings provide the model\n",
    "    with understanding of the meaning of each token, while positional embeddings provide\n",
    "    information about the position of each token in the sequence.\n",
    "\n",
    "    Attributes:\n",
    "        token_emb (layers.Embedding): Token embedding layer.\n",
    "        pos_emb (layers.Embedding): Position embedding layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Initializes the TokenAndPositionEmbedding layer.\n",
    "\n",
    "        Args:\n",
    "            maxlen (int): Maximum length of the sequences for positional encoding.\n",
    "            vocab_size (int): Size of the vocabulary for token encoding.\n",
    "            embed_dim (int): Dimensionality of the output embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the TokenAndPositionEmbedding layer.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): Input tensor of shape [batch_size, seq_len].\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor of shape [batch_size, seq_len, embed_dim], resulting from\n",
    "            adding token embeddings and position embeddings.\n",
    "        \"\"\"\n",
    "        # Compute the maximum sequence length\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "\n",
    "        # Create a range tensor representing positions\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "\n",
    "        # Compute the position embeddings\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        # Compute the token embeddings\n",
    "        x = self.token_emb(x)\n",
    "\n",
    "        # Add the token embeddings and position embeddings\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Mini GPT\n",
    "\n",
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vocab_size = 30000  # Only consider the top 20k words\n",
    "maxlen = 80  # Max sequence size\n",
    "embed_dim = 512  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 512  # Hidden layer size in feed forward network inside transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: keras in /usr/lib/python3/dist-packages (2.11.0)\n",
      "Collecting keras\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m166.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/lib/python3/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in ./.local/lib/python3.8/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in ./.local/lib/python3.8/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Collecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m194.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.6.2)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/lib/python3/dist-packages (from tensorflow) (2.10.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.33.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m171.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/lib/python3/dist-packages (from tensorflow) (3.3.0)\n",
      "Collecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/lib/python3/dist-packages (from tensorflow) (1.29.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.8/site-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/lib/python3/dist-packages (from tensorflow) (0.4.0)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.34.2)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m184.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.56.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m143.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in ./.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.2.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.25.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/lib/python3/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.2)\n",
      "Installing collected packages: libclang, flatbuffers, werkzeug, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, protobuf, keras, grpcio, google-auth, absl-py, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.4.0 flatbuffers-23.5.26 google-auth-2.22.0 google-auth-oauthlib-1.0.0 grpcio-1.56.2 keras-2.13.1 libclang-16.0.6 protobuf-4.23.4 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.33.0 werkzeug-2.3.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow-addons in ./.local/lib/python3.8/site-packages (0.21.0)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.8/site-packages (from tensorflow-addons) (23.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in ./.local/lib/python3.8/site-packages (from tensorflow-addons) (2.13.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_addons.optimizers import AdamW\n",
    "def MiniGPT():\n",
    "    \"\"\"\n",
    "    Constructs a mini version of the GPT model. The architecture is comprised of a\n",
    "    token and position embedding layer followed by a single Transformer block. The final\n",
    "    layer is a dense layer with softmax activation for prediction. \n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Mini GPT model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input layer expects inputs of shape (maxlen,) with type int32\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "\n",
    "    # Create the token and position embedding layer and compute the embeddings\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "\n",
    "    # Create the Transformer block and compute its output\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "\n",
    "    # Final dense layer with size equal to the vocabulary size\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "\n",
    "    # Construct the Keras model\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "\n",
    "    # Loss function for the training \n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Model compilation: use Adam optimizer and the defined loss function\n",
    "    # Note that we specify `None` for the second loss to not optimize based on the Transformer block's output\n",
    "    model.compile(\"adam\", loss=[loss_fn, None])\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {
    "a896bbaf-c33d-4700-b0b2-dc7aabd2e598.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "***Shout out to this amazing app: https://netron.app/***\n",
    "***You simply drag and drop your tensorflow or pytorch model into the app and it generates the diagram for you!***\n",
    "\n",
    "![Screen Shot 2023-06-25 at 6.55.38 PM.png](attachment:a896bbaf-c33d-4700-b0b2-dc7aabd2e598.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Text Generator Callback Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A callback to generate text from a trained model at the end of each epoch. It uses the model's \n",
    "    predictions to sample a token, add it to the input, and generate subsequent tokens.\n",
    "\n",
    "    Attributes:\n",
    "        max_tokens (int): The number of tokens to be generated after the prompt.\n",
    "        start_tokens (list): The token indices for the starting prompt.\n",
    "        index_to_word (list): Mapping from token indices to words, obtained from the TextVectorization layer.\n",
    "        k (int): Number of token predictions to consider for sampling the next token.\n",
    "        print_every (int): Frequency of print for the generated text (in number of epochs).\n",
    "    \"\"\"\n",
    "    def __init__(self, max_tokens, start_tokens, index_to_word, top_k=20, print_every=1):\n",
    "        \"\"\"\n",
    "        Initializes the TextGenerator callback.\n",
    "\n",
    "        Args:\n",
    "            max_tokens (int): Maximum number of tokens to be generated.\n",
    "            start_tokens (list): List of integers representing the starting tokens.\n",
    "            index_to_word (list): List of strings representing the mapping from indices to words.\n",
    "            top_k (int, optional): Number of top token predictions to sample from. Defaults to 10.\n",
    "            print_every (int, optional): Frequency of print (in number of epochs). Defaults to 1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.k = top_k\n",
    "        self.print_every = print_every\n",
    "        self.generated_texts = [] # for qualitative validation set\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        \"\"\"\n",
    "        Sample a token index from the token predictions based on their probabilities.\n",
    "\n",
    "        Args:\n",
    "            logits (tf.Tensor): The token predictions (logits) of the model.\n",
    "\n",
    "        Returns:\n",
    "            int: The sampled token index.\n",
    "        \"\"\"\n",
    "        # Select top-k logits and their indices\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "\n",
    "        # Apply softmax to transform logits into probabilities\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "\n",
    "        # Randomly select an index according to the probability distribution\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        \"\"\"\n",
    "        Convert a token index into the corresponding word.\n",
    "\n",
    "        Args:\n",
    "            number (int): The token index.\n",
    "\n",
    "        Returns:\n",
    "            str: The corresponding word.\n",
    "        \"\"\"\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        At the end of each epoch, generate text and print it.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): The current epoch number.\n",
    "            logs (dict, optional): Dictionary of metrics from the epoch. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Create a copy of start tokens for generation\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "\n",
    "        # Only generate text at specified frequency\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "\n",
    "        # Generate tokens until max tokens reached\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "\n",
    "            # Adjust padding based on length of start tokens\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "\n",
    "            x = np.array([x])\n",
    "\n",
    "            # Use the model to predict the probabilities for the next token\n",
    "            y, _ = self.model.predict(x)\n",
    "\n",
    "            # Sample a token from the model's output distribution\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "\n",
    "            # Append the token to the list of generated tokens\n",
    "            tokens_generated.append(sample_token)\n",
    "\n",
    "            # Add the token to the start tokens for the next generation\n",
    "            start_tokens.append(sample_token)\n",
    "\n",
    "            # Increase the number of tokens generated by 1\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "\n",
    "        # Convert the tokens into actual words and join them into a string\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        \n",
    "        self.generated_texts.append((epoch, txt)) # Store for evalutation after training\n",
    "\n",
    "\n",
    "        # Print the generated text\n",
    "        print(f\"generated text:\\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word/Index Mapping Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Callback Object\n",
    "\n",
    "***We also need to supply a starting prompt to act as a qualitative validation set to evaluate the models performance from a 'does it make more sense' per epoch. It will generate(predict) a text sequence continuation from the starting prompt at the end of every epoch to inspect.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_prompt = \"I would have\"\n",
    "\n",
    "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "num_tokens_generated = 42\n",
    "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "***I apologize for the scrolling your about to do. I wanted to generate text at each epoch so that along with loss there would be some qualitative evaluation on the models performance throughout training but I could not find a way to remove the progress bars for each step inside the epochs... If anyone reading this knows a way please comment.***\n",
    "\n",
    "***Until about `25` epochs many of the generations depending on the satrting prompt during training had nonsensical outputs. So we will use `25` to get a good baseline model to evaluate.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 80)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 80, 512)          51240960  \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " transformer_block (Transfor  (None, 80, 512)          2628096   \n",
      " merBlock)                                                       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 80, 100000)        51300000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 105,169,056\n",
      "Trainable params: 105,169,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "1/1 [==============================] - 0s 166ms/step loss: 4.6883 - dense_2_loss: 4.68\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "generated text:\n",
      "I would have a bunch a comment was all the way you have been in this is no way to the same most with our way for the way for their own way it was I didn't can have not a few for the right in\n",
      "\n",
      "121/121 [==============================] - 38s 287ms/step - loss: 4.6883 - dense_2_loss: 4.6883\n",
      "Epoch 2/25\n",
      "1/1 [==============================] - 0s 17ms/step- loss: 3.5588 - dense_2_loss: 3.55\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "generated text:\n",
      "I would have never been a very good job for more than a very good and a few of his family and other side of your comments. The problem with the most beautiful party of this man who has in a person who is not the\n",
      "\n",
      "121/121 [==============================] - 35s 289ms/step - loss: 3.5588 - dense_2_loss: 3.5588\n",
      "Epoch 3/25\n",
      "1/1 [==============================] - 0s 16ms/step- loss: 2.9574 - dense_2_loss: 2.95\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "generated text:\n",
      "I would have the Democrats have been a great deal with the most ignorant about his caucus about this guy in an equal rights to the combatants, Iran; your actions that the way and the same people to see that in the 1960s), we are one\n",
      "\n",
      "121/121 [==============================] - 35s 291ms/step - loss: 2.9574 - dense_2_loss: 2.9574\n",
      "Epoch 4/25\n",
      "1/1 [==============================] - 0s 16ms/step- loss: 2.4293 - dense_2_loss: 2.42\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "generated text:\n",
      "I would have no difference between Communism and the real crimes can be a person that have never seen in the same thing.                       \n",
      "\n",
      "121/121 [==============================] - 35s 292ms/step - loss: 2.4293 - dense_2_loss: 2.4293\n",
      "Epoch 5/25\n",
      "1/1 [==============================] - 0s 17ms/step- loss: 2.0512 - dense_2_loss: 2.05\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "generated text:\n",
      "I would have been billed for me.                                       \n",
      "\n",
      "121/121 [==============================] - 35s 293ms/step - loss: 2.0512 - dense_2_loss: 2.0512\n",
      "Epoch 6/25\n",
      "1/1 [==============================] - 0s 17ms/step- loss: 1.8043 - dense_2_loss: 1.80\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "generated text:\n",
      "I would have to be careful about getting al-Zarqawi's Momma, stripping here. Non-issue. There are responsible for their job in opposing players and that the same ADFG own fault, but not a license? Regulated by the law. It is a sacrifice? Circular bible on their neighbors\n",
      "\n",
      "121/121 [==============================] - 36s 294ms/step - loss: 1.8043 - dense_2_loss: 1.8043\n",
      "Epoch 7/25\n",
      "1/1 [==============================] - 0s 17ms/step- loss: 1.5863 - dense_2_loss: 1.58\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "generated text:\n",
      "I would have a good idea. It makes it a good reason that Allah has never mind after what was said when you already been told to do is not the Bern - not to get a 56 years after you get the first thing if\n",
      "\n",
      "121/121 [==============================] - 36s 296ms/step - loss: 1.5863 - dense_2_loss: 1.5863\n",
      "Epoch 8/25\n",
      "1/1 [==============================] - 0s 17ms/step- loss: 1.3551 - dense_2_loss: 1.35\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "generated text:\n",
      "I would have always be careful recommending system, that consistent with tards or hold their fear of mercantilism that situation.\" That will not controllable by any legislature. It is far from Amazon Park, I'm sure that's keeping Blue eyes are part of rapes, and Hawaii. If\n",
      "\n",
      "121/121 [==============================] - 36s 295ms/step - loss: 1.3551 - dense_2_loss: 1.3551\n",
      "Epoch 9/25\n",
      "1/1 [==============================] - 0s 16ms/step- loss: 1.1138 - dense_2_loss: 1.11\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "generated text:\n",
      "I would have been to figure out of a news from this case this guy is worse if a lesson from them.                        \n",
      "\n",
      "121/121 [==============================] - 36s 295ms/step - loss: 1.1138 - dense_2_loss: 1.1138\n",
      "Epoch 10/25\n",
      "1/1 [==============================] - 0s 16ms/step- loss: 0.9102 - dense_2_loss: 0.91\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "generated text:\n",
      "I would have plenty of guns in a good place online community service. Likely there are that the right that are one to back-up your head will end of your head with INCEST... see you you guys you guys going on the same way you guys\n",
      "\n",
      "121/121 [==============================] - 36s 295ms/step - loss: 0.9102 - dense_2_loss: 0.9102\n",
      "Epoch 11/25\n",
      "1/1 [==============================] - 0s 17ms/step- loss: 0.7537 - dense_2_loss: 0.75\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "generated text:\n",
      "I would have like to consider the most obnoxiuos person to the school a coach than the one thing as the Donald Trump keeps winning this is his mom and his face. Since none of a liberal party a racist and therefore protected reserve to Norquist\n",
      "\n",
      "121/121 [==============================] - 36s 296ms/step - loss: 0.7537 - dense_2_loss: 0.7537\n",
      "Epoch 12/25\n",
      "1/1 [==============================] - 0s 16ms/step- loss: 0.6282 - dense_2_loss: 0.62\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "generated text:\n",
      "I would have double rather than you, how to engage in future do you will be making light over it, but this is in my amused by your edits by 86.137.14.174 =\"              \n",
      "\n",
      "121/121 [==============================] - 36s 296ms/step - loss: 0.6282 - dense_2_loss: 0.6282\n",
      "Epoch 13/25\n",
      "1/1 [==============================] - 0s 16ms/step- loss: 0.5291 - dense_2_loss: 0.52\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "generated text:\n",
      "I would have to pay for a long we see how human rights in Alaska is not much better than any legislature. Good job Pete Kelly, you have been fat cunt, has been totally bogus and effort to be hard time because they are trying to\n",
      "\n",
      "121/121 [==============================] - 36s 296ms/step - loss: 0.5291 - dense_2_loss: 0.5291\n",
      "Epoch 14/25\n",
      "1/1 [==============================] - 0s 16ms/step- loss: 0.4543 - dense_2_loss: 0.45\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "generated text:\n",
      "I would have more intellectually dishonest here judging from the demanding that question to my friend of philosophy so polls. They show Trump and then they don't let him. His anger is the billionaire’s companies employ 57 percent men women and 43 percent women, “there are\n",
      "\n",
      "121/121 [==============================] - 36s 296ms/step - loss: 0.4543 - dense_2_loss: 0.4543\n",
      "Epoch 15/25\n",
      "1/1 [==============================] - 0s 16ms/step- loss: 0.3990 - dense_2_loss: 0.39\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "generated text:\n",
      "I would have been more description of evil and theme that color people are important chapter in America Awesome. This is another job and there to answer to make a strong opinions and there are either way to express anger at the other than wade through\n",
      "\n",
      "121/121 [==============================] - 36s 296ms/step - loss: 0.3990 - dense_2_loss: 0.3990\n",
      "Epoch 16/25\n",
      "1/1 [==============================] - 0s 16ms/step- loss: 0.3468 - dense_2_loss: 0.34\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "generated text:\n",
      "I would have to point out the Keto killed trainer Alexis Martinez in Loro Parque. Also, serious problems for \"incidents\" and reported from Kasatka, Ky, Orkid, Shouka, and Kanduke, and that is only and that is only and that is only and that and then and\n",
      "\n",
      "121/121 [==============================] - 36s 296ms/step - loss: 0.3468 - dense_2_loss: 0.3468\n",
      "Epoch 17/25\n",
      "1/1 [==============================] - 0s 17ms/step- loss: 0.3049 - dense_2_loss: 0.30\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "generated text:\n",
      "I would have actually read this article on a while ago, Dillon took this up, you high road anyway? Yeah, that said, Shannon, I agree with all you on this page you on your points. Mr Swine has not all times. Listening to agree that he\n",
      "\n",
      "121/121 [==============================] - 36s 296ms/step - loss: 0.3049 - dense_2_loss: 0.3049\n",
      "Epoch 18/25\n",
      "1/1 [==============================] - 0s 17ms/step- loss: 0.2665 - dense_2_loss: 0.26\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "generated text:\n",
      "I would have double down with over and it was killed. Free Speech and never walk, bus, bike share parking vehicles behind it. I was therefore anti-job. I would think a job fleshing out the best part where the numbers a lot here, I would take\n",
      "\n",
      "121/121 [==============================] - 36s 296ms/step - loss: 0.2665 - dense_2_loss: 0.2665\n",
      "Epoch 19/25\n",
      "1/1 [==============================] - 0s 16ms/step- loss: 0.2366 - dense_2_loss: 0.23\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "generated text:\n",
      "I would have been very funny that a bakery can be fined with failing to bake a wedding cake for a same sex couple, when same sex marriages were illegal to keep that keep on it as simply beyond term. A fool as a drug use\n",
      "\n",
      "121/121 [==============================] - 36s 296ms/step - loss: 0.2366 - dense_2_loss: 0.2366\n",
      "Epoch 20/25\n",
      "1/1 [==============================] - 0s 16ms/step- loss: 0.2123 - dense_2_loss: 0.21\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "generated text:\n",
      "I would have double down too with how it was handled. It came to a smear in a racist then a truly believe in the second. It is a way to stop the notion that money from the first interacted with the guy and then a\n",
      "\n",
      "121/121 [==============================] - 36s 298ms/step - loss: 0.2123 - dense_2_loss: 0.2123\n",
      "Epoch 21/25\n",
      "1/1 [==============================] - 0s 18ms/step- loss: 0.1936 - dense_2_loss: 0.19\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "generated text:\n",
      "I would have proof that your opinion and Marks wife Debra told me to the law of the main party. Also Knight (great Walter Mercado topic. That wasn't a company work from the street posts, waiting to allow a cadre of a little outside the street\n",
      "\n",
      "121/121 [==============================] - 36s 296ms/step - loss: 0.1936 - dense_2_loss: 0.1936\n",
      "Epoch 22/25\n",
      "1/1 [==============================] - 0s 16ms/step- loss: 0.1778 - dense_2_loss: 0.17\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "generated text:\n",
      "I would have double down with how it is that with too far from making it was bad that behind view of what it is woefully ignorant. I mean, after all, you watch organization true ass and you may be able to review offenses since you\n",
      "\n",
      "121/121 [==============================] - 36s 296ms/step - loss: 0.1778 - dense_2_loss: 0.1778\n",
      "Epoch 23/25\n",
      "1/1 [==============================] - 0s 17ms/step- loss: 0.1640 - dense_2_loss: 0.16\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "generated text:\n",
      "I would have double down with too based on it was like that by saying he's questioning those who paid in kind.                        \n",
      "\n",
      "121/121 [==============================] - 36s 296ms/step - loss: 0.1640 - dense_2_loss: 0.1640\n",
      "Epoch 24/25\n",
      "1/1 [==============================] - 0s 16ms/step- loss: 0.1541 - dense_2_loss: 0.15\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "generated text:\n",
      "I would have double down with it at why that was constantly taking a bit of it rubbish editor, why not the same thing that the liberals are done so far the tired of the Wikipedia policies. Isn't that time to sit there before. Kind of\n",
      "\n",
      "121/121 [==============================] - 36s 296ms/step - loss: 0.1541 - dense_2_loss: 0.1541\n",
      "Epoch 25/25\n",
      "1/1 [==============================] - 0s 17ms/step- loss: 0.1446 - dense_2_loss: 0.14\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "generated text:\n",
      "I would have double down too with how it was handled. It came to light in a smear piece in a local paper implying she is a racist and then a open letter caller her racist, xenophobic, homophobic, sexist, mean-spirited Donald Trump again. And anyway, even\n",
      "\n",
      "121/121 [==============================] - 36s 295ms/step - loss: 0.1446 - dense_2_loss: 0.1446\n"
     ]
    }
   ],
   "source": [
    "model = MiniGPT()\n",
    "\n",
    "N_EPOCHS = 25\n",
    "history  = model.fit(text_ds, verbose=1, epochs=N_EPOCHS, callbacks=[text_gen_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Training Loss Per Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Training loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Validation Set Per Epoch\n",
    "\n",
    "***Since we don't have a quantitative validation set in this situation we can use a qualitative validation set. These would be the generated text from the end of each epoch. This can give us some clues along with the losses per epoch to see how the models performance progressed through training.***\n",
    "\n",
    "***Lets inspect what the first five generations look llike compared to the last five during training.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Generated Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I would have a bunch a comment was all the way...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I would have never been a very good job for mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I would have the Democrats have been a great d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I would have no difference between Communism a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I would have been billed for me.              ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch                                     Generated Text\n",
       "0      0  I would have a bunch a comment was all the way...\n",
       "1      1  I would have never been a very good job for mo...\n",
       "2      2  I would have the Democrats have been a great d...\n",
       "3      3  I would have no difference between Communism a...\n",
       "4      4  I would have been billed for me.              ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Generated Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>I would have proof that your opinion and Marks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>I would have double down with how it is that w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>I would have double down with too based on it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>I would have double down with it at why that w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>I would have double down too with how it was h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch                                     Generated Text\n",
       "20     20  I would have proof that your opinion and Marks...\n",
       "21     21  I would have double down with how it is that w...\n",
       "22     22  I would have double down with too based on it ...\n",
       "23     23  I would have double down with it at why that w...\n",
       "24     24  I would have double down too with how it was h..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for epoch, text in text_gen_callback.generated_texts:\n",
    "#     print(f\"Epoch: {epoch+1}\\nGenerated Text:\\n{text}\\n\")\n",
    "    # Create a DataFrame\n",
    "    \n",
    "df_val = pd.DataFrame(text_gen_callback.generated_texts, columns=['Epoch', 'Generated Text'])\n",
    "\n",
    "display(df_val.head(5));df_val.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***It appears to that with more iterations the model becomes more and more realistic in its generations. The model starts producing less unknown tokens over time during training iterations.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "***Now that we have trained the model to generate toxic comments from a starting prompt we can begin to generate our synthetic data.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(starting_prompt=''):\n",
    "    new_start_prompt = \"here we\"\n",
    "    new_start_tokens = [word_to_index.get(word, 1) for word in new_start_prompt.split()]\n",
    "\n",
    "    text_gen_callback.start_tokens = new_start_tokens\n",
    "    text_gen_callback.on_epoch_end(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "generated text:\n",
      "here we see the case the stupid enough of them as a bunch of obvious that some of promoting hatchet men to do not want to have based on at press and openly encourages other things, so logically its various other nonviolent offenders government, middle\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_text(\"you are\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## State of training data\n",
    "\n",
    "* I learned alot on this project. It is interesting to me that with all of the data in the world there is still a shortage of industrial sized, cleaned, organized, and labeled training data. \n",
    "\n",
    "* It would seem that allthough in academia the glory goes to the next new model or algorithm when really the training data is probably more important at this point. It does not matter how fancy an algorithm is(some of them are pretty fancy) or a model is, without good training data we are limited in the problems we can tackle effectively.\n",
    "\n",
    "* During this project I researched training data sets and where they come from. Most of the open source ones come from a handful of instituions namely universities with a few corporations willing to share the training datasets or models which would not effect their market share significantly releasing them.\n",
    "\n",
    "* Even the sensitive subjetcs such as the language used in this projects training data is very important to solve very prominent problems we have.\n",
    "\n",
    "## Decisions made along the way\n",
    "* I started off this project trying to adapt a GAN to acheive this tast but there is little work done in this area and I had a lot of trouble trying to implement one of the papers where they achieve this task.\n",
    "\n",
    "* I chose this method as this model is lightweight and quick to train and tune for any specific style of language. As I mentioned at the beginning I originally tested this capabillity on the movie lines IMDB dataset and had great results. \n",
    "\n",
    "* The dataset I used turned out to be admittedly smaller than probably needed to get better results but I am confident that once I wrange up more nasty online comments the model will do much better after retraining.\n",
    "\n",
    "* The feature dimensions in the attention head made a huge difference on training time and performance. I had better results at 512 than it is currently at (256) but the time to train 25 epochs grew exponentially. If I had more Kaggle GPU hours I would have done my final training round with 512 in the attention head.\n",
    "\n",
    "* I originally tried a custom learning rate optimizing function but because of the large differences in epochs I was trying for different experiments I switched over to just using ADAM for this task.\n",
    "\n",
    "* The starting prompt matters. I found out that after looking at the popular bigrams and trigrams if I chose those sequences the model would generate sensible text more easily. \n",
    "\n",
    "* The length of the starting prompt also effects how different each output is dramatically.\n",
    "\n",
    "## Whats Next\n",
    "\n",
    "* I am going to run this model and compile a set of generated texts which are known to be toxic and add them to the training set and retrain the BERT based model I used when competing in this competition to see what results I get with the added training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: 'TokenAndPositionEmbedding'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b723af1a3d74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Toxic_MiniGPT1.keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/keras/saving/legacy/serialization.py\u001b[0m in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    383\u001b[0m     )\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0;34mf\"Unknown {printable_module_name}: '{class_name}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;34m\"Please ensure you are using a `keras.utils.custom_object_scope` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown layer: 'TokenAndPositionEmbedding'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "model.save('Toxic_MiniGPT1.keras')\n",
    "\n",
    "\n",
    "loaded_model = tf.keras.models.load_model('Toxic_MiniGPT1.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = 'Toxic_MiniGPT.h5'\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path)\n",
    "\n",
    "## Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Register the custom layer\n",
    "tf.keras.utils.get_custom_objects()['TransformerBlock'] = TransformerBlock\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
