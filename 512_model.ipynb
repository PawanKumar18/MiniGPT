{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniGPT For Generating Synthetic Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Toxic comments online come in many forms and in many arenas. There are currently several ways to mitigate these comments(for those organizations who wish to do so). Some of these ways include human moderators, and training machine learning models to detect toxicity in online comments.\n",
    "\n",
    "The issue with human moderators is that some of these platforms have grown so large so quickly that there are not nearly enough moderators to achieve any sense of control for most of these comments. The shear volume of toxicity and bots online makes it unrealistic to think we could do this job with humans at this point.\n",
    "\n",
    "Many companies are employing machine learning to assist with identifying toxic comments online automatically. The problem with this approach is the lack of labeled training data to train the models on.\n",
    "\n",
    "This is the problem I am going to solve using generative deep learning techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk) (4.64.1)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.8.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.3/774.3 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.8.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "## set seeds for repeatable conclusion\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data I will be using to train the generative model was released on Kaggle as part of an ongoing series of competitions sponsored by the [Google company Jigsaw](https://en.wikipedia.org/wiki/Jigsaw_(company)).\n",
    "\n",
    "The data consists of online comments with various severity levels of toxicity. There are versions of these comments labeled by human annotators wherein they label each comment as toxic or not, or other sets where they were labeled as different categories of toxic such as hatespeech, racist/sexist, obscene, etc. Although these are the labeled datasets we would be adding the synthetic data to in order to create more training data, for this task of simply generating similar text data we will only focus on the comments themselves.\n",
    "\n",
    "The data provided by this competition includes a total of `14,251` unique toxic comments. Theses are the comments I will use to train the generative model with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "The data came in two different files.\n",
    "\n",
    "1) Comments to score: This acts as a test dataset of comments for scoring after the model was trained.\n",
    "\n",
    "2) Validation data: This was the training data for the competition wherein there are two columns. One column labeled less toxic was a comment which human annotators labeled as less toxic than its more toxic counterpart in the other column. There was no actual training data where a comment was paired with its severity rating. The models were trained using creative techniques with the validation data and other classification data sets to train a model which predicted severity of comments.\n",
    "\n",
    "Since for our purposes we are only interested in the actual text comments themselves, I will only be using those columns from these datasources.\n",
    "\n",
    "I start by reading them all into pandas dataframes, isolating the text columns from each one, and stacking them all together so we have a single column of text when it is all said and done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7537 entries, 0 to 7536\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   comment_id  7537 non-null   int64 \n",
      " 1   text        7537 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 117.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6194    FC*K U\\n\\nWhy the fc*k should I get a warning ...\n",
       "2082    Hehe, nice. Nothing like a sockpuppet respondi...\n",
       "5005     VOTE NOW! Should Peggy Mitchell be on the Fic...\n",
       "5726               \\n\\nblow \\n\\nit out of your ass, spic.\n",
       "5655    \" (UTC)\\n\\nThese \"\"reliable sources\"\" wouldn't...\n",
       "5617                 Mine dispeared, somebody wax my ass.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = pd.read_csv('comments_to_score.csv')\n",
    "data1.info()\n",
    "\n",
    "## Isolate only text column\n",
    "data1 = data1['text']\n",
    "\n",
    "data1.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We can see the comments to score was the test file which contained only comments and their corresponding id's***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28422 entries, 0 to 28421\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   worker      28422 non-null  int64 \n",
      " 1   less_toxic  28422 non-null  object\n",
      " 2   more_toxic  28422 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 666.3+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-89246e0b6f04>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data2 = pd.read_csv('validation_data.csv',error_bad_lines=False, engine=\"python\")\n",
      "Skipping line 28424: unexpected end of data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5579</th>\n",
       "      <td>413</td>\n",
       "      <td>Well... You are a proper cunt.</td>\n",
       "      <td>This article has gone to hell \\n\\nI am tired ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23922</th>\n",
       "      <td>171</td>\n",
       "      <td>Edit: I see some can't let go of their old com...</td>\n",
       "      <td>I don't know; I've just finished watching the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27676</th>\n",
       "      <td>448</td>\n",
       "      <td>I am reliably informed that you are a tosser</td>\n",
       "      <td>I see you are a faggot too.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8616</th>\n",
       "      <td>591</td>\n",
       "      <td>REALLY REALLY ANGRY NOW GRRRRRRRRRRRR</td>\n",
       "      <td>\\n\\nThis block is unjustified, is because he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21867</th>\n",
       "      <td>616</td>\n",
       "      <td>OK, now this is pissing me off a bit. Apparent...</td>\n",
       "      <td>Aand they're back as , having created CALIFORN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20429</th>\n",
       "      <td>286</td>\n",
       "      <td>First Paragraph is a Joke \\n\\nHilarious how W...</td>\n",
       "      <td>\"\\n\\nWhich means nothing so ever.  Thank you e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       worker                                         less_toxic  \\\n",
       "5579      413                     Well... You are a proper cunt.   \n",
       "23922     171  Edit: I see some can't let go of their old com...   \n",
       "27676     448      I am reliably informed that you are a tosser    \n",
       "8616      591              REALLY REALLY ANGRY NOW GRRRRRRRRRRRR   \n",
       "21867     616  OK, now this is pissing me off a bit. Apparent...   \n",
       "20429     286   First Paragraph is a Joke \\n\\nHilarious how W...   \n",
       "\n",
       "                                              more_toxic  \n",
       "5579    This article has gone to hell \\n\\nI am tired ...  \n",
       "23922  I don't know; I've just finished watching the ...  \n",
       "27676                        I see you are a faggot too.  \n",
       "8616    \\n\\nThis block is unjustified, is because he ...  \n",
       "21867  Aand they're back as , having created CALIFORN...  \n",
       "20429  \"\\n\\nWhich means nothing so ever.  Thank you e...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = pd.read_csv('validation_data.csv',error_bad_lines=False, engine=\"python\")\n",
    "data2.info()\n",
    "\n",
    "data2.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-64cbe9b74f8b>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data4 = pd.read_csv('jigsaw-toxic-comment-train.csv',error_bad_lines=False, engine=\"python\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29403 entries, 0 to 29402\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   id             29403 non-null  object\n",
      " 1   comment_text   29403 non-null  object\n",
      " 2   toxic          29403 non-null  int64 \n",
      " 3   severe_toxic   29403 non-null  int64 \n",
      " 4   obscene        29403 non-null  int64 \n",
      " 5   threat         29403 non-null  int64 \n",
      " 6   insult         29403 non-null  int64 \n",
      " 7   identity_hate  29403 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 1.8+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 29405: unexpected end of data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>36787588ecefbca7</td>\n",
       "      <td>Please keep in mind that the verifiability acc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6179</th>\n",
       "      <td>107e33299b806076</td>\n",
       "      <td>I'M GOING TO RIP OFF YOUR TESTICLES THEN SHOVE...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24544</th>\n",
       "      <td>40db5bce11c98a15</td>\n",
       "      <td>Donner the biTCH \\n\\nExcuse me but I spent hou...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3628</th>\n",
       "      <td>09b73fd88f54d22b</td>\n",
       "      <td>as the whole world consider you Sunnis terrorists</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4273</th>\n",
       "      <td>0b680c5414c2c6e6</td>\n",
       "      <td>I've requested a change to Wolfpuss - congratu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>0355b79cb7deea97</td>\n",
       "      <td>is a common prostitute, a sock-puppet. I've do...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                       comment_text  \\\n",
       "20639  36787588ecefbca7  Please keep in mind that the verifiability acc...   \n",
       "6179   107e33299b806076  I'M GOING TO RIP OFF YOUR TESTICLES THEN SHOVE...   \n",
       "24544  40db5bce11c98a15  Donner the biTCH \\n\\nExcuse me but I spent hou...   \n",
       "3628   09b73fd88f54d22b  as the whole world consider you Sunnis terrorists   \n",
       "4273   0b680c5414c2c6e6  I've requested a change to Wolfpuss - congratu...   \n",
       "1244   0355b79cb7deea97  is a common prostitute, a sock-puppet. I've do...   \n",
       "\n",
       "       toxic  severe_toxic  obscene  threat  insult  identity_hate  sum  \n",
       "20639      0             0        1       0       0              0    1  \n",
       "6179       1             0        0       0       0              0    1  \n",
       "24544      1             0        1       0       1              0    3  \n",
       "3628       1             0        0       0       0              0    1  \n",
       "4273       1             0        0       0       0              0    1  \n",
       "1244       1             0        0       0       1              0    2  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4 = pd.read_csv('jigsaw-toxic-comment-train.csv',error_bad_lines=False, engine=\"python\")\n",
    "data4 = data4.dropna(how='all')\n",
    "data4.info()\n",
    "\n",
    "## Sum across labels to filter out clean comments\n",
    "data4['sum'] = data4.loc[:, 'toxic':].sum(axis=1)\n",
    "\n",
    "## Keep only comments with some type of label\n",
    "data4 = data4[data4['sum'] > 0]\n",
    "\n",
    "data4.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27531"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-79f711889c2d>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data5 = pd.read_csv('jigsaw-unintended-bias-train.csv',error_bad_lines=False, engine=\"python\")\n",
      "Skipping line 8257: unexpected end of data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8255 entries, 0 to 8254\n",
      "Data columns (total 45 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   id                                   8255 non-null   int64  \n",
      " 1   comment_text                         8255 non-null   object \n",
      " 2   toxic                                8255 non-null   float64\n",
      " 3   severe_toxicity                      8255 non-null   float64\n",
      " 4   obscene                              8255 non-null   float64\n",
      " 5   identity_attack                      8255 non-null   float64\n",
      " 6   insult                               8255 non-null   float64\n",
      " 7   threat                               8255 non-null   float64\n",
      " 8   asian                                1795 non-null   float64\n",
      " 9   atheist                              1795 non-null   float64\n",
      " 10  bisexual                             1795 non-null   float64\n",
      " 11  black                                1795 non-null   float64\n",
      " 12  buddhist                             1795 non-null   float64\n",
      " 13  christian                            1795 non-null   float64\n",
      " 14  female                               1795 non-null   float64\n",
      " 15  heterosexual                         1795 non-null   float64\n",
      " 16  hindu                                1795 non-null   float64\n",
      " 17  homosexual_gay_or_lesbian            1795 non-null   float64\n",
      " 18  intellectual_or_learning_disability  1795 non-null   float64\n",
      " 19  jewish                               1795 non-null   float64\n",
      " 20  latino                               1795 non-null   float64\n",
      " 21  male                                 1795 non-null   float64\n",
      " 22  muslim                               1795 non-null   float64\n",
      " 23  other_disability                     1795 non-null   float64\n",
      " 24  other_gender                         1795 non-null   float64\n",
      " 25  other_race_or_ethnicity              1795 non-null   float64\n",
      " 26  other_religion                       1795 non-null   float64\n",
      " 27  other_sexual_orientation             1795 non-null   float64\n",
      " 28  physical_disability                  1795 non-null   float64\n",
      " 29  psychiatric_or_mental_illness        1795 non-null   float64\n",
      " 30  transgender                          1795 non-null   float64\n",
      " 31  white                                1795 non-null   float64\n",
      " 32  created_date                         8255 non-null   object \n",
      " 33  publication_id                       8255 non-null   int64  \n",
      " 34  parent_id                            4170 non-null   float64\n",
      " 35  article_id                           8255 non-null   int64  \n",
      " 36  rating                               8255 non-null   object \n",
      " 37  funny                                8255 non-null   int64  \n",
      " 38  wow                                  8255 non-null   int64  \n",
      " 39  sad                                  8255 non-null   int64  \n",
      " 40  likes                                8255 non-null   int64  \n",
      " 41  disagree                             8255 non-null   int64  \n",
      " 42  sexual_explicit                      8255 non-null   float64\n",
      " 43  identity_annotator_count             8255 non-null   int64  \n",
      " 44  toxicity_annotator_count             8255 non-null   int64  \n",
      "dtypes: float64(32), int64(10), object(3)\n",
      "memory usage: 2.8+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-79f711889c2d>:25: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  data5['sum'] = data5.loc[:, 'toxic':].sum(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>240844</td>\n",
       "      <td>What do you think about Wolfson's attack on Th...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>273277.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>242804</td>\n",
       "      <td>And it's not like the grazing fees were high. ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>280714.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>239830</td>\n",
       "      <td>What a great leap forward.   Thanks for leadin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>26805.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>244714</td>\n",
       "      <td>\"40-20\\n12 hours ago\\nLabeling others with the...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>284088.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009</th>\n",
       "      <td>246096</td>\n",
       "      <td>When Tyree had a huge spill in the Whiteaker t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>287550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3229</th>\n",
       "      <td>245068</td>\n",
       "      <td>Touché.  The best number would be that mean.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>285297.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                       comment_text  toxic  \\\n",
       "624   240844  What do you think about Wolfson's attack on Th...    0.0   \n",
       "1805  242804  And it's not like the grazing fees were high. ...    0.0   \n",
       "157   239830  What a great leap forward.   Thanks for leadin...    0.0   \n",
       "2989  244714  \"40-20\\n12 hours ago\\nLabeling others with the...    0.6   \n",
       "4009  246096  When Tyree had a huge spill in the Whiteaker t...    0.0   \n",
       "3229  245068       Touché.  The best number would be that mean.    0.0   \n",
       "\n",
       "      severe_toxicity  obscene  identity_attack  insult  threat  asian  \\\n",
       "624               0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "1805              0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "157               0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "2989              0.0      0.0              0.5     0.5     0.0    NaN   \n",
       "4009              0.0      0.0              0.0     0.0     0.0    0.0   \n",
       "3229              0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "\n",
       "      atheist  ...    rating  funny  wow  sad  likes  disagree  \\\n",
       "624       NaN  ...  approved      0    0    0      0         0   \n",
       "1805      NaN  ...  approved      0    0    0      7         0   \n",
       "157       NaN  ...  approved      0    0    0      0         0   \n",
       "2989      NaN  ...  approved      0    0    0      0         0   \n",
       "4009      0.0  ...  approved      0    0    0      0         0   \n",
       "3229      NaN  ...  approved      0    0    0      1         0   \n",
       "\n",
       "      sexual_explicit  identity_annotator_count  toxicity_annotator_count  \\\n",
       "624               0.0                         0                         4   \n",
       "1805              0.0                         0                         4   \n",
       "157               0.0                         0                         4   \n",
       "2989              0.0                         0                        10   \n",
       "4009              0.0                         4                         4   \n",
       "3229              0.0                         0                         4   \n",
       "\n",
       "           sum  \n",
       "624   273277.0  \n",
       "1805  280714.0  \n",
       "157    26805.0  \n",
       "2989  284088.6  \n",
       "4009  287550.0  \n",
       "3229  285297.0  \n",
       "\n",
       "[6 rows x 46 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5 = pd.read_csv('jigsaw-unintended-bias-train.csv',error_bad_lines=False, engine=\"python\")\n",
    "data5.info()\n",
    "\n",
    "# drop_columns = [\n",
    "#     'created_date',\n",
    "#     'publication_id',\n",
    "#     'parent_id',\n",
    "#     'article_id', \n",
    "#     'rating', \n",
    "#     'funny', \n",
    "#     'wow', \n",
    "#     'sad', \n",
    "#     'likes', \n",
    "#     'disagree', \n",
    "#     'identity_annotator_count', \n",
    "#     'toxicity_annotator_count'\n",
    "# ]\n",
    "\n",
    "# data5 = data5.drop(columns=drop_columns, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Sum across labels to filter out clean comments\n",
    "data5['sum'] = data5.loc[:, 'toxic':].sum(axis=1)\n",
    "\n",
    "## Keep only comments with some type of label\n",
    "data5 = data5[data5['sum'] > 0]\n",
    "\n",
    "data5.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***This was the data provided to validate the models performance during training. The three columns are workers(annotators) and the other two are text columns which we will use both to train our generative model with.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>313</td>\n",
       "      <td>This article sucks \\n\\nwoo woo wooooooo</td>\n",
       "      <td>WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>\"And yes, people should recognize that but the...</td>\n",
       "      <td>Daphne Guinness \\n\\nTop of the mornin' my fav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>Western Media?\\n\\nYup, because every crime in...</td>\n",
       "      <td>\"Atom you don't believe actual photos of mastu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>347</td>\n",
       "      <td>And you removed it! You numbskull! I don't car...</td>\n",
       "      <td>You seem to have sand in your vagina.\\n\\nMight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539</td>\n",
       "      <td>smelly vagina \\n\\nBluerasberry why don't you ...</td>\n",
       "      <td>hey \\n\\nway to support nazis, you racist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   worker                                         less_toxic  \\\n",
       "0     313            This article sucks \\n\\nwoo woo wooooooo   \n",
       "1     188  \"And yes, people should recognize that but the...   \n",
       "2      82   Western Media?\\n\\nYup, because every crime in...   \n",
       "3     347  And you removed it! You numbskull! I don't car...   \n",
       "4     539   smelly vagina \\n\\nBluerasberry why don't you ...   \n",
       "\n",
       "                                          more_toxic  \n",
       "0  WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...  \n",
       "1   Daphne Guinness \\n\\nTop of the mornin' my fav...  \n",
       "2  \"Atom you don't believe actual photos of mastu...  \n",
       "3  You seem to have sand in your vagina.\\n\\nMight...  \n",
       "4           hey \\n\\nway to support nazis, you racist  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all columns into a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-ce653e639800>:7: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data3 = pd.read_csv('validation_data.csv',error_bad_lines=False, engine=\"python\")\n",
      "Skipping line 28424: unexpected end of data\n"
     ]
    }
   ],
   "source": [
    "## Isolate text column\n",
    "data2 = data2['more_toxic']\n",
    "data4 = data4['comment_text']\n",
    "data5 = data5['comment_text']\n",
    "\n",
    "## Isolate text column\n",
    "data3 = pd.read_csv('validation_data.csv',error_bad_lines=False, engine=\"python\")\n",
    "data3 = data3['less_toxic']\n",
    "\n",
    "text_column = pd.concat([data1, data2, data3, data4, data5], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n \\n\\nGjalexei, you asked about whether there is an \"\"anti-editorializing\"\" policy here.  There is, and it's called wikipedia:neutral point of view.  It discusses at some length  the case of what we should do when writing about a subject which most of us find repugnant.  Whilst you're not likely to get too many defenders of FGM here, the need for the policy should be clearer for articles like abortion, for instance.\\n\\nIf something you write is edited and you're not sure why, please continue to question such edits on the talk page.  Sometimes, you'll learn more about wikipedia policy.  Sometimes, you'll find out that some other people working on here can get it flat-out wrong ) Robert Merkel\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1\n",
       "I agree; I don't want to grant them the legitimacy of protestors. They're greedy, small-minded people who somehow seem to share the mass delusion that this is not only a good idea for themselves as individuals, but is the right thing to do for ranchers at large. Basically: take something that currently belongs to everyone, and give it to a select group of people, so they can profit.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            1\n",
       "blah blah blah kiss my ass                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   1\n",
       "\"\\n\\n User:GeorgiaWikiWriter \\n\\nHello Ymblanter, \\n\\nI have warned  calling it \"\"a group of fucking morons.\"\" His behavior needs to be addressed asap. Thanks. Jaqeli \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     1\n",
       "Regarding your passing \\n\\nBecause you willfully violate Wikipedia's copyright and because you intentionally publish libel, I will arrange to have your life terminated.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     1\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ..\n",
       "\"\\n\\n You have no idea what you're talking about \\nUnlike the silly kids who come to this site and put this false information everywhere, I actually KNOW as a FACT that Deep Purple, Black Sabbath and Led Zeppelin are HARD ROCK and NOT, by ANY MEANS AT ALL Heavy Metal. Sabbath and Deep Purple themselves have stated clearly many times that they are NOT HEAVY METAL. You are putting false information on this site, which is why this place is so unreliable. I am cleaning it up and making it more accurate. I would guess you are clearly much younger than me, I was brought up with these bands and I know better. The same with the stupid idea that \"\"heavy metal\"\" started in the late 60's. It DID NOT. It started in the mid 70's, I know, because I saw it form. Being the \"\"forefather\"\" of heavy metal does NOT MAKE YOU HEAVY METAL YOURSELF. It is you who is vandalising this website by putting false information on these bands pages after they have made clear several million times that they are NOT what everyone keeps saying they are. It is also easy to hear the difference. If I had the power, it would be people like YOU who would be blocked because you are making this site an unreliable source of information. IF you think you're right, ask the bands themselves. Then maybe you will change how you think. But I doubt it. Take care.  \"    1\n",
       " I don't understand. What is a vandalism blacklist and why am I on it? I've never vandalized any pages. The only time I was blocked was for breaking 3RR in an attempt to counter vandalism.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 1\n",
       "Dahn:Wait a little Dahn, of what you explained, that you were obligated by law in the 2002 census to say if oyu have relatives abroad, that means there about 19.4 millions Romanians in the whole world??? No offence, but this is absolutely out of discussion. When I had the 2002 census, there was no place in the questionary for that.\\nBanatean: although your explication makes no sense, let's see. You say that we should include only primary source information that means censuses. Come on, take a look at Poles, Italians or other large diasporas. At Poles by exemple, only USA, Canada and Poland are using census official infos. All others are unofficial sources, but they're however put there. Even for USA, when at the 2000 USA census, they were 8 997 000 Poles declared, they decided to add the unofficial larger figure of 9 300 000. Why this doesn't apply here??????? If you all sustain this stupid explanations, then answer my question: WHY AT ROMANIANS ARTICLE WE DON'T LIKE OTHERS, ICLUDE ALL KIND OF DATA?????????????? Come on guys, what is your proof for sustaining that? \\nArthur 28 October 2006                                                                                                                                                                                                                                           1\n",
       "Big ass warning type thing: You step to me on my user page, and I reserve the right to tell you to rack off, in any number of ways which may damage your ego.  You act nice and civil here, and I'll do the same.  Note, if you post random unhelpful shit, I will tell you to rack off as well.  Be a jerk at your own risk.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                1\n",
       "actually...while the rest of the nation pulled out of recession in the 1980's, Oregon lost a decade because Oregon was internationally known as Tax He11, USA. It was when liberal communities turned to Reaganomics 101, and invested millions of dollars in developing sister city relationships with Asia, granting property tax relief to corporations offshoring jobs to Oregon, our economy became so smoking hot in the 90's we created more jobs than we could fill, which saw our wages double in the decade. Today liberals only grant property tax relief to crony corporations that suit their political agenda. What you're failing to understand is we have the highest corporate tax rates in the 1st world.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  1\n",
       "Length: 24760, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_column.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like between the data provided for the competition there are many duplicates. However we can see that some comments are reused many more times than other comments. For example the most used comments were repeated `19` times in the datasets while others only `2` times. \n",
    "\n",
    "Since the duplications are not balanced if we left the data like this I am afraid we would be biasing the model towards the comments which were present more in the data. \n",
    "\n",
    "I will remove all duplicate comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total numer of comments in text data = 24760\n",
      "Numer of unique comments in text data = 24760\n",
      "Duplicate comments dropped\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total numer of comments in text data = {len(text_column)}\")\n",
    "print(f\"Numer of unique comments in text data = {len(text_column.unique())}\")\n",
    "\n",
    "text_column = text_column.drop_duplicates()\n",
    "print(\"Duplicate comments dropped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the toxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>noun_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>95.860000</td>\n",
       "      <td>16.010000</td>\n",
       "      <td>27.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>179.008437</td>\n",
       "      <td>33.781979</td>\n",
       "      <td>81.126052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.750000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>69.500000</td>\n",
       "      <td>13.250000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1079.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>703.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word_count  verb_count  noun_count\n",
       "count   100.000000  100.000000  100.000000\n",
       "mean     95.860000   16.010000   27.410000\n",
       "std     179.008437   33.781979   81.126052\n",
       "min       4.000000    0.000000    0.000000\n",
       "25%      17.750000    3.000000    4.000000\n",
       "50%      41.000000    7.000000    8.000000\n",
       "75%      69.500000   13.250000   17.000000\n",
       "max    1079.000000  270.000000  703.000000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "data['text'] = text_column\n",
    "data = data.sample(100)\n",
    "\n",
    "# Function to calculate word count\n",
    "def count_words(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "# Function to calculate verb count\n",
    "def count_verbs(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    verb_count = len([word for word, tag in tagged_words if tag.startswith('V')])\n",
    "    return verb_count\n",
    "\n",
    "# Function to calculate noun count\n",
    "def count_nouns(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    noun_count = len([word for word, tag in tagged_words if tag.startswith('N')])\n",
    "    return noun_count\n",
    "\n",
    "# Add word count column\n",
    "data['word_count'] = data['text'].apply(count_words)\n",
    "\n",
    "# Add verb count column\n",
    "data['verb_count'] = data['text'].apply(count_verbs)\n",
    "\n",
    "# Add noun count column\n",
    "data['noun_count'] = data['text'].apply(count_nouns)\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = data['word_count'].plot(kind='kde')\n",
    "data['verb_count'].plot(kind='kde', ax=ax)\n",
    "data['noun_count'].plot(kind='kde', ax=ax)\n",
    "\n",
    "ax.legend(['Word Count', 'Verb Count', 'Noun Count'])\n",
    "ax.set_title('Distribution of Word Count, Verb Count, and Noun Count')\n",
    "ax.set_xlabel('Count')\n",
    "ax.set_ylabel('Density')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these comments on average are quite short in length and contain more nouns than verbs on average.\n",
    "\n",
    "Since we have not done any cleaning of the data yet these distributions are not exact as the nltk package is not currently looking for misspelled words or different versions of word spellings which are used online sometimes.\n",
    "\n",
    "For example if a user knows that the platform they are on has limitations on language than they may spell a profane word to try to fool any auto detecting systems such as `Fuck==>Fxck, F*ck, Fukk, Fuuu*uukk`, etc.\n",
    "\n",
    "Therefore these counts will not detect all nouns and verbs but should give a decent sample.\n",
    "\n",
    "Knowing the underlying distributions of some of these features is important because after the synthetic data is generated we would most likely want it to follow the same distributions for these attributes of the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the most common N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>((Go, fuck), 265)</td>\n",
       "      <td>((Go, fuck, yourself), 265)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((fuck, yourself), 265)</td>\n",
       "      <td>((fuck, yourself, !), 265)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>((yourself, !), 265)</td>\n",
       "      <td>((yourself, !, Go), 264)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>((!, Go), 264)</td>\n",
       "      <td>((!, Go, fuck), 264)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((ca, n't), 138)</td>\n",
       "      <td>((you, ca, n't), 137)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>((you, ca), 137)</td>\n",
       "      <td>((ca, n't, keep), 135)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>((n't, keep), 135)</td>\n",
       "      <td>((n't, keep, me), 135)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>((keep, me), 135)</td>\n",
       "      <td>((keep, me, down), 135)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>((me, down), 135)</td>\n",
       "      <td>((me, down, nigger), 135)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>((down, nigger), 135)</td>\n",
       "      <td>((down, nigger, you), 135)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>((nigger, you), 135)</td>\n",
       "      <td>((nigger, you, ca), 134)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>((I, AM), 126)</td>\n",
       "      <td>((I, AM, A), 126)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>((AM, A), 126)</td>\n",
       "      <td>((AM, A, CUNTBAG), 126)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>((A, CUNTBAG), 126)</td>\n",
       "      <td>((A, CUNTBAG, ====), 125)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>((CUNTBAG, ====), 125)</td>\n",
       "      <td>((CUNTBAG, ====, I), 125)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>((====, I), 125)</td>\n",
       "      <td>((====, I, AM), 125)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>((BITCHES, THAT), 78)</td>\n",
       "      <td>((BITCHES, THAT, ARE), 78)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>((THAT, ARE), 78)</td>\n",
       "      <td>((THAT, ARE, READING), 78)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>((ARE, READING), 78)</td>\n",
       "      <td>((ARE, READING, THIS), 78)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>((READING, THIS), 78)</td>\n",
       "      <td>((READING, THIS, .), 78)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>((THIS, .), 78)</td>\n",
       "      <td>((THIS, ., JASENM222), 78)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>((., JASENM222), 78)</td>\n",
       "      <td>((., JASENM222, SUCKS), 78)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>((JASENM222, SUCKS), 78)</td>\n",
       "      <td>((JASENM222, SUCKS, FAT), 78)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>((SUCKS, FAT), 78)</td>\n",
       "      <td>((SUCKS, FAT, DICK.FUCKING), 78)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>((FAT, DICK.FUCKING), 78)</td>\n",
       "      <td>((FAT, DICK.FUCKING, BITCHES), 77)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>((DICK.FUCKING, BITCHES), 77)</td>\n",
       "      <td>((DICK.FUCKING, BITCHES, THAT), 77)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>((., I), 34)</td>\n",
       "      <td>((!, !, !), 30)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>((!, !), 34)</td>\n",
       "      <td>((SuPeRTR0LL, WiLL, LiVe), 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>((SuPeRTR0LL, WiLL), 23)</td>\n",
       "      <td>((WiLL, LiVe, FoReVeR), 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>((WiLL, LiVe), 23)</td>\n",
       "      <td>((LiVe, FoReVeR, !), 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>((LiVe, FoReVeR), 23)</td>\n",
       "      <td>((FoReVeR, !, iF), 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>((FoReVeR, !), 23)</td>\n",
       "      <td>((!, iF, You), 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>((!, iF), 23)</td>\n",
       "      <td>((iF, You, Do), 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>((iF, You), 23)</td>\n",
       "      <td>((You, Do, N'T), 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>((You, Do), 23)</td>\n",
       "      <td>((Do, N'T, ReSPeCT), 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>((Do, N'T), 23)</td>\n",
       "      <td>((N'T, ReSPeCT, THe), 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>((N'T, ReSPeCT), 23)</td>\n",
       "      <td>((ReSPeCT, THe, SuPeRTR0LL), 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>((ReSPeCT, THe), 23)</td>\n",
       "      <td>((THe, SuPeRTR0LL, You), 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>((THe, SuPeRTR0LL), 23)</td>\n",
       "      <td>((SuPeRTR0LL, You, WiLL), 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>((SuPeRTR0LL, You), 23)</td>\n",
       "      <td>((You, WiLL, Die), 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>((You, WiLL), 23)</td>\n",
       "      <td>((WiLL, Die, You), 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>((WiLL, Die), 23)</td>\n",
       "      <td>((Die, You, PaTHeTiC), 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>((Die, You), 23)</td>\n",
       "      <td>((You, PaTHeTiC, FooL), 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>((You, PaTHeTiC), 23)</td>\n",
       "      <td>((PaTHeTiC, FooL, !), 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>((PaTHeTiC, FooL), 23)</td>\n",
       "      <td>((FooL, !, SuPeRTR0LL), 22)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>((FooL, !), 23)</td>\n",
       "      <td>((!, SuPeRTR0LL, WiLL), 22)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>((!, SuPeRTR0LL), 22)</td>\n",
       "      <td>((., I, have), 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(('', ''), 20)</td>\n",
       "      <td>((., It, 's), 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>((``, ''), 18)</td>\n",
       "      <td>((*, *, *), 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>((,, and), 16)</td>\n",
       "      <td>((., I, 'm), 5)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          bigrams                             trigrams\n",
       "0               ((Go, fuck), 265)          ((Go, fuck, yourself), 265)\n",
       "1         ((fuck, yourself), 265)           ((fuck, yourself, !), 265)\n",
       "2            ((yourself, !), 265)             ((yourself, !, Go), 264)\n",
       "3                  ((!, Go), 264)                 ((!, Go, fuck), 264)\n",
       "4                ((ca, n't), 138)                ((you, ca, n't), 137)\n",
       "5                ((you, ca), 137)               ((ca, n't, keep), 135)\n",
       "6              ((n't, keep), 135)               ((n't, keep, me), 135)\n",
       "7               ((keep, me), 135)              ((keep, me, down), 135)\n",
       "8               ((me, down), 135)            ((me, down, nigger), 135)\n",
       "9           ((down, nigger), 135)           ((down, nigger, you), 135)\n",
       "10           ((nigger, you), 135)             ((nigger, you, ca), 134)\n",
       "11                 ((I, AM), 126)                    ((I, AM, A), 126)\n",
       "12                 ((AM, A), 126)              ((AM, A, CUNTBAG), 126)\n",
       "13            ((A, CUNTBAG), 126)            ((A, CUNTBAG, ====), 125)\n",
       "14         ((CUNTBAG, ====), 125)            ((CUNTBAG, ====, I), 125)\n",
       "15               ((====, I), 125)                 ((====, I, AM), 125)\n",
       "16          ((BITCHES, THAT), 78)           ((BITCHES, THAT, ARE), 78)\n",
       "17              ((THAT, ARE), 78)           ((THAT, ARE, READING), 78)\n",
       "18           ((ARE, READING), 78)           ((ARE, READING, THIS), 78)\n",
       "19          ((READING, THIS), 78)             ((READING, THIS, .), 78)\n",
       "20                ((THIS, .), 78)           ((THIS, ., JASENM222), 78)\n",
       "21           ((., JASENM222), 78)          ((., JASENM222, SUCKS), 78)\n",
       "22       ((JASENM222, SUCKS), 78)        ((JASENM222, SUCKS, FAT), 78)\n",
       "23             ((SUCKS, FAT), 78)     ((SUCKS, FAT, DICK.FUCKING), 78)\n",
       "24      ((FAT, DICK.FUCKING), 78)   ((FAT, DICK.FUCKING, BITCHES), 77)\n",
       "25  ((DICK.FUCKING, BITCHES), 77)  ((DICK.FUCKING, BITCHES, THAT), 77)\n",
       "26                   ((., I), 34)                      ((!, !, !), 30)\n",
       "27                   ((!, !), 34)       ((SuPeRTR0LL, WiLL, LiVe), 23)\n",
       "28       ((SuPeRTR0LL, WiLL), 23)          ((WiLL, LiVe, FoReVeR), 23)\n",
       "29             ((WiLL, LiVe), 23)             ((LiVe, FoReVeR, !), 23)\n",
       "30          ((LiVe, FoReVeR), 23)               ((FoReVeR, !, iF), 23)\n",
       "31             ((FoReVeR, !), 23)                   ((!, iF, You), 23)\n",
       "32                  ((!, iF), 23)                  ((iF, You, Do), 23)\n",
       "33                ((iF, You), 23)                 ((You, Do, N'T), 23)\n",
       "34                ((You, Do), 23)             ((Do, N'T, ReSPeCT), 23)\n",
       "35                ((Do, N'T), 23)            ((N'T, ReSPeCT, THe), 23)\n",
       "36           ((N'T, ReSPeCT), 23)     ((ReSPeCT, THe, SuPeRTR0LL), 23)\n",
       "37           ((ReSPeCT, THe), 23)         ((THe, SuPeRTR0LL, You), 23)\n",
       "38        ((THe, SuPeRTR0LL), 23)        ((SuPeRTR0LL, You, WiLL), 23)\n",
       "39        ((SuPeRTR0LL, You), 23)               ((You, WiLL, Die), 23)\n",
       "40              ((You, WiLL), 23)               ((WiLL, Die, You), 23)\n",
       "41              ((WiLL, Die), 23)           ((Die, You, PaTHeTiC), 23)\n",
       "42               ((Die, You), 23)          ((You, PaTHeTiC, FooL), 23)\n",
       "43          ((You, PaTHeTiC), 23)            ((PaTHeTiC, FooL, !), 23)\n",
       "44         ((PaTHeTiC, FooL), 23)          ((FooL, !, SuPeRTR0LL), 22)\n",
       "45                ((FooL, !), 23)          ((!, SuPeRTR0LL, WiLL), 22)\n",
       "46          ((!, SuPeRTR0LL), 22)                    ((., I, have), 5)\n",
       "47                 (('', ''), 20)                     ((., It, 's), 5)\n",
       "48                 ((``, ''), 18)                       ((*, *, *), 5)\n",
       "49                 ((,, and), 16)                      ((., I, 'm), 5)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text into words\n",
    "data['words'] = data['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Get bigrams and trigrams for each row\n",
    "data['bigrams']   = data['words'].apply(lambda x: list(ngrams(x, 2)))\n",
    "data['trigrams']  = data['words'].apply(lambda x: list(ngrams(x, 3)))\n",
    "# data['quadgrams'] = data['words'].apply(lambda x: list(ngrams(x, 4)))\n",
    "\n",
    "# Count the occurrences of bigrams and trigrams\n",
    "bigram_counts   = Counter([gram for grams in data['bigrams'] for gram in grams])\n",
    "trigram_counts  = Counter([gram for grams in data['trigrams'] for gram in grams])\n",
    "# quadgram_counts = Counter([gram for grams in data['quadgrams'] for gram in grams])\n",
    "\n",
    "# Get the most common bigrams, trigrams, and quadgrams\n",
    "most_common_bigrams   = bigram_counts.most_common(50)\n",
    "most_common_trigrams  = trigram_counts.most_common(50)\n",
    "# most_common_quadgrams = quadgram_counts.most_common(50)\n",
    "\n",
    "df_common_grams = pd.DataFrame()\n",
    "df_common_grams['bigrams']   = most_common_bigrams\n",
    "df_common_grams['trigrams']  = most_common_trigrams\n",
    "# df_common_grams['quadgrams'] = most_common_quadgrams\n",
    "\n",
    "# # Display the results\n",
    "# print('Most common bigrams:')\n",
    "# for bigram, count in most_common_bigrams:\n",
    "#     print(' '.join(bigram), count)\n",
    "\n",
    "# print('\\nMost common trigrams:')\n",
    "# for trigram, count in most_common_trigrams:\n",
    "#     print(' '.join(trigram), count)\n",
    "    \n",
    "# print('\\nMost common quadgrams:')\n",
    "# for quadgram, count in most_common_quadgrams:\n",
    "#     print(' '.join(quadgram), count)\n",
    "\n",
    "\n",
    "df_common_grams.iloc[:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the initial 10 or so most common bi-grams and tri-grams are repetitive punctuation marks.\n",
    "\n",
    "Traditionally these would be cleaned and removed when training models for NLP tasks, however due to the nature of this work many of these traditional techniques will limit the models ability to predict toxicity as well as with clean text.\n",
    "\n",
    "I happened to have competed in this competition and one thing all of us learned was that leaving capital letters and punctuation improved the models ability to infer toxicity and especially levels of toxicity. \n",
    "\n",
    "For example a phrase such as:\n",
    "\n",
    "`Are you kidding?`\n",
    "\n",
    "Conveys a much different meaning than the same words but put this way:\n",
    "\n",
    "`ARE YOU KIDDING!!!??`\n",
    "\n",
    "Traditional NLP techniques would have us convert all characters to lower case and remove punctuation so the model will interpret both of those texts the exact same way.\n",
    "\n",
    "When training sentiment based models or models where feeling and emotion is being conveyed in some way such as toxicity of comments, it is more than just the raw content of the words alone which gives the meaning. The puncuation and capitalizations are very expressive forms of language and as such for these problems do better left in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "* First we need load in our text column as tensorflow formatted dataset\n",
    "\n",
    "* Next we shuffle the data to avoid any patterns which may have been present\n",
    "\n",
    "* We then slice the data into batches for processing\n",
    "\n",
    "* Vectorize the text which will be used to create a corpus of vocabulary used when training and act as vector representations of our text\n",
    "\n",
    "* Create the corpus of vocabulary which is used to train and evaluate throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = 100000  ## Only consider the top 20k words\n",
    "maxlen = 80  ## Max sequence length\n",
    "batch_size = 128  ## Data loading batch sizes\n",
    "\n",
    "# Create a dataset from the pandas column\n",
    "text_column = text_column.astype(str)  # Convert all elements to strings\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(text_column)\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "text_ds = text_ds.shuffle(buffer_size=128)\n",
    "text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "# def custom_standardization(input_string):\n",
    "#     \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
    "#     lowercased = tf.strings.lower(input_string)\n",
    "#     stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "#     return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
    "\n",
    "\n",
    "## Create a vectorization layer and adapt it to the text\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=None,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  ## To get words back from token indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Labels\n",
    "\n",
    "Since we are building a generative auto-regressive model, we must train it to predict the next word by looking backwards and using the previous tokens to predict the highest probability for the next token.\n",
    "\n",
    "This is fairly easy to create labels for because we simply shuffle the `TRUE` data be one token and then when training the model compares the predicted text with the next indexed word.\n",
    "\n",
    "We can inspect what these samples and labels look like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to create target column\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
    "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "Well fuck u ur just a twat                                                                         \n",
      "\n",
      "Target Sequence:\n",
      "fuck u ur just a twat                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "by the vandal Pavel Vozenilek                                                                           \n",
      "\n",
      "Target Sequence:\n",
      "the vandal Pavel Vozenilek                                                                            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "\" [UNK] to Vandalism You e-mailed me about [UNK] and the [UNK] being vandalized because i wrote the truth!! [UNK] [UNK] WAS playing with Harry and could have \"\"done it\"\", so i don't see why you reported vandalism. BTW, how are u emailing me? I don't remember any \"\"give me your e-mail adress\"\" things.\"                          \n",
      "\n",
      "Target Sequence:\n",
      "[UNK] to Vandalism You e-mailed me about [UNK] and the [UNK] being vandalized because i wrote the truth!! [UNK] [UNK] WAS playing with Harry and could have \"\"done it\"\", so i don't see why you reported vandalism. BTW, how are u emailing me? I don't remember any \"\"give me your e-mail adress\"\" things.\"                           \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "You are a racist pig, loser. Don't vandalize again. 14:54, 28 Jan 2005 (UTC)                                                                  \n",
      "\n",
      "Target Sequence:\n",
      "are a racist pig, loser. Don't vandalize again. 14:54, 28 Jan 2005 (UTC)                                                                   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "You guys are a bunch of control freaks who band together to stomp out anything or anybody who doesn't think like you.                                                          \n",
      "\n",
      "Target Sequence:\n",
      "guys are a bunch of control freaks who band together to stomp out anything or anybody who doesn't think like you.                                                           \n"
     ]
    }
   ],
   "source": [
    "## Select samples from the training data set to inspect\n",
    "sample = text_ds.take(5) \n",
    "\n",
    "## Display some samples\n",
    "for x, y in sample:\n",
    "    # Convert token indices back to words\n",
    "    input_words  = [vocab[i] for i in x[0].numpy()]\n",
    "    target_words = [vocab[i] for i in y[0].numpy()]\n",
    "\n",
    "    print(\"\\n\\n\\n\\nInput Sequence:\")\n",
    "    print(\" \".join(input_words))\n",
    "    print(\"\\nTarget Sequence:\")\n",
    "    print(\" \".join(target_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***We can see that the target or label sequence is merely our ground truth text sequence we have just shifted by `1` token. This is what our model will use to evaluate during training.***\n",
    "\n",
    "* ***Cell below was for loading in and preprocessing the IMBD movie quotes dataset. This is the dataset I tested this approach on first.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Transformer Block and Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Creates a mask for causal (auto-regressive) self-attention. The returned mask has the shape \n",
    "    [batch_size, n_dest, n_src], where each entry at position (i, j, k) will be 1 if j >= k and 0 otherwise. \n",
    "    This is used to prevent the attention mechanism from attending to future positions during the forward pass.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of sequences in each batch.\n",
    "        n_dest (int): Number of destination attention heads.\n",
    "        n_src (int): Number of source attention heads.\n",
    "        dtype (tf.DType): Type of the output tensor.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: A tensor of shape [batch_size, n_dest, n_src] representing the mask.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create two range tensors i and j, where i has shape [n_dest, 1] and j has shape [n_src]\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "\n",
    "    # Create a mask where entry (i, j) is True if i >= j - n_src + n_dest and False otherwise\n",
    "    m = i >= j - n_src + n_dest\n",
    "\n",
    "    # Cast the mask to the desired data type\n",
    "    mask = tf.cast(m, dtype)\n",
    "\n",
    "    # Reshape the mask to have shape [1, n_dest, n_src]\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "\n",
    "    # Create a tensor with shape [2] that represents the multiples for tiling\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "\n",
    "    # Tile the mask tensor to have shape [batch_size, n_dest, n_src]\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    A Transformer block that includes multi-head self-attention and a feed-forward neural network.\n",
    "    Each of these two components has a residual connection and is followed by layer normalization.\n",
    "\n",
    "    Attributes:\n",
    "        att (layers.MultiHeadAttention): Multi-head self-attention layer.\n",
    "        ffn (keras.Sequential): Feed-forward neural network.\n",
    "        layernorm1 (layers.LayerNormalization): Layer normalization after the self-attention.\n",
    "        layernorm2 (layers.LayerNormalization): Layer normalization after the feed-forward network.\n",
    "        dropout1 (layers.Dropout): Dropout layer after the self-attention.\n",
    "        dropout2 (layers.Dropout): Dropout layer after the feed-forward network.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1,**kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer block.\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int): Dimensionality of the input embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            ff_dim (int): Number of units in the hidden layer of the feed-forward network.\n",
    "            rate (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer block.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): Input tensor of shape [batch_size, seq_len, embed_dim].\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor of shape [batch_size, seq_len, embed_dim].\n",
    "        \"\"\"\n",
    "        # Compute the shapes\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "\n",
    "        # Create the causal mask for the multi-head self-attention\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "\n",
    "        # Compute the output of the multi-head self-attention\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "\n",
    "        # Apply dropout to the attention output\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "\n",
    "        # Add the attention output to the inputs (residual connection) and normalize the result\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "\n",
    "        # Compute the output of the feed-forward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "\n",
    "        # Apply dropout to the feed-forward output\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "\n",
    "        # Add the feed-forward output to the previous output (residual connection) and normalize the result\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self): # 5\n",
    "        config = super().get_config()\n",
    "        # save constructor args\n",
    "        config['embed_dim'] = self.embed_dim\n",
    "        config['num_heads'] = self.num_heads\n",
    "        config['ff_dim'] = self.ff_dim\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Embedding layer\n",
    "\n",
    "***Create two separate embedding layers:***\n",
    "\n",
    "1) One for tokens \n",
    "\n",
    "2) One for token indices(positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    \"\"\"\n",
    "    Layer for combining token and positional embeddings. Token embeddings provide the model\n",
    "    with understanding of the meaning of each token, while positional embeddings provide\n",
    "    information about the position of each token in the sequence.\n",
    "\n",
    "    Attributes:\n",
    "        token_emb (layers.Embedding): Token embedding layer.\n",
    "        pos_emb (layers.Embedding): Position embedding layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, name=None, **kwargs):\n",
    "        super(TokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        \n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the TokenAndPositionEmbedding layer.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): Input tensor of shape [batch_size, seq_len].\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor of shape [batch_size, seq_len, embed_dim], resulting from\n",
    "            adding token embeddings and position embeddings.\n",
    "        \"\"\"\n",
    "        # Compute the maximum sequence length\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "\n",
    "        # Create a range tensor representing positions\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "\n",
    "        # Compute the position embeddings\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        # Compute the token embeddings\n",
    "        x = self.token_emb(x)\n",
    "\n",
    "        # Add the token embeddings and position embeddings\n",
    "        return x + positions\n",
    "    \n",
    "    def get_config(self): # 5\n",
    "        config = super().get_config()\n",
    "        # save constructor args\n",
    "        config['maxlen'] = self.maxlen\n",
    "        config['vocab_size'] = self.vocab_size\n",
    "        config['embed_dim'] = self.embed_dim\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Mini GPT\n",
    "\n",
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vocab_size = 30000  # Only consider the top 20k words\n",
    "maxlen = 80  # Max sequence size\n",
    "embed_dim = 512  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 512  # Hidden layer size in feed forward network inside transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow_addons.optimizers import AdamW\n",
    "def MiniGPT():\n",
    "    \"\"\"\n",
    "    Constructs a mini version of the GPT model. The architecture is comprised of a\n",
    "    token and position embedding layer followed by a single Transformer block. The final\n",
    "    layer is a dense layer with softmax activation for prediction. \n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Mini GPT model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input layer expects inputs of shape (maxlen,) with type int32\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "\n",
    "    # Create the token and position embedding layer and compute the embeddings\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "\n",
    "    # Create the Transformer block and compute its output\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "\n",
    "    # Final dense layer with size equal to the vocabulary size\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "\n",
    "    # Construct the Keras model\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "\n",
    "    # Loss function for the training \n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Model compilation: use Adam optimizer and the defined loss function\n",
    "    # Note that we specify `None` for the second loss to not optimize based on the Transformer block's output\n",
    "    model.compile(\"adam\", loss=[loss_fn, None])\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {
    "a896bbaf-c33d-4700-b0b2-dc7aabd2e598.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "***Shout out to this amazing app: https://netron.app/***\n",
    "***You simply drag and drop your tensorflow or pytorch model into the app and it generates the diagram for you!***\n",
    "\n",
    "![Screen Shot 2023-06-25 at 6.55.38 PM.png](attachment:a896bbaf-c33d-4700-b0b2-dc7aabd2e598.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Text Generator Callback Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A callback to generate text from a trained model at the end of each epoch. It uses the model's \n",
    "    predictions to sample a token, add it to the input, and generate subsequent tokens.\n",
    "\n",
    "    Attributes:\n",
    "        max_tokens (int): The number of tokens to be generated after the prompt.\n",
    "        start_tokens (list): The token indices for the starting prompt.\n",
    "        index_to_word (list): Mapping from token indices to words, obtained from the TextVectorization layer.\n",
    "        k (int): Number of token predictions to consider for sampling the next token.\n",
    "        print_every (int): Frequency of print for the generated text (in number of epochs).\n",
    "    \"\"\"\n",
    "    def __init__(self, max_tokens, start_tokens, index_to_word, top_k=20, print_every=1,**kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the TextGenerator callback.\n",
    "\n",
    "        Args:\n",
    "            max_tokens (int): Maximum number of tokens to be generated.\n",
    "            start_tokens (list): List of integers representing the starting tokens.\n",
    "            index_to_word (list): List of strings representing the mapping from indices to words.\n",
    "            top_k (int, optional): Number of top token predictions to sample from. Defaults to 10.\n",
    "            print_every (int, optional): Frequency of print (in number of epochs). Defaults to 1.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.k = top_k\n",
    "        self.print_every = print_every\n",
    "        self.generated_texts = [] # for qualitative validation set\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        \"\"\"\n",
    "        Sample a token index from the token predictions based on their probabilities.\n",
    "\n",
    "        Args:\n",
    "            logits (tf.Tensor): The token predictions (logits) of the model.\n",
    "\n",
    "        Returns:\n",
    "            int: The sampled token index.\n",
    "        \"\"\"\n",
    "        # Select top-k logits and their indices\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "\n",
    "        # Apply softmax to transform logits into probabilities\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "\n",
    "        # Randomly select an index according to the probability distribution\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        \"\"\"\n",
    "        Convert a token index into the corresponding word.\n",
    "\n",
    "        Args:\n",
    "            number (int): The token index.\n",
    "\n",
    "        Returns:\n",
    "            str: The corresponding word.\n",
    "        \"\"\"\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        At the end of each epoch, generate text and print it.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): The current epoch number.\n",
    "            logs (dict, optional): Dictionary of metrics from the epoch. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Create a copy of start tokens for generation\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "\n",
    "        # Only generate text at specified frequency\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "\n",
    "        # Generate tokens until max tokens reached\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "\n",
    "            # Adjust padding based on length of start tokens\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "\n",
    "            x = np.array([x])\n",
    "\n",
    "            # Use the model to predict the probabilities for the next token\n",
    "            y, _ = self.model.predict(x)\n",
    "\n",
    "            # Sample a token from the model's output distribution\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "\n",
    "            # Append the token to the list of generated tokens\n",
    "            tokens_generated.append(sample_token)\n",
    "\n",
    "            # Add the token to the start tokens for the next generation\n",
    "            start_tokens.append(sample_token)\n",
    "\n",
    "            # Increase the number of tokens generated by 1\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "\n",
    "        # Convert the tokens into actual words and join them into a string\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        \n",
    "        self.generated_texts.append((epoch, txt)) # Store for evalutation after training\n",
    "\n",
    "\n",
    "        # Print the generated text\n",
    "        print(f\"generated text:\\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word/Index Mapping Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Callback Object\n",
    "\n",
    "***We also need to supply a starting prompt to act as a qualitative validation set to evaluate the models performance from a 'does it make more sense' per epoch. It will generate(predict) a text sequence continuation from the starting prompt at the end of every epoch to inspect.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_prompt = \"I would have\"\n",
    "\n",
    "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "num_tokens_generated = 42\n",
    "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "***I apologize for the scrolling your about to do. I wanted to generate text at each epoch so that along with loss there would be some qualitative evaluation on the models performance throughout training but I could not find a way to remove the progress bars for each step inside the epochs... If anyone reading this knows a way please comment.***\n",
    "\n",
    "***Until about `25` epochs many of the generations depending on the satrting prompt during training had nonsensical outputs. So we will use `25` to get a good baseline model to evaluate.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 80)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 80, 512)          51240960  \n",
      " g_2 (TokenAndPositionEmbedd                                     \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_block_2 (Transf  (None, 80, 512)          2628096   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 80, 100000)        51300000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 105,169,056\n",
      "Trainable params: 105,169,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "1/1 [==============================] - 0s 162ms/step loss: 4.1947 - dense_8_loss: 4.19\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "generated text:\n",
      "I would have no one of our school that you support to the public schools.                               \n",
      "\n",
      "194/194 [==============================] - 39s 197ms/step - loss: 4.1947 - dense_8_loss: 4.1947\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 25ms/step- loss: 3.1873 - dense_8_loss: 3.18\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "generated text:\n",
      "I would have been in [UNK] when the way to the [UNK] the article has a few months ago the same thing that the police department of those who are not see what was a [UNK] in a few of [UNK] and a person and the\n",
      "\n",
      "194/194 [==============================] - 38s 196ms/step - loss: 3.1873 - dense_8_loss: 3.1873\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 0s 23ms/step- loss: 2.6611 - dense_8_loss: 2.66\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "generated text:\n",
      "I would have an example of the point of the fact that you are in this article, as a typical of the liberals or any of that he had to be that he was a reduction of the last decade where the Democratic Convention and then\n",
      "\n",
      "194/194 [==============================] - 38s 197ms/step - loss: 2.6611 - dense_8_loss: 2.6611\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 24ms/step- loss: 2.2477 - dense_8_loss: 2.24\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "generated text:\n",
      "I would have been that some of the GOP a lot of the GOP controlled by the last night, they had just have been told the last night to have a big no-no's if the station when they were two or a time and we had\n",
      "\n",
      "194/194 [==============================] - 38s 196ms/step - loss: 2.2477 - dense_8_loss: 2.2477\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 23ms/step- loss: 1.9629 - dense_8_loss: 1.96\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "generated text:\n",
      "I would have stayed on the [UNK] My friend that school is to [UNK] Lonely Hearts 1, [UNK] he was killed because a client. Just stating that he was a nod in the GOP nomination the Woroniecki's theology- and the GOP works for Oregon State and\n",
      "\n",
      "194/194 [==============================] - 38s 197ms/step - loss: 1.9629 - dense_8_loss: 1.9629\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 21ms/step- loss: 1.7110 - dense_8_loss: 1.71\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "generated text:\n",
      "I would have a few days after you have to say that you are better \"good\" or mental illness.                           \n",
      "\n",
      "194/194 [==============================] - 38s 197ms/step - loss: 1.7110 - dense_8_loss: 1.7110\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 22ms/step- loss: 1.4559 - dense_8_loss: 1.45\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "generated text:\n",
      "I would have beaten, [UNK] is interested to expertise on their own special needs to read their sixties. Portland is the city council and our parents. Point in the tournament. The city council positions on [UNK] The GOP is a disambiguation page to get to PDX\n",
      "\n",
      "194/194 [==============================] - 38s 197ms/step - loss: 1.4559 - dense_8_loss: 1.4559\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 23ms/step- loss: 1.2217 - dense_8_loss: 1.22\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "generated text:\n",
      "I would have been very much time to find them for the enforced and advertised.                               \n",
      "\n",
      "194/194 [==============================] - 38s 197ms/step - loss: 1.2217 - dense_8_loss: 1.2217\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 22ms/step- loss: 1.0358 - dense_8_loss: 1.03\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "generated text:\n",
      "I would have to tell me what a better already!                                    \n",
      "\n",
      "194/194 [==============================] - 38s 197ms/step - loss: 1.0358 - dense_8_loss: 1.0358\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 21ms/step- loss: 0.8874 - dense_8_loss: 0.88\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "generated text:\n",
      "I would have listened. I is not questioned the motives. KWAX an ok and was an article and was done a student run. I have to be upfront about economics of 2012, WW of a coach in Eugene, and tortured logic of Western-actor, Sam Elliot, Sam\n",
      "\n",
      "194/194 [==============================] - 38s 196ms/step - loss: 0.8874 - dense_8_loss: 0.8874\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 23ms/step- loss: 0.7655 - dense_8_loss: 0.76\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "generated text:\n",
      "I would have some point to having them money on over the first started vibrating stick in a third of extra EWEB home in a few extra money for their level. My computer to pay in the cab and I had their services. If you don't\n",
      "\n",
      "194/194 [==============================] - 38s 196ms/step - loss: 0.7655 - dense_8_loss: 0.7655\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 22ms/step- loss: 0.6677 - dense_8_loss: 0.66\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "generated text:\n",
      "I would have a slight difference here. The union would be the act in the good majority of those in Portland weekly publication.                       \n",
      "\n",
      "194/194 [==============================] - 38s 196ms/step - loss: 0.6677 - dense_8_loss: 0.6677\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 23ms/step- loss: 0.5857 - dense_8_loss: 0.58\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "generated text:\n",
      "I would have to say a beauty of the English language. Unlike Mr. Trump those of the kids priority rather are being able to get out of Oregon and cleaner burning table for the society. Once again, I do we. You have a huge area, so\n",
      "\n",
      "194/194 [==============================] - 38s 196ms/step - loss: 0.5857 - dense_8_loss: 0.5857\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 22ms/step- loss: 0.5152 - dense_8_loss: 0.51\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "generated text:\n",
      "I would have to waste your life somewhere                                      \n",
      "\n",
      "194/194 [==============================] - 38s 197ms/step - loss: 0.5152 - dense_8_loss: 0.5152\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 0s 22ms/step- loss: 0.4544 - dense_8_loss: 0.45\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "generated text:\n",
      "I would have to say that [UNK] driver is not in it every way of the story applies to the Bundy to them, and all the people visualize the vision\". [UNK]               \n",
      "\n",
      "194/194 [==============================] - 38s 196ms/step - loss: 0.4544 - dense_8_loss: 0.4544\n"
     ]
    }
   ],
   "source": [
    "model = MiniGPT()\n",
    "\n",
    "N_EPOCHS = 15\n",
    "history  = model.fit(text_ds, verbose=1, epochs=N_EPOCHS, callbacks=[text_gen_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Training Loss Per Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Training loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Validation Set Per Epoch\n",
    "\n",
    "***Since we don't have a quantitative validation set in this situation we can use a qualitative validation set. These would be the generated text from the end of each epoch. This can give us some clues along with the losses per epoch to see how the models performance progressed through training.***\n",
    "\n",
    "***Lets inspect what the first five generations look llike compared to the last five during training.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Generated Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I would have an argument to the right to see a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I would have no one of our school that you sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I would have been in [UNK] when the way to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>I would have an example of the point of the fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>I would have been that some of the GOP a lot o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch                                     Generated Text\n",
       "0      0  I would have an argument to the right to see a...\n",
       "1      0  I would have no one of our school that you sup...\n",
       "2      1  I would have been in [UNK] when the way to the...\n",
       "3      2  I would have an example of the point of the fa...\n",
       "4      3  I would have been that some of the GOP a lot o..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Generated Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>I would have a slight difference here. The uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>I would have to say a beauty of the English la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>I would have to waste your life somewhere     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>I would have to say that [UNK] driver is not i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>here we wanted to wait to change if the same [...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch                                     Generated Text\n",
       "12     11  I would have a slight difference here. The uni...\n",
       "13     12  I would have to say a beauty of the English la...\n",
       "14     13  I would have to waste your life somewhere     ...\n",
       "15     14  I would have to say that [UNK] driver is not i...\n",
       "16      0  here we wanted to wait to change if the same [..."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for epoch, text in text_gen_callback.generated_texts:\n",
    "#     print(f\"Epoch: {epoch+1}\\nGenerated Text:\\n{text}\\n\")\n",
    "    # Create a DataFrame\n",
    "    \n",
    "df_val = pd.DataFrame(text_gen_callback.generated_texts, columns=['Epoch', 'Generated Text'])\n",
    "\n",
    "display(df_val.head(5));df_val.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***It appears to that with more iterations the model becomes more and more realistic in its generations. The model starts producing less unknown tokens over time during training iterations.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "***Now that we have trained the model to generate toxic comments from a starting prompt we can begin to generate our synthetic data.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(starting_prompt=''):\n",
    "    new_start_prompt = \"here we\"\n",
    "    new_start_tokens = [word_to_index.get(word, 1) for word in new_start_prompt.split()]\n",
    "\n",
    "    text_gen_callback.start_tokens = new_start_tokens\n",
    "    text_gen_callback.on_epoch_end(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "generated text:\n",
      "here we have a severe mental illness and has nothing to do with [UNK] showing these two or how our troops in there is something to be there. I can be way to spraying by lowering the overall Northwest and the individual city [UNK] the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_text(\"you are\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## State of training data\n",
    "\n",
    "* I learned alot on this project. It is interesting to me that with all of the data in the world there is still a shortage of industrial sized, cleaned, organized, and labeled training data. \n",
    "\n",
    "* It would seem that allthough in academia the glory goes to the next new model or algorithm when really the training data is probably more important at this point. It does not matter how fancy an algorithm is(some of them are pretty fancy) or a model is, without good training data we are limited in the problems we can tackle effectively.\n",
    "\n",
    "* During this project I researched training data sets and where they come from. Most of the open source ones come from a handful of instituions namely universities with a few corporations willing to share the training datasets or models which would not effect their market share significantly releasing them.\n",
    "\n",
    "* Even the sensitive subjetcs such as the language used in this projects training data is very important to solve very prominent problems we have.\n",
    "\n",
    "## Decisions made along the way\n",
    "* I started off this project trying to adapt a GAN to acheive this tast but there is little work done in this area and I had a lot of trouble trying to implement one of the papers where they achieve this task.\n",
    "\n",
    "* I chose this method as this model is lightweight and quick to train and tune for any specific style of language. As I mentioned at the beginning I originally tested this capabillity on the movie lines IMDB dataset and had great results. \n",
    "\n",
    "* The dataset I used turned out to be admittedly smaller than probably needed to get better results but I am confident that once I wrange up more nasty online comments the model will do much better after retraining.\n",
    "\n",
    "* The feature dimensions in the attention head made a huge difference on training time and performance. I had better results at 512 than it is currently at (256) but the time to train 25 epochs grew exponentially. If I had more Kaggle GPU hours I would have done my final training round with 512 in the attention head.\n",
    "\n",
    "* I originally tried a custom learning rate optimizing function but because of the large differences in epochs I was trying for different experiments I switched over to just using ADAM for this task.\n",
    "\n",
    "* The starting prompt matters. I found out that after looking at the popular bigrams and trigrams if I chose those sequences the model would generate sensible text more easily. \n",
    "\n",
    "* The length of the starting prompt also effects how different each output is dramatically.\n",
    "\n",
    "## Whats Next\n",
    "\n",
    "* I am going to run this model and compile a set of generated texts which are known to be toxic and add them to the training set and retrain the BERT based model I used when competing in this competition to see what results I get with the added training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Toxic_MiniGPT_512.keras')\n",
    "\n",
    "\n",
    "with tf.keras.utils.custom_object_scope({\n",
    "    'TransformerBlock': TransformerBlock,\n",
    "    'TokenAndPositionEmbedding': TokenAndPositionEmbedding\n",
    "}):\n",
    "    loaded_model = tf.keras.models.load_model('Toxic_MiniGPT_512.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path where you want to save the vocab\n",
    "file_path = 'vocab.txt'\n",
    "\n",
    "# Write each word from the vocab array to the file\n",
    "with open(file_path, 'w') as file:\n",
    "    for word in vocab:\n",
    "        file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'vocab.txt'\n",
    "\n",
    "# Read each line from the file and append it to the vocab list\n",
    "loaded_vocab = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        loaded_vocab.append(line.strip())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
